# -*- coding: utf-8 -*-
from __future__ import annotations

# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘  â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•‘
â•‘  â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•‘
â•‘  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•‘
â•‘  â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘â–„â–„ â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•‘
â•‘  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â•‘
â•‘  â•šâ•â•  â•šâ•â• â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•    â•šâ•â•â–€â–€â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•â•   â•šâ•â•   â•‘
â•‘                                                                              â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘                                                                              â•‘
â•‘                   QUANTITATIVE ALPHA GENERATION PLATFORM                     â•‘
â•‘                                                                              â•‘
â•‘              Reinforcement Learning â€¢ Algorithmic Trading â€¢ AI               â•‘
â•‘                                                                              â•‘
â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
â•‘                                                                              â•‘
â•‘   STATUS:  â— LIVE        LATENCY: 0.34ms       UPTIME: 99.97%                â•‘
â•‘   MODELS:  ACTIVE (12)   SIGNALS: 7/min      AUM: *****                      â•‘
â•‘                                                                              â•‘
â•‘              "Where Mathematics Meets Market Microstructure"                 â•‘
â•‘                                                                              â•‘
â•‘                    [ SYSTEM ONLINE ] v5.0.6 | 2026-01-06                    â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

V5.0.6 MARL ANTI-COLLAPSE MECHANISMS
================================================================================
CRITICAL FIX FOR POLICY COLLAPSE AND OVER-COORDINATION:

Problem: 5/8 agents collapsed to near-deterministic policies (entropy < 0.15 bits)
         - Agent xs: 0% BUY (complete collapse)
         - Agent 5m: 0.4% BUY (severe collapse) 
         - Agent m: 99.6% BUY (severe collapse)
         - Shared z_shared signal overwhelms local state information

Root Causes:
  1. Q-spread loss BROKEN: 1/(var+Îµ) always hits cap at 10.0
  2. No entropy regularization in actor loss
  3. Greedy action selection with no exploration
  4. No per-agent adaptive exploration

Solutions Implemented:
  1. ExplorationManager class
     - Temperature-scaled softmax action selection
     - Per-agent epsilon based on policy entropy
     - Collapsed agents get boosted exploration (30% random)
     - Automatic temperature annealing

  2. Fixed DiversityLoss.compute_qvalue_spread_loss()
     - Changed from 1/(var+Îµ) to -log(var+Îµ)
     - Added range penalty for Q-value magnitude collapse
     - Added correlation penalty for agent Q-value similarity
     - Continuous gradient signal (no more capping at 10.0)

  3. Entropy Bonus in Actor Loss
     - actor_loss = -Q(s,a) - Î² * H(Ï€)
     - Warmup: steps 0-100, coef 0.05 â†’ 0.1
     - Hold: steps 100-5000, coef = 0.1
     - Decay: steps 5000+, coef â†’ 0.01

  4. Collapse Recovery Mechanism
     - Track consecutive collapse per agent
     - Soft reset (noise injection) after 10 consecutive collapses
     - Breaks persistent local minima

  5. GatedEntanglement Module (Architectural)
     - Learnable gate for local vs shared signal balance
     - Caps z_shared influence at 30%
     - Ensures local signals always dominate

Expected Results:
  Before: Collapsed 5/8, Mean Entropy 0.26, System ğŸ”´ CRITICAL
  After:  Collapsed 0/8, Mean Entropy 0.72+, System âœ… HEALTHY

References:
  - Haarnoja et al. 2018: SAC entropy regularization
  - Lowe et al. 2017: MADDPG over-coordination
  - Rashid et al. 2018: QMIX value decomposition
  - Sunehag et al. 2018: VDN mixing networks

================================================================================

V5.0.5 AGENT-LEVEL LEARNING DIAGNOSTICS - MARL OBSERVABILITY
================================================================================
PURPOSE: Make agent-specific learning dynamics observable to diagnose:
  - Policy collapse (agents becoming near-deterministic)
  - Over-coordination (agents ignoring local state information)
  - Gradient homogenisation (agents receiving identical gradient signals)
  - Q-value saturation (exploding value estimates)

THEORETICAL FOUNDATION (CTDE Literature):
  - Oliehoek et al. 2016: Decentralised POMDPs
  - Foerster et al. 2018: COMA counterfactual baselines
  - Lowe et al. 2017: MADDPG over-coordination issues
  - Rashid et al. 2018: QMIX value decomposition

NEW COMPONENTS:
1. AgentLevelDiagnostics class
   - Policy entropy computation (collapse detection)
   - Pairwise action correlation (over-coordination detection)
   - Gradient flow statistics per agent
   - TD error distribution per agent
   - Q-value saturation detection

2. Enhanced _print_training_metrics()
   - Per-agent entropy column
   - Status indicators: âœ…OK, ğŸ”´COLL, ğŸŸ¡LOW-H, ğŸŸ SAT
   - Policy collapse warnings
   - Over-coordination warnings
   - MARL diagnostic summary section

METRICS OUTPUT EXAMPLE:
  ğŸ¤– PER-AGENT METRICS
  Agent       Q-Mean   Q-Std  Q-Spread   BUY %  SELL %  Entropy  Samples  Status
  10m         0.0251  0.0392    0.0784    1.4%   98.6%   0.0891      512  ğŸ”´COLL
  5m         -0.1084  0.0601    0.1203   50.2%   49.8%   0.9998      512  âœ…OK

  âš ï¸  POLICY COLLAPSE DETECTED in 1 agents: 10m
  ğŸ”¬ MARL DIAGNOSTIC SUMMARY
    Mean Policy Entropy:    0.5444 bits (max=1.0)
    Collapsed Agents:       1/8
    System Health:          ğŸŸ¡ DEGRADED

================================================================================

V5.0.4 NON-BLOCKING SIGNAL PUBLISHER - ABLY TIMEOUT FIX
================================================================================
CRITICAL FIX FOR "ABLY PUBLISH TIMEOUT" ISSUE:

Problem: await channel.publish() can timeout for 30-240+ seconds.
         During this time, ALL async operations are blocked.
         Feature messages queue but aren't processed.
         State cache goes stale (same root cause as before, different location).

Evidence from logs:
  [07:33:30] httpcore.ConnectTimeout during signal publish
  [07:33:30] - [07:37:18] = 3 min 48 sec gap in feature messages

Solution: AsyncSignalPublisher runs publishing in a background thread.
         - Signal publishing runs in separate ThreadPoolExecutor
         - Main event loop continues processing features
         - Timeouts logged but don't block data flow
         - Automatic retry with configurable timeout (10s default)

Components:
1. AsyncSignalPublisher class
   - Thread pool executor for background publishing
   - Non-blocking submit_signal() method
   - Retry logic with timeout
   - Statistics tracking

2. Modified _publish_direct()
   - Uses AsyncSignalPublisher instead of blocking await
   - Falls back to timeout-wrapped blocking if publisher unavailable

================================================================================

V5.0.3 NON-BLOCKING TRAINING EXECUTOR - STALE DATA FIX
================================================================================
CRITICAL FIX FOR "STALE DATA" ISSUE DURING TRAINING:

Problem: Training takes 20-60+ seconds, blocking the main event loop.
         During this time, Ably messages cannot be processed, causing
         the state cache to become stale (no new data flowing).

Solution: AsyncTrainingExecutor runs training in a separate thread.
         - Main thread continues processing Ably messages
         - Data ingestion never interrupted
         - State cache remains fresh during training

Components:
1. AsyncTrainingExecutor class
   - Thread pool executor for background training
   - Non-blocking submit_training() method
   - Training status tracking
   - Error handling in background thread

2. Modified training loop
   - Checks if training already in progress (skips if so)
   - Submits training without blocking
   - Reports results asynchronously

================================================================================

V5.0.0 (K1RL_QU1NT_V5) - DEADLOCK PREVENTION & STAGNATION FIX
================================================================================
CRITICAL FIXES FOR VOTING DEADLOCK AND Q-VALUE STAGNATION:

1. CONFIDENCE-BASED TIE BREAKER
   - Replaces BUY bias on 50/50 voting ties
   - Uses Q-value spread (confidence) to determine winner
   - Deadlock detection: After 10 consecutive ties, injects random exploration

2. Q-VALUE DIVERSITY INJECTION
   - Tracks per-agent vote history
   - If agent votes same way 20 times, injects small noise
   - Prevents agents from getting stuck

3. SIGNAL DIVERSITY ENFORCEMENT
   - Tracks last 30 published signals
   - If 20 consecutive identical signals detected, flips the signal
   - Last resort mechanism to ensure trading diversity

4. HEALTH MONITORING SYSTEM
   - State cache freshness monitor (alerts if data stale > 30s)
   - Ably connection watchdog (alerts on disconnect)

5. EVENT NAME FIX (V5.0.1)
   - Fixed subscription to both "integrated-features" AND "feature" events
   - Publisher sends both event types, receiver now catches both
   - Added visible logging when feature messages received

================================================================================

V8.6.2 FIX: Multi-Agent Experience Buffer Fix
- MultiAgentExperience dataclass with full agent attribution
- MultiAgentExperienceReplay with per-agent sampling
- ConcurrentStateCollector for staleness prevention
- QMIX/VDN/COMA mixing networks for value decomposition
- Proper CTDE (Centralized Training, Decentralized Execution)

V8.6.1 FIX: Agent Multipliers Credit Assignment Integration
- Proper per-agent credit assignment for multi-agent learning
- Fixed missing add_reward method in RewardBatchProcessor
- Compute agent_multipliers before signal publishing
- Apply credit scaling in store_experience_for_agent

================================================================================

"""
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 26 12:48:33 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Nov 26 09:22:07 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
# from __future__ import annotations  # MOVED TO TOP OF FILE
"""
Created on Fri Nov  7 13:54:28 2025

@author: ENG Karl
"""


# -*- coding: utf-8 -*-
"""

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Thu Oct 23 22:01:25 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-

# -*- coding: utf-8 -*-
"""
Created on Thu Oct 23 18:04:07 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Tue Oct 21 13:20:52 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Oct 15 20:14:23 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Oct  4 21:06:41 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Oct  3 17:00:38 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Oct  3 10:35:15 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Tue Sep 30 08:55:52 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Sep 29 17:27:09 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Sep 29 16:14:20 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Sep 26 22:35:29 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Sep 26 18:33:52 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Sep 22 16:22:31 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Sep 20 13:39:43 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Sep 20 09:27:39 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Sep 15 19:22:13 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Sep  7 15:25:31 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Sep  7 14:11:33 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Sep  7 13:53:02 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Sep  3 21:32:44 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Sep  3 19:40:36 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Tue Sep  2 23:31:12 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Tue Sep  2 22:12:35 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug 31 08:43:42 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Aug 25 13:59:48 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug 17 00:20:31 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug 10 09:14:22 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sat Aug  9 21:36:06 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Aug  8 15:20:27 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Aug  4 16:56:53 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Aug  4 15:39:29 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Aug  4 10:06:32 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug  3 22:53:08 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug  3 17:15:27 2025

@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Aug  3 10:04:31 2025
n
@author: ENG Karl
"""

# -*- coding: utf-8 -*-
"""
Created on Sun Jul 27 17:58:04 2025

@author: ENG Karl
"""
# NOTE: These pip commands are for Colab/Jupyter - comment out for standalone use
!pip install ably
!pip install colorama
!pip install --upgrade pip
!pip install --upgrade cryptography google-auth google-cloud-storage
!pip install tensorflow
!pip install qiskit qiskit-aer


# --- Imports ---
import tensorflow as tf
import asyncio
import json
import logging
import math
import random
import sys
import os
import pickle
import joblib
import ably
from collections import  deque, namedtuple, Counter , defaultdict

from datetime import datetime
from keras.saving import register_keras_serializable

import matplotlib.pyplot as plt
from collections import deque
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
import ssl
# REMOVED DUPLICATE: import os

import threading
import queue
import time
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import asyncio
# REMOVED DUPLICATE: from collections import deque
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Dict, Any, Optional, List, Callable
from threading import Thread, Lock

import numpy as np
import pandas as pd

from huggingface_hub import HfApi

import websockets

from scipy.stats import kurtosis, skew
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.utils.validation import check_is_fitted
import nest_asyncio
import uuid
# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import asyncio
# REMOVED DUPLICATE: import smtplib
from email.mime.text import MIMEText
# REMOVED DUPLICATE: import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset, DataLoader
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import torch.nn as nn
# REMOVED DUPLICATE: import random
import traceback
# REMOVED DUPLICATE: import asyncio
from ably import AblyRealtime
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import os
# REMOVED DUPLICATE: import threading
import torch.optim as optim

from typing import Tuple, Optional, Dict

# REMOVED DUPLICATE: import os
import shutil
import zipfile
from google.cloud import storage

# REMOVED DUPLICATE: import uuid
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import joblib
# REMOVED DUPLICATE: import pickle
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: from sklearn.preprocessing import StandardScaler

# Add this import at the top of your file (around line 50-60 with other imports)
from collections.abc import Mapping
from types import MappingProxyType

# V8.5: Voting Ensemble Trainer for gradient voting during training


# Alternative: Use the string name check instead

import requests
# REMOVED DUPLICATE: import json
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import queue
# REMOVED DUPLICATE: from datetime import datetime
from collections import deque, Counter, defaultdict
from dataclasses import dataclass, field
# REMOVED DUPLICATE: from typing import Dict, Any, Optional, List, Callable
from enum import Enum
from contextlib import contextmanager

from email.mime.application import MIMEApplication  # Add this import
# REMOVED DUPLICATE: from email.mime.text import MIMEText
# REMOVED DUPLICATE: from email.mime.multipart import MIMEMultipart
# REMOVED DUPLICATE: from email.mime.base import MIMEBase
# REMOVED DUPLICATE: from email import encoders


# ============================================================================
# CHECKPOINT SYSTEM IMPORTS - INTEGRATED FROM checkpoint_class.txt
# ============================================================================
from pathlib import Path
from typing import Set
# Note: Other imports (torch, json, os, time, threading, queue, datetime,
#       dataclass, field, Enum, logging, google.cloud.storage) already exist above
# ============================================================================
# ============================================================================
# QUANTUM ADVISOR FIX IMPORT
# ============================================================================
# Import the quantum advisor diagnostic and fix functions
# This enables comprehensive quantum advisor logging and ensures proper integration

# ============================================================================
# AGENT MULTIPLIERS CREDIT ASSIGNMENT MODULE - V8.6.1 FIX
# ============================================================================
# Integrated from agent_multipliers_fix.py
# Provides per-agent credit assignment for proper multi-agent learning
# References:
# - COMA (Foerster et al., 2018): Counterfactual credit assignment
# - Difference Rewards (Tumer & Agogino, 2007)
# - QMIX (Rashid et al., 2018): Monotonic value decomposition
# ============================================================================

class CreditAssignmentMethod(Enum):
    """Available credit assignment strategies for multi-agent learning"""
    EQUAL = "equal"                    # All agents get multiplier 1.0
    AGREEMENT_CONFIDENCE = "agreement_confidence"  # Based on agreement + Q-spread
    SHAPLEY = "shapley"                # Approximate Shapley values
    DIFFERENCE = "difference"          # Difference rewards (counterfactual)
    VDN = "vdn"                        # Value Decomposition Networks
    QMIX = "qmix"                      # Monotonic Q-value mixing


def compute_agent_credit_multipliers(
    agent_actions: Dict[str, int],
    agent_q_values: Dict[str, np.ndarray],
    final_action: int,
    method: str = 'agreement_confidence'
) -> Dict[str, float]:
    """
    Compute per-agent credit assignment multipliers.
    
    This function addresses the root cause of agent homogenization by providing
    differentiated reward scaling based on each agent's contribution.
    
    Methods:
    - 'equal': All agents get multiplier 1.0 (baseline)
    - 'agreement_confidence': Based on agreement with final vote + Q-spread confidence
    - 'shapley': Approximate Shapley values (more expensive)
    - 'difference': Difference rewards (counterfactual)
    
    Args:
        agent_actions: Dict of agent_name â†’ action (0=BUY, 1=SELL, 2=HOLD)
        agent_q_values: Dict of agent_name â†’ Q-values array [Q(BUY), Q(SELL)]
        final_action: The final voted action
        method: Credit assignment method to use
        
    Returns:
        Dict of agent_name â†’ multiplier (typically 0.5 to 1.5)
    """
    if not agent_actions:
        return {}
    
    if method == 'equal':
        return {agent: 1.0 for agent in agent_actions}
    
    if method == 'agreement_confidence':
        return _compute_agreement_confidence_multipliers(
            agent_actions, agent_q_values, final_action
        )
    
    if method == 'shapley':
        return _compute_approximate_shapley(
            agent_actions, agent_q_values, final_action
        )
    
    if method == 'difference':
        return _compute_difference_rewards(
            agent_actions, agent_q_values, final_action
        )
    
    # Default fallback
    return {agent: 1.0 for agent in agent_actions}


def _compute_agreement_confidence_multipliers(
    agent_actions: Dict[str, int],
    agent_q_values: Dict[str, np.ndarray],
    final_action: int
) -> Dict[str, float]:
    """
    Credit based on agreement with final decision and Q-value confidence.
    High confidence + correct = big bonus; High confidence + wrong = big penalty.
    """
    multipliers = {}
    
    for agent_name, action in agent_actions.items():
        q_vals = agent_q_values.get(agent_name)
        if q_vals is None:
            multipliers[agent_name] = 1.0
            continue
        
        if not isinstance(q_vals, np.ndarray):
            q_vals = np.array(q_vals, dtype=np.float32)
        q_vals = q_vals.flatten()
        
        # Compute confidence (Q-spread)
        if len(q_vals) >= 2:
            q_spread = abs(q_vals[0] - q_vals[1])
            confidence = min(1.0, q_spread / 2.0)
        else:
            confidence = 0.5
        
        agreed = (action == final_action)
        
        if agreed:
            multipliers[agent_name] = 1.0 + 0.5 * confidence  # [1.0, 1.5]
        else:
            multipliers[agent_name] = 1.0 - 0.5 * confidence  # [0.5, 1.0]
    
    return multipliers


def _compute_approximate_shapley(
    agent_actions: Dict[str, int],
    agent_q_values: Dict[str, np.ndarray],
    final_action: int,
    n_samples: int = 100
) -> Dict[str, float]:
    """Approximate Shapley values using random permutation sampling."""
    agents = list(agent_actions.keys())
    n_agents = len(agents)
    
    if n_agents == 0:
        return {}
    
    shapley_values = {agent: 0.0 for agent in agents}
    
    for _ in range(n_samples):
        perm = np.random.permutation(agents)
        coalition = set()
        prev_value = 0.0
        
        for agent in perm:
            coalition.add(agent)
            coalition_value = sum(
                agent_q_values.get(a, np.array([0, 0]))[final_action if final_action < 2 else 0]
                for a in coalition
            )
            marginal = coalition_value - prev_value
            shapley_values[agent] += marginal
            prev_value = coalition_value
    
    for agent in agents:
        shapley_values[agent] /= n_samples
    
    sv_list = list(shapley_values.values())
    mean_sv = np.mean(sv_list) if sv_list else 0
    std_sv = np.std(sv_list) + 1e-8 if sv_list else 1e-8
    
    multipliers = {}
    for agent in agents:
        z = (shapley_values[agent] - mean_sv) / std_sv
        multipliers[agent] = 1.0 + 0.5 * np.clip(z / 2, -1, 1)
    
    return multipliers


def _compute_difference_rewards(
    agent_actions: Dict[str, int],
    agent_q_values: Dict[str, np.ndarray],
    final_action: int
) -> Dict[str, float]:
    """Difference rewards: D_i = R(s, a) - R(s, a_{-i})"""
    agents = list(agent_actions.keys())
    n_agents = len(agents)
    
    if n_agents == 0:
        return {}
    
    full_value = sum(
        agent_q_values.get(a, np.array([0, 0]))[final_action if final_action < 2 else 0]
        for a in agents
    )
    
    multipliers = {}
    for agent in agents:
        q_val = agent_q_values.get(agent, np.array([0, 0]))
        agent_contribution = q_val[final_action if final_action < 2 else 0]
        diff = agent_contribution
        
        avg_contribution = full_value / n_agents if n_agents > 0 else 0
        if abs(avg_contribution) < 1e-8:
            multipliers[agent] = 1.0
        else:
            ratio = diff / avg_contribution
            multipliers[agent] = max(0.5, min(1.5, ratio))
    
    return multipliers


def apply_agent_multipliers_to_reward(
    global_reward: float,
    agent_name: str,
    agent_multipliers: Dict[str, float],
    default_multiplier: float = 1.0
) -> float:
    """Apply credit assignment multiplier to scale agent's reward."""
    if not agent_multipliers:
        return global_reward
    
    multiplier = agent_multipliers.get(agent_name, default_multiplier)
    return global_reward * multiplier


def decompose_reward_vdn(
    global_reward: float,
    agent_names: List[str],
    weights: Optional[Dict[str, float]] = None
) -> Dict[str, float]:
    """VDN-style simple reward decomposition: r_i = w_i * r_global / Î£w_j"""
    if weights is None:
        weights = {name: 1.0 for name in agent_names}
    
    total_weight = sum(weights.values())
    return {
        name: (weights.get(name, 1.0) / total_weight) * global_reward
        for name in agent_names
    }


# ============================================================================
# END AGENT MULTIPLIERS MODULE
# ============================================================================

# ============================================================================
# MULTI-AGENT EXPERIENCE MODULE - V8.6.2 FIX
# ============================================================================
# Addresses Root Causes of Agent Homogenization:
# 1. Single action per experience â†’ Per-agent action storage
# 2. Single reward per experience â†’ Agent-specific credit assignment  
# 3. Cached/stale states â†’ Concurrent state collection
# 4. Timeframe replication â†’ Unique timeframe state handling
# 5. No agent_id in Experience â†’ Full agent attribution
#
# Based on research from:
# - QMIX: Monotonic Value Function Factorisation (Rashid et al., 2018)
# - VDN: Value Decomposition Networks (Sunehag et al., 2017)
# - COMA: Counterfactual Multi-Agent Policy Gradients (Foerster et al., 2018)
# ============================================================================

@dataclass
class MultiAgentExperience:
    """
    Enhanced experience structure with proper multi-agent attribution.
    
    EXTENDS original Experience with:
    1. originating_agent_id: Tracks WHO generated this experience
    2. actions: Dict mapping each agent to its action (not single action)
    3. rewards: Dict mapping each agent to its individual reward
    4. q_values: Per-agent Q-values at decision time
    5. timestamp: For temporal ordering and staleness detection
    6. global_state: Shared state for centralized critic (QMIX/COMA style)
    """
    # Core attribution
    originating_agent_id: str                           # WHO generated this
    timestamp: float = field(default_factory=time.time) # WHEN generated
    
    # Per-agent states (concurrent, not cached)
    states: Dict[str, Dict[str, np.ndarray]] = field(default_factory=dict)
    # agent_name -> timeframe -> state_vector
    
    next_states: Dict[str, Dict[str, np.ndarray]] = field(default_factory=dict)
    
    # Per-agent actions (CRITICAL FIX: each agent's actual action)
    actions: Dict[str, np.ndarray] = field(default_factory=dict)
    # agent_name -> one-hot action vector
    
    # Per-agent rewards (CRITICAL FIX: individual credit assignment)
    rewards: Dict[str, float] = field(default_factory=dict)
    # agent_name -> individual reward
    
    # Global/shared reward for team-level learning
    global_reward: float = 0.0
    
    # Per-agent Q-values at decision time (for QMIX-style mixing)
    q_values: Dict[str, np.ndarray] = field(default_factory=dict)
    # agent_name -> Q-values for all actions
    
    # Global state for centralized critic (QMIX hypernetwork input)
    global_state: Optional[np.ndarray] = None
    
    # Episode metadata
    done: bool = False
    episode_id: Optional[str] = None
    step_in_episode: int = 0
    
    # Counterfactual data for COMA-style credit assignment
    counterfactual_baselines: Dict[str, float] = field(default_factory=dict)
    # agent_name -> marginalized Q-value baseline
    
    # Legacy compatibility: single action/reward for backward compat
    action: Optional[np.ndarray] = None
    reward: float = 0.0
    
    def get_agent_experience(self, agent_name: str) -> 'AgentSpecificExperience':
        """
        Extract agent-specific view of this experience.
        Used for training individual agents with proper credit assignment.
        """
        return AgentSpecificExperience(
            agent_id=agent_name,
            state=self.states.get(agent_name, {}),
            action=self.actions.get(agent_name, np.zeros(2)),
            reward=self.rewards.get(agent_name, self.global_reward),
            next_state=self.next_states.get(agent_name, {}),
            done=self.done,
            q_values=self.q_values.get(agent_name),
            advantage=self._compute_advantage(agent_name),
            other_agents_actions={
                k: v for k, v in self.actions.items() if k != agent_name
            }
        )
    
    def _compute_advantage(self, agent_name: str) -> float:
        """Compute COMA-style counterfactual advantage for an agent."""
        if agent_name not in self.q_values or agent_name not in self.counterfactual_baselines:
            return 0.0
        
        q_current = self.q_values[agent_name]
        action = self.actions.get(agent_name)
        
        if action is None or q_current is None:
            return 0.0
        
        action_idx = int(np.argmax(action)) if len(action.shape) > 0 else int(action)
        q_chosen = q_current[action_idx] if action_idx < len(q_current) else 0.0
        baseline = self.counterfactual_baselines.get(agent_name, np.mean(q_current))
        
        return float(q_chosen - baseline)
    
    def to_legacy_experience(self) -> 'Experience':
        """Convert to legacy Experience format for backward compatibility."""
        return Experience(
            states=self.states,
            action=self.action if self.action is not None else np.zeros(2),
            reward=self.reward if self.reward != 0 else self.global_reward,
            next_states=self.next_states,
            done=self.done,
            originating_agent_id=self.originating_agent_id,
            agent_multipliers={
                name: self.rewards.get(name, 1.0) / max(0.001, self.global_reward)
                for name in self.rewards
            } if self.global_reward != 0 else None,
            timestamp=self.timestamp,
            global_reward=self.global_reward
        )


@dataclass  
class AgentSpecificExperience:
    """
    Agent-centric view of an experience for individual training.
    This is what each agent's network actually trains on.
    """
    agent_id: str
    state: Dict[str, np.ndarray]      # timeframe -> state
    action: np.ndarray
    reward: float                      # Individual reward (not global!)
    next_state: Dict[str, np.ndarray]
    done: bool
    q_values: Optional[np.ndarray] = None
    advantage: float = 0.0
    other_agents_actions: Dict[str, np.ndarray] = field(default_factory=dict)


class MultiAgentExperienceReplay:
    """
    Enhanced replay buffer with proper multi-agent credit assignment.
    
    EXTENDS ExperienceReplay with:
    1. Per-agent experience retrieval (sample_for_agent)
    2. Agent index for fast retrieval
    3. Priority-based sampling
    4. Agent-specific sampling weights
    
    Can be used as drop-in replacement for ExperienceReplay.
    """
    
    def __init__(
        self,
        capacity: int = 100_000,
        device: str = "cpu",
        agent_names: Optional[List[str]] = None,
        priority_alpha: float = 0.6,
        priority_beta: float = 0.4
    ):
        self.capacity = capacity
        self.device = device
        self.agent_names = agent_names or []
        self.priority_alpha = priority_alpha
        self.priority_beta = priority_beta
        
        # Main buffer storage - compatible with both Experience types
        self.buffer: List = []
        self.priorities: np.ndarray = np.zeros(capacity, dtype=np.float32)
        self.ptr = 0
        self.size = 0
        
        # Per-agent index for fast retrieval
        self._agent_indices: Dict[str, List[int]] = {
            name: [] for name in agent_names
        } if agent_names else {}
        
        # Thread safety
        self._lock = threading.RLock()
        
        # Statistics
        self.stats = {
            'total_added': 0,
            'total_sampled': 0,
            'per_agent_samples': {name: 0 for name in agent_names} if agent_names else {}
        }
        
        logger.info(
            f"MultiAgentExperienceReplay initialized: "
            f"capacity={capacity}, agents={len(agent_names) if agent_names else 0}"
        )
    
    def add(self, experience, priority: float = 1.0):
        """Add experience with proper indexing for agent-specific retrieval."""
        with self._lock:
            idx = self.ptr
            
            # Overwrite or append
            if len(self.buffer) < self.capacity:
                self.buffer.append(experience)
            else:
                # Remove old experience from agent indices
                old_exp = self.buffer[idx]
                old_agent = getattr(old_exp, 'originating_agent_id', None)
                if old_agent and old_agent in self._agent_indices:
                    try:
                        self._agent_indices[old_agent].remove(idx)
                    except ValueError:
                        pass
                self.buffer[idx] = experience
            
            # Update priority
            self.priorities[idx] = priority ** self.priority_alpha
            
            # Update agent index
            agent_id = getattr(experience, 'originating_agent_id', None)
            if agent_id:
                if agent_id not in self._agent_indices:
                    self._agent_indices[agent_id] = []
                self._agent_indices[agent_id].append(idx)
            
            # Update pointer
            self.ptr = (self.ptr + 1) % self.capacity
            self.size = min(self.size + 1, self.capacity)
            self.stats['total_added'] += 1
    
    def append(self, experience):
        """Alias for add() - backward compatibility with ExperienceReplay"""
        self.add(experience)
    
    def sample(self, batch_size: int) -> List:
        """Sample batch using prioritized experience replay."""
        with self._lock:
            if self.size < batch_size:
                if self.size == 0:
                    return []
                batch_size = self.size
            
            # Compute sampling probabilities
            probs = self.priorities[:self.size]
            probs_sum = probs.sum()
            if probs_sum < 1e-8:
                probs = np.ones(self.size) / self.size
            else:
                probs = probs / probs_sum
            
            # Sample indices
            indices = np.random.choice(self.size, size=batch_size, p=probs, replace=False)
            
            experiences = [self.buffer[i] for i in indices]
            self.stats['total_sampled'] += batch_size
            
            return experiences
    
    def sample_for_agent(
        self, 
        agent_name: str, 
        batch_size: int,
        include_others: bool = True
    ) -> List[AgentSpecificExperience]:
        """
        CRITICAL FIX: Sample experiences relevant to a specific agent.
        
        This ensures each agent trains on experiences it generated OR
        experiences where its action/reward are properly attributed.
        """
        with self._lock:
            agent_indices = self._agent_indices.get(agent_name, [])
            
            if include_others:
                all_indices = list(range(self.size))
                weights = np.ones(self.size)
                for idx in agent_indices:
                    if idx < len(weights):
                        weights[idx] *= 2.0  # 2x weight for own experiences
                weights = weights / (weights.sum() + 1e-8)
                
                if self.size < batch_size:
                    indices = all_indices
                else:
                    indices = np.random.choice(
                        all_indices, size=batch_size, p=weights, replace=False
                    )
            else:
                if len(agent_indices) < batch_size:
                    if len(agent_indices) == 0:
                        return []
                    indices = agent_indices
                else:
                    indices = random.sample(agent_indices, batch_size)
            
            # Convert to agent-specific experiences
            agent_experiences = []
            for idx in indices:
                exp = self.buffer[idx]
                if isinstance(exp, MultiAgentExperience):
                    agent_exp = exp.get_agent_experience(agent_name)
                else:
                    # Legacy Experience - create agent-specific view
                    agent_exp = AgentSpecificExperience(
                        agent_id=agent_name,
                        state=exp.states.get(agent_name, {}),
                        action=exp.action,
                        reward=exp.get_scaled_reward(agent_name) if hasattr(exp, 'get_scaled_reward') else exp.reward,
                        next_state=exp.next_states.get(agent_name, {}),
                        done=exp.done,
                        q_values=None,
                        advantage=0.0,
                        other_agents_actions={}
                    )
                agent_experiences.append(agent_exp)
            
            if agent_name in self.stats['per_agent_samples']:
                self.stats['per_agent_samples'][agent_name] += len(agent_experiences)
            
            return agent_experiences
    
    def sample_joint_batch(
        self, 
        batch_size: int,
        agent_names: List[str]
    ) -> Tuple[Dict[str, List[AgentSpecificExperience]], List]:
        """
        Sample a batch with aligned experiences for all agents.
        Used for QMIX/VDN style joint training.
        """
        full_experiences = self.sample(batch_size)
        
        agent_batches = {name: [] for name in agent_names}
        
        for exp in full_experiences:
            for agent_name in agent_names:
                if isinstance(exp, MultiAgentExperience):
                    agent_exp = exp.get_agent_experience(agent_name)
                else:
                    agent_exp = AgentSpecificExperience(
                        agent_id=agent_name,
                        state=exp.states.get(agent_name, {}),
                        action=exp.action,
                        reward=exp.reward,
                        next_state=exp.next_states.get(agent_name, {}),
                        done=exp.done
                    )
                agent_batches[agent_name].append(agent_exp)
        
        return agent_batches, full_experiences
    
    def update_priorities(self, indices: List[int], td_errors: np.ndarray):
        """Update priorities based on TD errors"""
        with self._lock:
            for idx, td_error in zip(indices, td_errors):
                if 0 <= idx < self.size:
                    self.priorities[idx] = (abs(td_error) + 1e-6) ** self.priority_alpha
    
    def __len__(self) -> int:
        return self.size
    
    def is_ready(self, min_size: int = 64) -> bool:
        return self.size >= min_size
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            'size': self.size,
            'capacity': self.capacity,
            'utilization': self.size / self.capacity if self.capacity > 0 else 0,
            'total_added': self.stats['total_added'],
            'total_sampled': self.stats['total_sampled'],
            'per_agent_samples': self.stats['per_agent_samples']
        }
    
    def clear(self):
        """Clear all experiences"""
        with self._lock:
            self.buffer = []
            self.priorities = np.zeros(self.capacity, dtype=np.float32)
            self.ptr = 0
            self.size = 0
            for name in self._agent_indices:
                self._agent_indices[name] = []


class ConcurrentStateCollector:
    """
    Collects states from all agents concurrently to prevent staleness.
    
    Problem being fixed:
    - Original code cached states and reused stale values
    - This caused agents to learn from outdated market conditions
    """
    
    def __init__(
        self,
        agent_names: List[str],
        timeframes: List[str],
        state_dim: int,
        staleness_threshold_ms: float = 100.0
    ):
        self.agent_names = agent_names
        self.timeframes = timeframes
        self.state_dim = state_dim
        self.staleness_threshold_ms = staleness_threshold_ms
        
        self._lock = threading.RLock()
        self._states: Dict[str, Dict[str, np.ndarray]] = {}
        self._timestamps: Dict[str, Dict[str, float]] = {}
        
        for agent in agent_names:
            self._states[agent] = {tf: np.zeros(state_dim) for tf in timeframes}
            self._timestamps[agent] = {tf: 0.0 for tf in timeframes}
    
    def update_state(self, agent_name: str, timeframe: str, state: np.ndarray):
        """Update a single agent's state with timestamp"""
        with self._lock:
            if agent_name not in self._states:
                self._states[agent_name] = {}
                self._timestamps[agent_name] = {}
            self._states[agent_name][timeframe] = state.copy()
            self._timestamps[agent_name][timeframe] = time.time() * 1000
    
    def update_agent_all_timeframes(self, agent_name: str, state: np.ndarray):
        """Update all timeframes for an agent with the same state"""
        with self._lock:
            if agent_name not in self._states:
                self._states[agent_name] = {}
                self._timestamps[agent_name] = {}
            timestamp = time.time() * 1000
            for tf in self.timeframes:
                self._states[agent_name][tf] = state.copy()
                self._timestamps[agent_name][tf] = timestamp
    
    def collect_all_states(self) -> Tuple[Dict[str, Dict[str, np.ndarray]], float]:
        """Atomically collect states from all agents."""
        collection_time = time.time() * 1000
        
        with self._lock:
            states = {}
            for agent in self.agent_names:
                states[agent] = {}
                for tf in self.timeframes:
                    state = self._states.get(agent, {}).get(tf, np.zeros(self.state_dim))
                    states[agent][tf] = state.copy()
            
            return states, collection_time
    
    def is_fresh(self, agent_name: str, timeframe: str) -> bool:
        """Check if a specific state is fresh"""
        with self._lock:
            ts = self._timestamps.get(agent_name, {}).get(timeframe, 0.0)
            age_ms = time.time() * 1000 - ts
            return age_ms < self.staleness_threshold_ms


class QMIXMixingNetwork(nn.Module):
    """
    QMIX Mixing Network for multi-agent value decomposition.
    
    Implements: Q_tot = f(Q_1, Q_2, ..., Q_n; s)
    where f is a monotonic function enforced by non-negative weights.
    """
    
    def __init__(
        self,
        num_agents: int,
        state_dim: int,
        embed_dim: int = 32,
        hypernet_embed: int = 64
    ):
        super().__init__()
        self.num_agents = num_agents
        self.state_dim = state_dim
        self.embed_dim = embed_dim
        
        # Hypernetworks generate mixing network weights from global state
        self.hyper_w1 = nn.Sequential(
            nn.Linear(state_dim, hypernet_embed),
            nn.ReLU(),
            nn.Linear(hypernet_embed, num_agents * embed_dim)
        )
        self.hyper_b1 = nn.Linear(state_dim, embed_dim)
        
        self.hyper_w2 = nn.Sequential(
            nn.Linear(state_dim, hypernet_embed),
            nn.ReLU(),
            nn.Linear(hypernet_embed, embed_dim)
        )
        self.hyper_b2 = nn.Sequential(
            nn.Linear(state_dim, embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim, 1)
        )
        
        logger.info(f"QMIXMixingNetwork: {num_agents} agents, state_dim={state_dim}")
    
    def forward(self, agent_qs: torch.Tensor, global_state: torch.Tensor) -> torch.Tensor:
        """Combine individual Q-values into Q_tot."""
        batch_size = agent_qs.size(0)
        
        w1 = torch.abs(self.hyper_w1(global_state))
        w1 = w1.view(batch_size, self.num_agents, self.embed_dim)
        b1 = self.hyper_b1(global_state).view(batch_size, 1, self.embed_dim)
        
        agent_qs = agent_qs.view(batch_size, 1, self.num_agents)
        hidden = F.elu(torch.bmm(agent_qs, w1) + b1)
        
        w2 = torch.abs(self.hyper_w2(global_state))
        w2 = w2.view(batch_size, self.embed_dim, 1)
        b2 = self.hyper_b2(global_state).view(batch_size, 1, 1)
        
        q_tot = torch.bmm(hidden, w2) + b2
        return q_tot.squeeze(-1).squeeze(-1)


class VDNMixingNetwork(nn.Module):
    """Value Decomposition Network - simple additive Q-value mixing."""
    
    def __init__(self, num_agents: int):
        super().__init__()
        self.num_agents = num_agents
        self.weights = nn.Parameter(torch.ones(num_agents))
    
    def forward(self, agent_qs: torch.Tensor, global_state: torch.Tensor = None) -> torch.Tensor:
        weights = F.softplus(self.weights)
        weighted_qs = agent_qs * weights.unsqueeze(0)
        return weighted_qs.sum(dim=1)


class COMACounterfactualCritic(nn.Module):
    """COMA-style centralized critic for counterfactual credit assignment."""
    
    def __init__(
        self,
        state_dim: int,
        num_agents: int,
        action_dim: int = 2,
        hidden_dim: int = 128
    ):
        super().__init__()
        self.state_dim = state_dim
        self.num_agents = num_agents
        self.action_dim = action_dim
        
        input_dim = state_dim + num_agents * action_dim
        
        self.critic = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_agents * action_dim)
        )
    
    def forward(
        self, 
        global_state: torch.Tensor,
        joint_actions: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size = global_state.size(0)
        flat_actions = joint_actions.view(batch_size, -1)
        x = torch.cat([global_state, flat_actions], dim=-1)
        
        q_all = self.critic(x)
        q_values = q_all.view(batch_size, self.num_agents, self.action_dim)
        baselines = q_values.mean(dim=-1)
        
        return q_values, baselines
    
    def compute_advantages(
        self,
        global_state: torch.Tensor,
        joint_actions: torch.Tensor,
        chosen_actions: torch.Tensor
    ) -> torch.Tensor:
        q_values, baselines = self.forward(global_state, joint_actions)
        q_chosen = q_values.gather(2, chosen_actions.unsqueeze(-1).long()).squeeze(-1)
        return q_chosen - baselines


def create_multi_agent_experience(
    originating_agent: str,
    agent_names: List[str],
    states_dict: Dict[str, Dict[str, np.ndarray]],
    actions: Dict[str, np.ndarray],
    rewards: Dict[str, float],
    global_reward: float,
    q_values: Dict[str, np.ndarray],
    done: bool,
    next_states_dict: Dict[str, Dict[str, np.ndarray]] = None,
    episode_id: str = None,
    step: int = 0
) -> MultiAgentExperience:
    """
    Factory function to create properly structured multi-agent experiences.
    
    Usage:
        exp = create_multi_agent_experience(
            originating_agent=agent_name,
            agent_names=system.agent_names,
            states_dict=current_states,
            actions={name: action_array for name, action_array in agent_actions.items()},
            rewards=decomposed_rewards,
            global_reward=global_reward,
            q_values=agent_q_values,
            done=done
        )
    """
    exp = MultiAgentExperience(
        originating_agent_id=originating_agent,
        timestamp=time.time(),
        states=states_dict,
        next_states=next_states_dict or states_dict,
        actions=actions,
        rewards=rewards,
        global_reward=global_reward,
        q_values=q_values,
        done=done,
        episode_id=episode_id,
        step_in_episode=step,
        # Legacy compatibility
        action=actions.get(originating_agent, np.zeros(2)),
        reward=rewards.get(originating_agent, global_reward)
    )
    
    # Compute counterfactual baselines if Q-values available
    for agent_name, q in q_values.items():
        if q is not None and len(q) > 0:
            exp.counterfactual_baselines[agent_name] = float(np.mean(q))
    
    return exp


# ============================================================================
# END MULTI-AGENT EXPERIENCE MODULE
# ============================================================================

ABLY_API_KEY =  "4vT80g.E0lfvg:reqqX942--QJVafOQsgRDWsBXIDtgDxg51szTmLkIeM"
ably_client = AblyRealtime(ABLY_API_KEY)
STATE_DIM = 64
ACTION_DIM = 2

# Assuming these constants are accessible at the top of your file
TIMEFRAME_LENGTHS = {
    # === High-Frequency Zone (Volatility Capture) ===
    'xs': 5,     # tick
    's': 10,     # ultra
    'm': 20,     # fast

    # === Critical Trading Zones ===
    'l': 30,     # scalp
    'xl': 60,    # 1min
    'xxl': 120,  # 2min

    # === Structure & Regime Detection ===
    '5m': 300,   # 5min
    '10m': 600,  # 10min
}
EXPECTED_TIMEFRAMES = sorted(TIMEFRAME_LENGTHS.keys()) # ['5m', 'l', 'm', 's', 'xl', 'xs']
STATE_DIM = 64

tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()

nest_asyncio.apply()
# Change DEBUG to INFO to reduce output
import logging

class DebugAndErrorFilter(logging.Filter):
    def filter(self, record):
        return record.levelno == logging.DEBUG or record.levelno >= logging.ERROR

import sys, os, logging

# Ensure real-time streaming in Colab
os.environ['PYTHONUNBUFFERED'] = '1'
try:
    sys.stdout.reconfigure(line_buffering=True)
except Exception:
    pass

# Create handler that writes to Colab output
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.DEBUG)   # Capture all first
handler.addFilter(DebugAndErrorFilter())

formatter = logging.Formatter(
    '%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
    datefmt='%H:%M:%S'
)
handler.setFormatter(formatter)

# Reset root logger (important in Colab)
root_logger = logging.getLogger()
root_logger.handlers.clear()
root_logger.addHandler(handler)
root_logger.setLevel(logging.DEBUG)

# Your system logger
logger = logging.getLogger("QuantumSystemLogger")
logger.setLevel(logging.DEBUG)

# --- Color formatter (INFO, ERROR, CRITICAL only) ---
try:
    from colorama import Fore, Style, init
    init(autoreset=True)

    class FilteredColorFormatter(logging.Formatter):
        COLORS = {
            'INFO': Fore.GREEN,
            'ERROR': Fore.RED,
            'CRITICAL': Fore.MAGENTA
        }
        def format(self, record):
            if record.levelname not in ['INFO', 'ERROR', 'CRITICAL']:
                return ""  # Suppress other logs
            color = self.COLORS.get(record.levelname, '')
            timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')
            msg = f"{timestamp} | {record.levelname:<8} | {record.name} | {record.getMessage()}"
            return f"{color}{msg}{Style.RESET_ALL}"

    for handler in logger.handlers:
        handler.setFormatter(FilteredColorFormatter())
except Exception as e:
    print(f"[LoggingBooster] Color formatting disabled: {e}")

# --- Apply same filters to submodules ---
for name in logging.root.manager.loggerDict:
    sublogger = logging.getLogger(name)
    sublogger.handlers = logger.handlers
    sublogger.setLevel(logging.CRITICAL)

# ============================================================================
# SILENCE QISKIT VERBOSE LOGGING
# ============================================================================
# Qiskit produces extremely verbose INFO logs during transpilation
# Silence all qiskit loggers to WARNING level or above
qiskit_loggers = [
    'qiskit',
    'qiskit.compiler',
    'qiskit.compiler.transpiler',
    'qiskit.passmanager',
    'qiskit.passmanager.base_tasks',
    'qiskit.transpiler',
    'qiskit.providers',
    'qiskit.circuit',
]

for qiskit_logger_name in qiskit_loggers:
    qiskit_logger = logging.getLogger(qiskit_logger_name)
    qiskit_logger.setLevel(logging.WARNING)  # Only show WARNING and above
    qiskit_logger.propagate = False  # Don't propagate to root logger

# Also silence by pattern matching
for name in list(logging.root.manager.loggerDict.keys()):
    if 'qiskit' in name.lower():
        logging.getLogger(name).setLevel(logging.WARNING)
        logging.getLogger(name).propagate = False

print("âœ… Qiskit logging silenced (WARNING level only)")
# ============================================================================

# --- Optional: TensorFlow verbosity ---
try:
    import tensorflow as tf
    tf.get_logger().setLevel('INFO')
    tf.autograph.set_verbosity(1)
except Exception as e:
    logger.warning(f"TensorFlow logger setup skipped: {e}")


# ================================================================
# QUANTUM ADVISOR DIAGNOSTIC AND FIX
# ================================================================
# This patch ensures quantum advisor is properly initialized,
# linked to agents, and logs its predictions.
# ================================================================

# REMOVED DUPLICATE: import logging
logger = logging.getLogger(__name__)

def diagnose_quantum_advisor(system):
    """
    Comprehensive diagnostic for quantum advisor setup.
    Returns dict with status and issues found.
    """
    issues = []
    status = {}

    print("\n" + "="*80)
    print("ğŸ” QUANTUM ADVISOR DIAGNOSTIC")
    print("="*80)

    # Check 1: Does system have quantum_advisor?
    if not hasattr(system, 'quantum_advisor'):
        issues.append("System missing 'quantum_advisor' attribute")
        status['has_advisor_attr'] = False
    else:
        status['has_advisor_attr'] = True

        # Check if it's None
        if system.quantum_advisor is None:
            issues.append("system.quantum_advisor is None")
            status['advisor_is_none'] = True
        else:
            status['advisor_is_none'] = False
            print(f"âœ… System has quantum_advisor: {type(system.quantum_advisor).__name__}")

    # Check 2: Does system have agents?
    if not hasattr(system, 'agents'):
        issues.append("System missing 'agents' attribute")
        status['has_agents'] = False
        return {'status': status, 'issues': issues}

    status['has_agents'] = True
    status['agent_count'] = len(system.agents)
    print(f"âœ… System has {len(system.agents)} agents")

    # Check 3: Do agents have quantum_advisor linked?
    agents_with_advisor = 0
    agents_without_advisor = []

    for name, agent in system.agents.items():
        if hasattr(agent, 'quantum_advisor'):
            if agent.quantum_advisor is not None:
                agents_with_advisor += 1
            else:
                agents_without_advisor.append(f"{name} (None)")
        else:
            agents_without_advisor.append(f"{name} (missing attr)")

    status['agents_with_advisor'] = agents_with_advisor
    status['agents_without_advisor'] = len(agents_without_advisor)

    if agents_without_advisor:
        issues.append(f"Agents without advisor: {agents_without_advisor}")
        print(f"âŒ {len(agents_without_advisor)} agents without quantum_advisor:")
        for agent_name in agents_without_advisor:
            print(f"   - {agent_name}")
    else:
        print(f"âœ… All {len(system.agents)} agents have quantum_advisor")

    # Check 4: Test quantum advisor prediction
    if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
        try:
            import torch
            import numpy as np

            # Create dummy state - use correct 2D shape [batch_size, state_dim]
            state_dim = getattr(system, 'state_dim', 58)
            test_state = torch.randn(2, state_dim)

            with torch.no_grad():
                forecast, confidence = system.quantum_advisor(test_state)

            # Check shapes
            qa = system.quantum_advisor
            expected_forecast_shape = (2, qa.forecast_horizon)
            expected_confidence_shape = (2, qa.forecast_horizon)

            if forecast.shape == expected_forecast_shape and confidence.shape == expected_confidence_shape:
                print(f"âœ… Quantum advisor prediction test passed")
                print(f"   Forecast shape: {forecast.shape}")
                print(f"   Confidence shape: {confidence.shape}")
                print(f"   Sample forecast values: {forecast[0].cpu().numpy()[:3]}")
                print(f"   Sample confidence values: {confidence[0].cpu().numpy()[:3]}")
                status['prediction_test'] = 'passed'
            else:
                issues.append(f"Shape mismatch: forecast={forecast.shape} (expected {expected_forecast_shape}), confidence={confidence.shape} (expected {expected_confidence_shape})")
                print(f"âš ï¸ Shape mismatch detected")
                status['prediction_test'] = 'shape_mismatch'

        except Exception as e:
            issues.append(f"Quantum advisor prediction test failed: {e}")
            print(f"âŒ Quantum advisor prediction test failed: {e}")
            status['prediction_test'] = 'failed'
            import traceback
            traceback.print_exc()

    # Summary
    print("\n" + "="*80)
    if issues:
        print(f"âŒ DIAGNOSTIC COMPLETE: {len(issues)} issues found")
        for i, issue in enumerate(issues, 1):
            print(f"   {i}. {issue}")
    else:
        print("âœ… DIAGNOSTIC COMPLETE: No issues found!")
    print("="*80 + "\n")

    return {'status': status, 'issues': issues}


def fix_quantum_advisor_integration(system):
    """
    Fix quantum advisor integration issues.
    Creates advisor if missing and links to all agents.
    """
    print("\n" + "="*80)
    print("ğŸ”§ FIXING QUANTUM ADVISOR INTEGRATION")
    print("="*80)

    fixed = []

    # Fix 1: Create quantum advisor if missing
    if not hasattr(system, 'quantum_advisor') or system.quantum_advisor is None:
        print("\n1ï¸âƒ£ Creating Quantum Forecasting Advisor...")
        try:

            state_dim = getattr(system, 'state_dim', 58)
            system.quantum_advisor = QuantumForecastingAdvisor(
                state_dim=state_dim,
                forecast_horizon=30
            )
            print(f"   âœ… Created quantum advisor (state_dim={state_dim})")
            fixed.append("Created quantum advisor")
        except Exception as e:
            print(f"   âŒ Failed to create quantum advisor: {e}")
            import traceback
            traceback.print_exc()
            return fixed
    else:
        print("\n1ï¸âƒ£ Quantum advisor already exists")

    # Fix 2: Link advisor to all agents
    if hasattr(system, 'agents') and hasattr(system, 'quantum_advisor'):
        print("\n2ï¸âƒ£ Linking quantum advisor to agents...")
        linked_count = 0

        for name, agent in system.agents.items():
            try:
                # Force set the quantum_advisor attribute
                agent.quantum_advisor = system.quantum_advisor
                linked_count += 1
                print(f"   âœ… [{name}] Linked to quantum advisor")
            except Exception as e:
                print(f"   âŒ [{name}] Failed to link: {e}")

        if linked_count > 0:
            fixed.append(f"Linked advisor to {linked_count} agents")
            print(f"\n   âœ… Linked quantum advisor to {linked_count}/{len(system.agents)} agents")

    # Fix 3: Enable quantum advisor logging in agents
    print("\n3ï¸âƒ£ Enabling quantum advisor logging...")
    if hasattr(system, 'agents'):
        for name, agent in system.agents.items():
            # Make sure agent has the quantum advisor attribute
            if not hasattr(agent, 'quantum_advisor'):
                agent.quantum_advisor = system.quantum_advisor
                print(f"   âœ… [{name}] Added quantum_advisor attribute")

            # Check if agent's predict method will use quantum advisor
            if hasattr(agent, 'predict'):
                print(f"   âœ… [{name}] Has predict method")

    print("\n" + "="*80)
    print(f"âœ… FIX COMPLETE: {len(fixed)} fixes applied")
    for fix in fixed:
        print(f"   - {fix}")
    print("="*80 + "\n")

    return fixed



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ROBUSTNESS UTILITIES (V8.5.4 - ADDED: November 04, 2025)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
V8.5.4 ROBUSTNESS ENHANCEMENTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
This section adds comprehensive robustness features to the quantum trading system:

1. Central configuration management
2. Safe tensor validation with NaN/Inf handling
3. Robust normalization with fallback
4. Error handling decorators
5. Automatic retry logic
6. System health monitoring
7. Graceful degradation

These utilities wrap critical operations to ensure the system remains stable
even under extreme conditions or unexpected inputs.
"""

from collections import defaultdict
from functools import wraps
import traceback



# ============================================================================
# ============================================================================
#                    QUANTUM DIVERSITY PRESERVATION MODULE
#                    INTEGRATED FROM quantum_diversity_solution.py
# ============================================================================
# SOLUTION: Systemic Action Homogenization in Multi-Agent Financial Decision Systems
#
# This module implements architectural solutions to prevent action-level degeneracy
# while preserving shared contextual information in multi-agent RL systems.
#
# Key Principles:
# 1. Agent-specific latent transformations (prevent z_shared dominance)
# 2. Diversity-aware loss functions (explicit action diversity penalties)
# 3. Confidence-weighted coordination (adaptive shared signal strength)
# 4. State differentiation mechanisms (timeframe-specific features)
#
# References:
# - Banerjee (1992): Herding behavior in financial markets
# - Oliehoek et al. (2016): Centralized training, decentralized execution
# - Tian et al. (2021): Representation collapse in contrastive learning
# ============================================================================

# ==============================================================================
# SOLUTION 1: AGENT-SPECIFIC LATENT PROJECTORS
# ==============================================================================

class AgentSpecificLatentProjector(nn.Module):
    """
    Transforms shared latent z_shared into agent-specific representations.
    
    Prevents shared signal dominance by learning agent-specific "views" of
    the global context.
    
    Mathematical formulation:
        z_agent_i = Proj_i(z_shared) + Î² * Ïµ_i
    
    where:
        - Proj_i is a learnable agent-specific transformation
        - Î² is a diversity strength parameter
        - Ïµ_i is agent-specific noise for exploration
    """
    
    def __init__(self, latent_dim: int, num_agents: int, 
                 diversity_strength: float = 0.2):
        super().__init__()
        self.latent_dim = latent_dim
        self.num_agents = num_agents
        self.diversity_strength = diversity_strength
        
        # Agent-specific projection networks
        self.agent_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(latent_dim, latent_dim),
                nn.LayerNorm(latent_dim),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(latent_dim, latent_dim),
                nn.Tanh()  # Bound output
            ) for _ in range(num_agents)
        ])
        
        # Diversity injection (learnable per-agent bias)
        self.diversity_bias = nn.Parameter(
            torch.randn(num_agents, latent_dim) * 0.01
        )
        
        logger.info(f"AgentSpecificLatentProjector: {num_agents} agents, "
                   f"diversity_strength={diversity_strength}")
    
    def forward(self, z_shared: torch.Tensor, 
                training: bool = True) -> torch.Tensor:
        """
        Args:
            z_shared: Shared latent (batch, latent_dim)
            training: Whether in training mode
            
        Returns:
            Agent-specific latents (num_agents, batch, latent_dim)
        """
        batch_size = z_shared.size(0)
        
        # Apply agent-specific transformations
        z_agents = []
        for i, projector in enumerate(self.agent_projectors):
            # Project shared latent
            z_agent = projector(z_shared)  # (batch, latent_dim)
            
            # Add agent-specific bias
            z_agent = z_agent + self.diversity_bias[i].unsqueeze(0)
            
            # Add exploration noise during training
            if training and self.diversity_strength > 0:
                noise = torch.randn_like(z_agent) * self.diversity_strength
                z_agent = z_agent + noise
            
            z_agents.append(z_agent)
        
        # Stack: (num_agents, batch, latent_dim)
        return torch.stack(z_agents, dim=0)


# ==============================================================================
# SOLUTION 2: DIVERSITY-AWARE LOSS FUNCTIONS
# ==============================================================================

class DiversityLoss(nn.Module):
    """
    Explicit loss term to penalize action homogenization.
    
    Combines multiple diversity metrics:
    1. Action entropy: Encourage uniform action distribution
    2. Pairwise disagreement: Reward agents taking different actions
    3. Q-value spread: Penalize collapse to identical Q-values
    
    Loss = Î»_entropy * L_entropy + Î»_disagree * L_disagree + Î»_spread * L_spread
    """
    
    def __init__(self, num_agents: int, action_dim: int = 2,
                 lambda_entropy: float = 0.1,
                 lambda_disagree: float = 0.5,
                 lambda_spread: float = 0.3):
        super().__init__()
        self.num_agents = num_agents
        self.action_dim = action_dim
        self.lambda_entropy = lambda_entropy
        self.lambda_disagree = lambda_disagree
        self.lambda_spread = lambda_spread
        
        # Target: uniform action distribution
        self.target_action_dist = 1.0 / action_dim
        
    def compute_action_entropy_loss(self, actions: torch.Tensor) -> torch.Tensor:
        """
        Penalize low entropy in action distribution.
        
        Args:
            actions: (batch, num_agents) - discrete actions
            
        Returns:
            entropy_loss: scalar
        """
        batch_size = actions.size(0)
        
        # Count action frequencies across agents
        action_counts = torch.zeros(batch_size, self.action_dim, 
                                    device=actions.device)
        
        for i in range(self.action_dim):
            action_counts[:, i] = (actions == i).float().sum(dim=1)
        
        # Normalize to probabilities
        action_probs = action_counts / self.num_agents
        
        # Compute entropy: -Î£ p(a) log p(a)
        entropy = -(action_probs * torch.log(action_probs + 1e-8)).sum(dim=1)
        
        # Target: maximum entropy (log(action_dim))
        max_entropy = np.log(self.action_dim)
        
        # Loss: encourage high entropy
        entropy_loss = max_entropy - entropy.mean()
        
        return entropy_loss
    
    def compute_disagreement_loss(self, actions: torch.Tensor) -> torch.Tensor:
        """
        Reward pairwise disagreement between agents.
        
        Args:
            actions: (batch, num_agents)
            
        Returns:
            disagreement_loss: scalar
        """
        batch_size = actions.size(0)
        
        # Compute pairwise disagreement rate
        disagreements = []
        for i in range(self.num_agents):
            for j in range(i + 1, self.num_agents):
                disagreement = (actions[:, i] != actions[:, j]).float()
                disagreements.append(disagreement)
        
        if not disagreements:
            return torch.tensor(0.0, device=actions.device)
        
        # Average disagreement rate
        avg_disagreement = torch.stack(disagreements).mean()
        
        # Loss: penalize high agreement (low disagreement)
        # Target: 50% disagreement for binary actions
        target_disagreement = 0.5
        disagreement_loss = (target_disagreement - avg_disagreement) ** 2
        
        return disagreement_loss
    
    def compute_qvalue_spread_loss(self, 
                                   q_values_all: List[torch.Tensor]) -> torch.Tensor:
        """
        V5.0.6 FIX: Penalize collapse of Q-values using log-variance.
        
        Previous implementation (BROKEN):
            spread_loss = 1.0 / (q_var + 1e-3)  # Explodes when varâ†’0
            spread_loss = clamp(spread_loss, max=10.0)  # Always at cap
        
        New implementation:
            spread_loss = -log(q_var + Îµ)  # Continuous gradient
            + range_penalty                # Penalize narrow Q-value range
            + correlation_penalty          # Penalize similar Q-value patterns
        
        Args:
            q_values_all: List of (batch, action_dim) tensors, one per agent
            
        Returns:
            spread_loss: scalar (lower = more diversity)
        """
        if not q_values_all:
            return torch.tensor(0.0)
        
        # Ensure all tensors are on same device and have same shape
        device = q_values_all[0].device
        
        # Normalize shapes
        normalized_q = []
        for q in q_values_all:
            if q.dim() == 1:
                q = q.unsqueeze(0)
            # Take first 2 Q-values (BUY/SELL)
            if q.size(-1) >= 2:
                q = q[..., :2]
            normalized_q.append(q)
        
        try:
            # Stack Q-values: (num_agents, batch, action_dim)
            q_stack = torch.stack(normalized_q, dim=0)
        except RuntimeError:
            # Shape mismatch - return safe default
            return torch.tensor(0.5, device=device)
        
        # ================================================================
        # Component 1: Log-Variance Loss
        # Encourages HIGH variance across agents (diversity)
        # ================================================================
        q_var = torch.var(q_stack, dim=0).mean()
        
        # Log-variance: when var is LOW, -log(var) is HIGH (penalty)
        # when var is HIGH, -log(var) is LOW (reward)
        log_var_loss = -torch.log(q_var + 0.01)
        
        # Clamp to reasonable range
        log_var_loss = torch.clamp(log_var_loss, min=-5.0, max=5.0)
        
        # ================================================================
        # Component 2: Q-Value Range Penalty
        # Penalize if all agents have similar Q-value magnitudes
        # ================================================================
        q_range = q_stack.max() - q_stack.min()
        # Penalize if range < 0.1 (agents predicting nearly identical values)
        range_penalty = F.relu(0.1 - q_range) * 10.0
        
        # ================================================================
        # Component 3: Per-Agent Spread Penalty
        # Each agent should have meaningful Q-value spread (BUY vs SELL)
        # ================================================================
        per_agent_spread = (q_stack[..., 0] - q_stack[..., 1]).abs().mean(dim=1)
        # Penalize agents with tiny spread (can't distinguish BUY from SELL)
        spread_penalty = F.relu(0.05 - per_agent_spread).mean() * 5.0
        
        # ================================================================
        # Component 4: Q-Value Correlation Penalty (NEW)
        # Penalize when agents' Q-values are highly correlated
        # ================================================================
        corr_penalty = torch.tensor(0.0, device=device)
        if len(normalized_q) >= 2:
            # Compute correlation between agent Q-values
            q_flat = q_stack.view(q_stack.size(0), -1)  # (agents, batch*actions)
            q_centered = q_flat - q_flat.mean(dim=1, keepdim=True)
            
            # Simplified correlation: cosine similarity
            norms = q_centered.norm(dim=1, keepdim=True).clamp(min=1e-6)
            q_normed = q_centered / norms
            
            # Correlation matrix
            corr_matrix = torch.mm(q_normed, q_normed.t())
            
            # Penalty for high off-diagonal correlation
            n_agents = corr_matrix.size(0)
            mask = 1 - torch.eye(n_agents, device=device)
            off_diag_corr = (corr_matrix.abs() * mask).sum() / mask.sum().clamp(min=1)
            
            # Penalize correlation > 0.5
            corr_penalty = F.relu(off_diag_corr - 0.5) * 2.0
        
        # ================================================================
        # Total Spread Loss
        # ================================================================
        spread_loss = log_var_loss + range_penalty + spread_penalty + corr_penalty
        
        return spread_loss
    
    def forward(self, actions: torch.Tensor, 
                q_values_all: List[torch.Tensor]) -> Tuple[torch.Tensor, dict]:
        """
        Compute total diversity loss.
        
        Args:
            actions: (batch, num_agents) - discrete actions
            q_values_all: List of Q-value tensors
            
        Returns:
            total_loss: scalar
            metrics: dict with component losses
        """
        # Component losses
        entropy_loss = self.compute_action_entropy_loss(actions)
        disagree_loss = self.compute_disagreement_loss(actions)
        spread_loss = self.compute_qvalue_spread_loss(q_values_all)
        
        # Weighted combination
        total_loss = (
            self.lambda_entropy * entropy_loss +
            self.lambda_disagree * disagree_loss +
            self.lambda_spread * spread_loss
        )
        
        metrics = {
            'diversity_loss': total_loss.item(),
            'entropy_loss': entropy_loss.item(),
            'disagreement_loss': disagree_loss.item(),
            'spread_loss': spread_loss.item()
        }
        
        return total_loss, metrics


# ==============================================================================
# V5.0.5: AGENT-LEVEL LEARNING DIAGNOSTICS
# ==============================================================================
# Purpose: Make agent-specific learning dynamics observable to diagnose:
#   - Policy collapse (agents becoming near-deterministic)
#   - Over-coordination (agents taking identical actions)  
#   - Gradient homogenisation (agents receiving similar gradient signals)
#
# Reference: Based on MARL diagnostic needs from CTDE literature
#   - Oliehoek et al. 2016 (Dec-POMDPs)
#   - Foerster et al. 2018 (COMA)
#   - Lowe et al. 2017 (MADDPG)
#   - Rashid et al. 2018 (QMIX)
# ==============================================================================

class AgentLevelDiagnostics:
    """
    Lightweight diagnostic utility for monitoring agent-specific learning dynamics
    in centralised-training-decentralised-execution (CTDE) multi-agent RL.
    
    Tracks and exposes per-agent metrics without altering training dynamics:
    - Policy entropy (detects collapse to deterministic policies)
    - Action correlation (detects over-coordination)
    - Gradient flow statistics (detects suppressed learning signals)
    - TD error distribution per agent (detects value function issues)
    - Q-value saturation (detects exploding/vanishing value estimates)
    
    All computations use already-computed values and are detached from gradients.
    """
    
    def __init__(self, agent_names: List[str], action_dim: int = 2, 
                 history_window: int = 100, enabled: bool = True):
        """
        Args:
            agent_names: List of agent identifiers
            action_dim: Number of actions (2 for BUY/SELL)
            history_window: Rolling window for historical metrics
            enabled: If False, all methods return immediately (no overhead)
        """
        self.agent_names = list(agent_names)
        self.action_dim = action_dim
        self.history_window = history_window
        self.enabled = enabled
        
        # Per-agent rolling histories (for trend detection)
        self._init_histories()
        
        # Thresholds for alerts
        self.policy_collapse_threshold = 0.05  # Entropy below this = collapsed
        self.overcoordination_threshold = 0.90  # Action correlation above this = warning
        self.gradient_suppression_threshold = 1e-6  # Grad norm below this = suppressed
        self.q_saturation_threshold = 10.0  # |Q| above this = saturated
        
        # Diagnostic counters
        self.step_count = 0
        self.collapse_warnings = {name: 0 for name in agent_names}
        self.coordination_warnings = 0
        
        logger.info(f"AgentLevelDiagnostics initialized for {len(agent_names)} agents")
    
    def _init_histories(self):
        """Initialize per-agent historical buffers."""
        self.entropy_history = {name: deque(maxlen=self.history_window) 
                               for name in self.agent_names}
        self.action_history = {name: deque(maxlen=self.history_window) 
                              for name in self.agent_names}
        self.q_mean_history = {name: deque(maxlen=self.history_window) 
                              for name in self.agent_names}
        self.q_std_history = {name: deque(maxlen=self.history_window) 
                             for name in self.agent_names}
        self.grad_norm_history = {name: deque(maxlen=self.history_window) 
                                 for name in self.agent_names}
        self.td_error_history = {name: deque(maxlen=self.history_window) 
                                for name in self.agent_names}
    
    def compute_policy_entropy(self, action_probs: torch.Tensor) -> float:
        """
        Compute entropy of action distribution (in bits).
        
        Low entropy = policy collapse (near-deterministic).
        Max entropy for binary actions = 1.0 bit (uniform).
        
        Args:
            action_probs: (batch, action_dim) probabilities, or logits
            
        Returns:
            Mean entropy across batch (detached, scalar)
        """
        if not self.enabled:
            return 0.0
            
        with torch.no_grad():
            if action_probs.dim() == 1:
                action_probs = action_probs.unsqueeze(0)
            
            # Convert logits to probs if needed
            if action_probs.sum(dim=-1).mean().item() > 1.5:  # Likely logits
                action_probs = torch.softmax(action_probs, dim=-1)
            
            # Clamp for numerical stability
            probs_clamped = action_probs.clamp(min=1e-8, max=1.0 - 1e-8)
            
            # Shannon entropy in bits
            entropy = -torch.sum(probs_clamped * torch.log2(probs_clamped), dim=-1)
            
            return entropy.mean().item()
    
    def compute_action_correlation(self, agent_actions: Dict[str, int]) -> Dict[str, float]:
        """
        Compute pairwise action agreement between agents.
        
        High correlation = over-coordination (agents not using their local info).
        
        Args:
            agent_actions: {agent_name: action_int}
            
        Returns:
            {
                'mean_agreement': float,  # Average pairwise agreement
                'max_agreement_pair': (agent1, agent2, agreement),
                'unique_actions': int  # Number of distinct actions taken
            }
        """
        if not self.enabled:
            return {'mean_agreement': 0.5, 'unique_actions': self.action_dim}
        
        actions = list(agent_actions.values())
        n = len(actions)
        
        if n < 2:
            return {'mean_agreement': 0.0, 'unique_actions': len(set(actions))}
        
        # Update action histories
        for name, action in agent_actions.items():
            if name in self.action_history:
                self.action_history[name].append(action)
        
        # Compute pairwise agreement
        agreements = []
        max_agree = (None, None, 0.0)
        
        agent_names = list(agent_actions.keys())
        for i in range(len(agent_names)):
            for j in range(i + 1, len(agent_names)):
                name_i, name_j = agent_names[i], agent_names[j]
                
                # Recent agreement rate
                hist_i = list(self.action_history.get(name_i, []))
                hist_j = list(self.action_history.get(name_j, []))
                
                if len(hist_i) > 5 and len(hist_j) > 5:
                    min_len = min(len(hist_i), len(hist_j))
                    agree_rate = sum(1 for a, b in zip(hist_i[-min_len:], hist_j[-min_len:]) 
                                    if a == b) / min_len
                else:
                    agree_rate = 1.0 if actions[i] == actions[j] else 0.0
                
                agreements.append(agree_rate)
                
                if agree_rate > max_agree[2]:
                    max_agree = (name_i, name_j, agree_rate)
        
        return {
            'mean_agreement': np.mean(agreements) if agreements else 0.0,
            'max_agreement_pair': max_agree,
            'unique_actions': len(set(actions))
        }
    
    def compute_gradient_stats(self, agent_grads: Dict[str, torch.Tensor]) -> Dict[str, Dict[str, float]]:
        """
        Compute gradient statistics per agent (from actor networks).
        
        Used to detect gradient suppression or explosion.
        
        Args:
            agent_grads: {agent_name: flattened_gradient_tensor}
            
        Returns:
            {agent_name: {'norm': float, 'mean': float, 'std': float, 'max': float}}
        """
        if not self.enabled:
            return {}
        
        stats = {}
        
        with torch.no_grad():
            for name, grad in agent_grads.items():
                if grad is None or grad.numel() == 0:
                    stats[name] = {'norm': 0.0, 'mean': 0.0, 'std': 0.0, 'max': 0.0}
                    continue
                
                grad_np = grad.detach().cpu().numpy().flatten()
                
                norm = float(np.linalg.norm(grad_np))
                stats[name] = {
                    'norm': norm,
                    'mean': float(np.mean(grad_np)),
                    'std': float(np.std(grad_np)),
                    'max': float(np.max(np.abs(grad_np)))
                }
                
                # Update history
                if name in self.grad_norm_history:
                    self.grad_norm_history[name].append(norm)
        
        return stats
    
    def compute_td_error_stats(self, 
                              agent_td_errors: Dict[str, torch.Tensor]) -> Dict[str, Dict[str, float]]:
        """
        Compute TD error statistics per agent.
        
        Large TD errors = value function not converged.
        Tiny TD errors = possible overfitting or value collapse.
        
        Args:
            agent_td_errors: {agent_name: (batch,) TD errors}
            
        Returns:
            {agent_name: {'mean': float, 'std': float, 'max': float, 'abs_mean': float}}
        """
        if not self.enabled:
            return {}
        
        stats = {}
        
        with torch.no_grad():
            for name, td in agent_td_errors.items():
                if td is None or (isinstance(td, torch.Tensor) and td.numel() == 0):
                    stats[name] = {'mean': 0.0, 'std': 0.0, 'max': 0.0, 'abs_mean': 0.0}
                    continue
                
                if isinstance(td, torch.Tensor):
                    td_np = td.detach().cpu().numpy().flatten()
                else:
                    td_np = np.array(td).flatten()
                
                abs_mean = float(np.mean(np.abs(td_np)))
                stats[name] = {
                    'mean': float(np.mean(td_np)),
                    'std': float(np.std(td_np)),
                    'max': float(np.max(np.abs(td_np))),
                    'abs_mean': abs_mean
                }
                
                # Update history
                if name in self.td_error_history:
                    self.td_error_history[name].append(abs_mean)
        
        return stats
    
    def detect_policy_collapse(self, 
                              agent_entropies: Dict[str, float]) -> Dict[str, bool]:
        """
        Detect which agents have collapsed to near-deterministic policies.
        
        Args:
            agent_entropies: {agent_name: entropy_value}
            
        Returns:
            {agent_name: is_collapsed}
        """
        if not self.enabled:
            return {name: False for name in self.agent_names}
        
        collapsed = {}
        for name, entropy in agent_entropies.items():
            is_collapsed = entropy < self.policy_collapse_threshold
            collapsed[name] = is_collapsed
            
            if is_collapsed:
                self.collapse_warnings[name] = self.collapse_warnings.get(name, 0) + 1
            
            # Update history
            if name in self.entropy_history:
                self.entropy_history[name].append(entropy)
        
        return collapsed
    
    def detect_overcoordination(self, correlation_stats: Dict) -> bool:
        """
        Detect if agents are over-coordinated (taking identical actions too often).
        
        Args:
            correlation_stats: Output from compute_action_correlation
            
        Returns:
            True if over-coordination detected
        """
        if not self.enabled:
            return False
        
        mean_agreement = correlation_stats.get('mean_agreement', 0.0)
        
        if mean_agreement > self.overcoordination_threshold:
            self.coordination_warnings += 1
            return True
        
        return False
    
    def detect_gradient_suppression(self, 
                                   grad_stats: Dict[str, Dict[str, float]]) -> Dict[str, bool]:
        """
        Detect which agents have suppressed gradient signals.
        
        Args:
            grad_stats: Output from compute_gradient_stats
            
        Returns:
            {agent_name: is_suppressed}
        """
        if not self.enabled:
            return {name: False for name in self.agent_names}
        
        suppressed = {}
        for name, stats in grad_stats.items():
            norm = stats.get('norm', 0.0)
            suppressed[name] = norm < self.gradient_suppression_threshold
        
        return suppressed
    
    def get_per_agent_summary(self, 
                             q_values_per_agent: Dict[str, np.ndarray],
                             actions_per_agent: Dict[str, int],
                             td_errors_per_agent: Optional[Dict[str, np.ndarray]] = None,
                             grad_norms_per_agent: Optional[Dict[str, float]] = None
                             ) -> Dict[str, Dict]:
        """
        Generate comprehensive per-agent diagnostic summary.
        
        Args:
            q_values_per_agent: {agent_name: q_values array}
            actions_per_agent: {agent_name: action_int}
            td_errors_per_agent: Optional {agent_name: td_errors array}
            grad_norms_per_agent: Optional {agent_name: gradient_norm}
            
        Returns:
            {agent_name: {
                'q_mean', 'q_std', 'q_min', 'q_max', 'q_spread',
                'action', 'entropy', 'is_collapsed',
                'td_error_mean', 'grad_norm', 'is_grad_suppressed'
            }}
        """
        if not self.enabled:
            return {}
        
        self.step_count += 1
        summary = {}
        
        for name in self.agent_names:
            agent_summary = {}
            
            # Q-value statistics
            if name in q_values_per_agent:
                q = np.array(q_values_per_agent[name])
                agent_summary['q_mean'] = float(np.mean(q))
                agent_summary['q_std'] = float(np.std(q))
                agent_summary['q_min'] = float(np.min(q))
                agent_summary['q_max'] = float(np.max(q))
                agent_summary['q_spread'] = float(np.max(q) - np.min(q))
                agent_summary['q_saturated'] = abs(agent_summary['q_max']) > self.q_saturation_threshold
                
                # Compute policy entropy from Q-values (softmax)
                q_tensor = torch.tensor(q, dtype=torch.float32)
                probs = torch.softmax(q_tensor, dim=-1)
                agent_summary['entropy'] = self.compute_policy_entropy(probs)
                agent_summary['is_collapsed'] = agent_summary['entropy'] < self.policy_collapse_threshold
                
                # Update histories
                if name in self.q_mean_history:
                    self.q_mean_history[name].append(agent_summary['q_mean'])
                if name in self.q_std_history:
                    self.q_std_history[name].append(agent_summary['q_std'])
            
            # Action taken
            if name in actions_per_agent:
                agent_summary['action'] = actions_per_agent[name]
            
            # TD error
            if td_errors_per_agent and name in td_errors_per_agent:
                td = np.array(td_errors_per_agent[name])
                agent_summary['td_error_mean'] = float(np.mean(np.abs(td)))
                agent_summary['td_error_std'] = float(np.std(td))
            
            # Gradient norm
            if grad_norms_per_agent and name in grad_norms_per_agent:
                agent_summary['grad_norm'] = grad_norms_per_agent[name]
                agent_summary['is_grad_suppressed'] = (
                    agent_summary['grad_norm'] < self.gradient_suppression_threshold
                )
            
            summary[name] = agent_summary
        
        return summary
    
    def print_diagnostic_report(self, summary: Dict[str, Dict], 
                               correlation_stats: Optional[Dict] = None,
                               verbose: bool = False):
        """
        Print formatted diagnostic report to stdout.
        
        Args:
            summary: Output from get_per_agent_summary
            correlation_stats: Output from compute_action_correlation
            verbose: If True, print full details; if False, only warnings
        """
        if not self.enabled:
            return
        
        has_warnings = False
        
        # Check for issues
        collapsed_agents = [name for name, s in summary.items() if s.get('is_collapsed', False)]
        suppressed_agents = [name for name, s in summary.items() if s.get('is_grad_suppressed', False)]
        saturated_agents = [name for name, s in summary.items() if s.get('q_saturated', False)]
        
        is_overcoordinated = False
        if correlation_stats:
            is_overcoordinated = correlation_stats.get('mean_agreement', 0) > self.overcoordination_threshold
        
        has_warnings = collapsed_agents or suppressed_agents or saturated_agents or is_overcoordinated
        
        if verbose or has_warnings:
            print("\n" + "="*100)
            print(f"{'ğŸ”¬ AGENT-LEVEL LEARNING DIAGNOSTICS':^100}")
            print(f"{'Step ' + str(self.step_count):^100}")
            print("="*100)
            
            # Policy collapse warnings
            if collapsed_agents:
                print("\nâš ï¸  POLICY COLLAPSE DETECTED:")
                for name in collapsed_agents:
                    entropy = summary[name].get('entropy', 0)
                    print(f"   - {name}: entropy={entropy:.4f} (threshold={self.policy_collapse_threshold})")
            
            # Over-coordination warning
            if is_overcoordinated and correlation_stats:
                print("\nâš ï¸  OVER-COORDINATION DETECTED:")
                print(f"   Mean action agreement: {correlation_stats['mean_agreement']:.2%}")
                pair = correlation_stats.get('max_agreement_pair')
                if pair and pair[0]:
                    print(f"   Most correlated: {pair[0]} â†” {pair[1]} ({pair[2]:.2%})")
            
            # Gradient suppression warnings
            if suppressed_agents:
                print("\nâš ï¸  GRADIENT SUPPRESSION DETECTED:")
                for name in suppressed_agents:
                    grad = summary[name].get('grad_norm', 0)
                    print(f"   - {name}: grad_norm={grad:.2e} (threshold={self.gradient_suppression_threshold:.2e})")
            
            # Q-value saturation warnings
            if saturated_agents:
                print("\nâš ï¸  Q-VALUE SATURATION DETECTED:")
                for name in saturated_agents:
                    q_max = summary[name].get('q_max', 0)
                    print(f"   - {name}: |Q|_max={q_max:.2f} (threshold={self.q_saturation_threshold})")
            
            if verbose:
                # Full per-agent table
                print("\nğŸ“Š PER-AGENT DIAGNOSTIC SUMMARY")
                print("-"*110)
                header = f"{'Agent':<12} {'Q-Mean':>9} {'Q-Std':>9} {'Q-Spread':>9} {'Entropy':>9} {'TD-Err':>9} {'Grad':>10} {'Status':<15}"
                print(header)
                print("-"*110)
                
                for name in sorted(summary.keys()):
                    s = summary[name]
                    
                    # Status indicators
                    status = []
                    if s.get('is_collapsed'):
                        status.append("ğŸ”´COLL")
                    if s.get('is_grad_suppressed'):
                        status.append("ğŸŸ¡SUPP")
                    if s.get('q_saturated'):
                        status.append("ğŸŸ SAT")
                    if not status:
                        status.append("âœ…OK")
                    
                    row = (f"{name:<12} "
                           f"{s.get('q_mean', 0):>9.4f} "
                           f"{s.get('q_std', 0):>9.4f} "
                           f"{s.get('q_spread', 0):>9.4f} "
                           f"{s.get('entropy', 0):>9.4f} "
                           f"{s.get('td_error_mean', 0):>9.4f} "
                           f"{s.get('grad_norm', 0):>10.2e} "
                           f"{' '.join(status):<15}")
                    print(row)
                
                print("-"*110)
                
                # Correlation matrix (if many agents)
                if correlation_stats:
                    print(f"\nğŸ”— Action Correlation: {correlation_stats['mean_agreement']:.2%} mean agreement")
                    print(f"   Unique actions this step: {correlation_stats.get('unique_actions', '?')}/{self.action_dim}")
            
            print("="*100 + "\n")
        
        elif self.step_count % 100 == 0:
            # Periodic mini-summary even without warnings
            print(f"[Diagnostics Step {self.step_count}] "
                  f"Collapsed: {len(collapsed_agents)}, "
                  f"Suppressed: {len(suppressed_agents)}, "
                  f"Saturated: {len(saturated_agents)}")
    
    def get_metrics_dict(self, summary: Dict[str, Dict],
                        correlation_stats: Optional[Dict] = None) -> Dict:
        """
        Return metrics as a flat dictionary for logging/tensorboard.
        
        Args:
            summary: Output from get_per_agent_summary
            correlation_stats: Output from compute_action_correlation
            
        Returns:
            Flat dict with prefixed keys like 'agent/10m/q_mean'
        """
        if not self.enabled:
            return {}
        
        metrics = {}
        
        # Per-agent metrics
        for name, s in summary.items():
            prefix = f"agent/{name}"
            for key, value in s.items():
                if isinstance(value, (int, float)):
                    metrics[f"{prefix}/{key}"] = value
        
        # Aggregate metrics
        if summary:
            metrics['agents/mean_entropy'] = np.mean([s.get('entropy', 0) for s in summary.values()])
            metrics['agents/mean_q_std'] = np.mean([s.get('q_std', 0) for s in summary.values()])
            metrics['agents/num_collapsed'] = sum(1 for s in summary.values() if s.get('is_collapsed', False))
        
        # Coordination metrics
        if correlation_stats:
            metrics['coordination/mean_agreement'] = correlation_stats.get('mean_agreement', 0)
            metrics['coordination/unique_actions'] = correlation_stats.get('unique_actions', 0)
        
        return metrics


# Global diagnostic instance (initialized lazily)
_agent_diagnostics: Optional[AgentLevelDiagnostics] = None

def get_agent_diagnostics() -> Optional[AgentLevelDiagnostics]:
    """Get the global agent diagnostics instance."""
    global _agent_diagnostics
    return _agent_diagnostics

def init_agent_diagnostics(agent_names: List[str], action_dim: int = 2,
                          enabled: bool = True) -> AgentLevelDiagnostics:
    """Initialize the global agent diagnostics instance."""
    global _agent_diagnostics
    if _agent_diagnostics is None:
        _agent_diagnostics = AgentLevelDiagnostics(
            agent_names=agent_names,
            action_dim=action_dim,
            enabled=enabled
        )
        print(f"âœ… AgentLevelDiagnostics initialized for {len(agent_names)} agents")
    return _agent_diagnostics


print("âœ… Agent-Level Learning Diagnostics loaded (V5.0.5)")
# ==============================================================================
# END OF AGENT-LEVEL DIAGNOSTICS
# ==============================================================================


# ==============================================================================
# V5.0.6: EXPLORATION MANAGER - Anti-Collapse Exploration Control
# ==============================================================================
# Purpose: Centralized exploration management with entropy-aware adaptation
# 
# Key Features:
#   - Temperature annealing for softmax action selection
#   - Per-agent epsilon based on policy entropy
#   - Automatic collapse detection and exploration boost
#   - Training step tracking for schedules
#
# References:
#   - Haarnoja et al. 2018 (SAC entropy regularization)
#   - Lowe et al. 2017 (MADDPG exploration)
# ==============================================================================

class ExplorationManager:
    """
    Manages exploration parameters for all agents with entropy-aware adaptation.
    
    Collapsed agents (entropy < threshold) receive boosted exploration to
    break out of local minima. Temperature annealing encourages exploration
    early in training, then shifts to exploitation.
    """
    
    def __init__(self, 
                 agent_names: List[str],
                 initial_temperature: float = 1.0,
                 min_temperature: float = 0.1,
                 temperature_decay: float = 0.9995,
                 base_epsilon: float = 0.1,
                 collapse_epsilon: float = 0.3,
                 entropy_threshold: float = 0.15,
                 severe_collapse_threshold: float = 0.05):
        """
        Args:
            agent_names: List of agent identifiers
            initial_temperature: Starting temperature for softmax
            min_temperature: Minimum temperature (prevents pure greedy)
            temperature_decay: Per-step decay rate
            base_epsilon: Default epsilon for healthy agents
            collapse_epsilon: Boosted epsilon for collapsed agents
            entropy_threshold: Entropy below this = collapsed
            severe_collapse_threshold: Entropy below this = severe collapse
        """
        self.agent_names = list(agent_names)
        self.initial_temperature = initial_temperature
        self.min_temperature = min_temperature
        self.temperature_decay = temperature_decay
        self.base_epsilon = base_epsilon
        self.collapse_epsilon = collapse_epsilon
        self.entropy_threshold = entropy_threshold
        self.severe_collapse_threshold = severe_collapse_threshold
        
        # Current state
        self.temperature = initial_temperature
        self.training_step = 0
        
        # Per-agent tracking
        self.agent_epsilons = {name: base_epsilon for name in agent_names}
        self.agent_entropies = {name: 1.0 for name in agent_names}  # Start optimistic
        self.collapse_counters = {name: 0 for name in agent_names}
        
        # Statistics
        self.random_actions_count = 0
        self.total_actions_count = 0
        
        logger.info(f"ExplorationManager initialized: temp={initial_temperature}, "
                   f"base_Îµ={base_epsilon}, collapse_Îµ={collapse_epsilon}")
    
    def get_temperature(self) -> float:
        """Get current temperature for softmax action selection."""
        return max(self.min_temperature, self.temperature)
    
    def get_epsilon(self, agent_name: str, agent_entropy: Optional[float] = None) -> float:
        """
        Get per-agent epsilon based on entropy.
        
        Collapsed agents get high epsilon to force exploration.
        Healthy agents get base epsilon.
        
        Args:
            agent_name: Agent identifier
            agent_entropy: Current policy entropy (optional, uses cached if None)
            
        Returns:
            Epsilon value for this agent
        """
        if agent_entropy is not None:
            self.agent_entropies[agent_name] = agent_entropy
        
        entropy = self.agent_entropies.get(agent_name, 1.0)
        
        if entropy < self.severe_collapse_threshold:
            # Severe collapse: maximum exploration
            epsilon = self.collapse_epsilon
            self.collapse_counters[agent_name] = self.collapse_counters.get(agent_name, 0) + 1
        elif entropy < self.entropy_threshold:
            # Moderate collapse: boosted exploration
            # Linear interpolation between base and collapse epsilon
            collapse_severity = 1.0 - (entropy / self.entropy_threshold)
            epsilon = self.base_epsilon + collapse_severity * (self.collapse_epsilon - self.base_epsilon)
            self.collapse_counters[agent_name] = self.collapse_counters.get(agent_name, 0) + 1
        else:
            # Healthy: base exploration with decay
            decay_factor = self.temperature_decay ** self.training_step
            epsilon = max(0.01, self.base_epsilon * decay_factor)
            self.collapse_counters[agent_name] = 0
        
        self.agent_epsilons[agent_name] = epsilon
        return epsilon
    
    def select_action_with_exploration(self, 
                                       q_values: torch.Tensor,
                                       agent_name: str,
                                       agent_entropy: Optional[float] = None,
                                       training: bool = True) -> Tuple[int, Dict]:
        """
        Full action selection with temperature + epsilon-greedy.
        
        Process:
        1. Check epsilon for random action
        2. Apply temperature scaling to Q-values
        3. Sample from softmax distribution (if training) or argmax (if inference)
        
        Args:
            q_values: Q-value tensor (action_dim,) or (1, action_dim)
            agent_name: Agent identifier
            agent_entropy: Current policy entropy
            training: Whether in training mode
            
        Returns:
            (action_index, metadata_dict)
        """
        self.total_actions_count += 1
        
        # Flatten Q-values
        if q_values.dim() > 1:
            q_values = q_values.flatten()
        
        q_values = q_values.detach()
        action_dim = q_values.size(-1)
        
        # Get exploration parameters
        temperature = self.get_temperature()
        epsilon = self.get_epsilon(agent_name, agent_entropy) if training else 0.0
        
        metadata = {
            'temperature': temperature,
            'epsilon': epsilon,
            'was_random': False,
            'action_probs': None
        }
        
        # Epsilon-greedy check
        if training and random.random() < epsilon:
            action = random.randint(0, action_dim - 1)
            self.random_actions_count += 1
            metadata['was_random'] = True
            return action, metadata
        
        # Temperature-scaled softmax
        if training and temperature > 0.01:
            # Softmax with temperature
            scaled_q = q_values / temperature
            # Numerical stability
            scaled_q = scaled_q - scaled_q.max()
            probs = F.softmax(scaled_q, dim=-1)
            metadata['action_probs'] = probs.cpu().numpy()
            
            # Sample from distribution
            action = torch.multinomial(probs, 1).item()
        else:
            # Greedy action
            action = q_values.argmax(dim=-1).item()
        
        return action, metadata
    
    def step(self):
        """Update exploration parameters after training step."""
        self.training_step += 1
        
        # Temperature decay
        self.temperature = max(
            self.min_temperature,
            self.initial_temperature * (self.temperature_decay ** self.training_step)
        )
    
    def get_random_action_rate(self) -> float:
        """Get the percentage of actions that were random."""
        if self.total_actions_count == 0:
            return 0.0
        return self.random_actions_count / self.total_actions_count
    
    def reset_stats(self):
        """Reset action statistics."""
        self.random_actions_count = 0
        self.total_actions_count = 0
    
    def get_collapsed_agents(self) -> List[str]:
        """Get list of agents currently in collapsed state."""
        return [name for name, eps in self.agent_epsilons.items() 
                if eps > self.base_epsilon * 1.5]
    
    def get_stats(self) -> Dict:
        """Get exploration statistics for logging."""
        return {
            'temperature': self.get_temperature(),
            'mean_epsilon': np.mean(list(self.agent_epsilons.values())),
            'max_epsilon': max(self.agent_epsilons.values()),
            'random_action_rate': self.get_random_action_rate(),
            'collapsed_agents': len(self.get_collapsed_agents()),
            'training_step': self.training_step,
            'per_agent_epsilon': dict(self.agent_epsilons),
            'per_agent_entropy': dict(self.agent_entropies)
        }


# Global exploration manager instance
_exploration_manager: Optional[ExplorationManager] = None

def get_exploration_manager() -> Optional[ExplorationManager]:
    """Get the global exploration manager instance."""
    global _exploration_manager
    return _exploration_manager

def init_exploration_manager(agent_names: List[str], **kwargs) -> ExplorationManager:
    """Initialize the global exploration manager."""
    global _exploration_manager
    if _exploration_manager is None:
        _exploration_manager = ExplorationManager(agent_names, **kwargs)
        print(f"âœ… ExplorationManager initialized for {len(agent_names)} agents")
    return _exploration_manager


# ==============================================================================
# V5.0.6: GATED ENTANGLEMENT - Local vs Shared Signal Balance
# ==============================================================================
# Purpose: Learn when to use local Q-values vs entangled Q-values
# 
# Problem: z_shared signal can overwhelm agent-specific state information,
#          causing agents to ignore their local observations.
#
# Solution: Learnable gate that caps z_shared influence at max_gate (30%)
#           to ensure local signals always dominate.
#
# References:
#   - Rashid et al. 2018 (QMIX - value decomposition)
#   - Sunehag et al. 2018 (VDN - mixing networks)
# ==============================================================================

class GatedEntanglement(nn.Module):
    """
    Learnable gate that balances local Q-values vs entangled Q-values.
    
    The gate learns per-state when coordination is beneficial vs when
    the agent should trust its local prediction. A max_gate cap ensures
    local signals are always dominant.
    
    q_final = (1 - gate) * q_local + gate * q_entangled
    where gate âˆˆ [0, max_gate]
    """
    
    def __init__(self, state_dim: int, hidden_dim: int = 64, max_gate: float = 0.3):
        """
        Args:
            state_dim: Dimension of state input
            hidden_dim: Hidden layer size for gate network
            max_gate: Maximum gate value (caps z_shared influence)
        """
        super().__init__()
        self.state_dim = state_dim
        self.max_gate = max_gate
        
        # Gate network: state â†’ [0, 1] â†’ scaled to [0, max_gate]
        self.gate_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # Initialize with low gate values (favor local)
        self._init_low_gate()
        
        logger.info(f"GatedEntanglement: state_dim={state_dim}, max_gate={max_gate}")
    
    def _init_low_gate(self):
        """Initialize gate network to output low values initially."""
        with torch.no_grad():
            # Set final layer bias negative to start with low gate
            final_layer = self.gate_net[-2]  # Linear before Sigmoid
            if hasattr(final_layer, 'bias') and final_layer.bias is not None:
                final_layer.bias.fill_(-1.0)  # Sigmoid(-1) â‰ˆ 0.27
    
    def forward(self, 
                local_q: torch.Tensor, 
                entangled_q: torch.Tensor, 
                state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Blend local and entangled Q-values based on learned gate.
        
        Args:
            local_q: Q-values from local prediction (batch, action_dim)
            entangled_q: Q-values from entangled prediction (batch, action_dim)
            state: Current state (batch, state_dim)
            
        Returns:
            blended_q: Gated combination (batch, action_dim)
            gate_value: The gate values used (batch, 1) - for logging
        """
        # Compute gate value
        gate_raw = self.gate_net(state)  # (batch, 1)
        gate = gate_raw * self.max_gate  # Scale to [0, max_gate]
        
        # Blend Q-values
        blended_q = (1 - gate) * local_q + gate * entangled_q
        
        return blended_q, gate
    
    def get_gate_stats(self, state: torch.Tensor) -> Dict[str, float]:
        """Get gate statistics for a batch of states."""
        with torch.no_grad():
            gate_raw = self.gate_net(state)
            gate = gate_raw * self.max_gate
            
            return {
                'gate_mean': gate.mean().item(),
                'gate_std': gate.std().item(),
                'gate_min': gate.min().item(),
                'gate_max': gate.max().item()
            }


print("âœ… ExplorationManager and GatedEntanglement loaded (V5.0.6)")
# ==============================================================================
# END OF V5.0.6 ANTI-COLLAPSE MECHANISMS
# ==============================================================================


# ==============================================================================
# SOLUTION 3: ADAPTIVE Z_SHARED WEIGHTING
# ==============================================================================

class AdaptiveCoordinationModule(nn.Module):
    """
    Dynamically adjusts z_shared influence based on agent confidence.
    
    Key insight: Agents with low confidence should rely more on shared signal,
    while confident agents should trust their own predictions.
    
    Weight calculation:
        w_i = Ïƒ(ConfNet(state_i, q_i))
        q_final_i = (1 - w_i) * q_independent_i + w_i * q_entangled_i
    """
    
    def __init__(self, state_dim: int, action_dim: int = 2):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Confidence estimation network
        self.confidence_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # Output in [0, 1]
        )
        
    def estimate_confidence(self, state: torch.Tensor, 
                          q_values: torch.Tensor) -> torch.Tensor:
        """
        Estimate agent's confidence in its prediction.
        
        High confidence indicators:
        - Large Q-value spread (clear preference)
        - Consistent recent predictions
        - Low state uncertainty
        
        Args:
            state: (batch, state_dim)
            q_values: (batch, action_dim)
            
        Returns:
            confidence: (batch, 1) in [0, 1]
        """
        # Concatenate state and Q-values
        features = torch.cat([state, q_values], dim=-1)
        
        # Estimate confidence
        confidence = self.confidence_net(features)
        
        return confidence
    
    def forward(self, state: torch.Tensor,
                q_independent: torch.Tensor,
                q_entangled: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Adaptively blend independent and entangled Q-values.
        
        Args:
            state: (batch, state_dim)
            q_independent: (batch, action_dim) - without z_shared
            q_entangled: (batch, action_dim) - with z_shared
            
        Returns:
            q_final: (batch, action_dim)
            weight: (batch, 1) - coordination weight used
        """
        # Estimate confidence
        confidence = self.estimate_confidence(state, q_independent)
        
        # Inverse relationship: high confidence â†’ low z_shared weight
        z_weight = 1.0 - confidence
        
        # Blend Q-values
        q_final = (confidence * q_independent + 
                   z_weight * q_entangled)
        
        return q_final, z_weight


# ==============================================================================
# SOLUTION 4: TIMEFRAME-SPECIFIC STATE AUGMENTATION
# ==============================================================================

class TimeframeStateAugmenter(nn.Module):
    """
    Adds timeframe-specific features to prevent state cache staleness.
    
    Injects:
    1. Timeframe embeddings (learned representations)
    2. Temporal position encoding
    3. Volatility indicators per timeframe
    """
    
    def __init__(self, state_dim: int, timeframes: List[str],
                 augment_dim: int = 8):
        super().__init__()
        self.state_dim = state_dim
        self.timeframes = timeframes
        self.augment_dim = augment_dim
        self.num_timeframes = len(timeframes)
        
        # Learnable timeframe embeddings
        self.timeframe_embeddings = nn.Embedding(
            self.num_timeframes, augment_dim
        )
        
        # Projection to match state dimension
        self.projection = nn.Linear(augment_dim, state_dim)
        
        # Timeframe index mapping
        self.tf_to_idx = {tf: i for i, tf in enumerate(timeframes)}
        
    def forward(self, state: torch.Tensor, 
                timeframe: str) -> torch.Tensor:
        """
        Augment state with timeframe-specific information.
        
        Args:
            state: (batch, state_dim)
            timeframe: str - timeframe identifier
            
        Returns:
            augmented_state: (batch, state_dim)
        """
        batch_size = state.size(0)
        
        # Get timeframe index
        tf_idx = self.tf_to_idx.get(timeframe, 0)
        
        # Get timeframe embedding
        tf_emb = self.timeframe_embeddings(
            torch.tensor(tf_idx, device=state.device)
        ).unsqueeze(0).expand(batch_size, -1)
        
        # Project to state dimension
        tf_features = self.projection(tf_emb)
        
        # Add to state (residual connection)
        augmented_state = state + tf_features
        
        return augmented_state


# ==============================================================================
# SOLUTION 5: DIVERSITY METRICS TRACKER
# ==============================================================================

class DiversityMetricsTracker:
    """
    Tracks diversity metrics over time for monitoring and debugging.
    """
    
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.metrics_history = deque(maxlen=window_size)
        self.action_distribution_history = deque(maxlen=window_size)
        
    def update(self, actions: torch.Tensor, q_values: List[torch.Tensor], 
               diversity_metrics: dict):
        """
        Update metrics with new batch.
        
        Args:
            actions: (batch, num_agents)
            q_values: List of Q-value tensors
            diversity_metrics: dict from DiversityLoss
        """
        # Store metrics
        self.metrics_history.append(diversity_metrics)
        
        # Compute action distribution
        if actions.numel() > 0:
            action_counts = torch.bincount(
                actions.flatten().long(), 
                minlength=2
            ).float()
            action_dist = action_counts / action_counts.sum()
            self.action_distribution_history.append(action_dist.cpu().numpy())
    
    def get_summary(self) -> dict:
        """
        Get summary statistics over recent window.
        
        Returns:
            dict with mean/std of metrics
        """
        if not self.metrics_history:
            return {}
        
        summary = {}
        
        # Aggregate metrics
        keys = self.metrics_history[0].keys()
        for key in keys:
            values = [m[key] for m in self.metrics_history if key in m]
            if values:
                summary[f'{key}_mean'] = np.mean(values)
                summary[f'{key}_std'] = np.std(values)
        
        # Action distribution stats
        if self.action_distribution_history:
            action_dists = np.array(list(self.action_distribution_history))
            summary['action_0_ratio_mean'] = action_dists[:, 0].mean()
            summary['action_1_ratio_mean'] = action_dists[:, 1].mean() if action_dists.shape[1] > 1 else 0
            summary['action_balance'] = 1.0 - abs(summary['action_0_ratio_mean'] - 0.5) * 2
        
        return summary
    
    def log_summary(self, step: int = 0):
        """
        Log summary to console/logger.
        """
        summary = self.get_summary()
        if summary:
            logger.info(f"[Step {step}] Diversity Metrics:")
            for key, value in summary.items():
                logger.info(f"  {key}: {value:.4f}")


# ==============================================================================
# DIVERSITY-AWARE TRAINING HELPER
# ==============================================================================

def compute_diversity_loss_for_training(
    actions_batch: torch.Tensor,
    q_values_list: List[torch.Tensor],
    diversity_loss_module: DiversityLoss,
    diversity_coef: float = 0.2
) -> Tuple[torch.Tensor, dict]:
    """
    Helper function to compute diversity loss during training.
    
    Args:
        actions_batch: (batch, num_agents) tensor of actions
        q_values_list: List of Q-value tensors, one per agent
        diversity_loss_module: DiversityLoss instance
        diversity_coef: coefficient for diversity loss
        
    Returns:
        scaled_loss: diversity_coef * diversity_loss
        metrics: dict with diversity metrics
    """
    div_loss, metrics = diversity_loss_module(actions_batch, q_values_list)
    scaled_loss = diversity_coef * div_loss
    metrics['diversity_loss_scaled'] = scaled_loss.item()
    return scaled_loss, metrics


print("âœ… Quantum Diversity Preservation Module loaded")
# ============================================================================
# END OF QUANTUM DIVERSITY PRESERVATION MODULE
# ============================================================================


# ============================================================================
# ============================================================================
#                    V5.0.3: NON-BLOCKING TRAINING EXECUTOR
# ============================================================================
# Solves the "stale data" issue by running training in a separate thread.
# The main event loop (Ably message handler) is never blocked during 
# gradient computations, ensuring continuous data flow.
# ============================================================================
import concurrent.futures
import threading
from queue import Queue, Empty

class AsyncTrainingExecutor:
    """
    Runs training in a separate thread pool to prevent blocking 
    the Ably event loop during gradient computations.
    
    Problem Solved:
    - Training takes 20-60+ seconds
    - During this time, Ably messages cannot be processed
    - State cache becomes stale (no new data)
    
    Solution:
    - Training runs in background thread
    - Main thread continues processing Ably messages
    - Data ingestion never interrupted
    """
    
    def __init__(self, max_workers: int = 1):
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=max_workers,
            thread_name_prefix="TrainingWorker"
        )
        self.training_in_progress = threading.Event()
        self.training_queue = Queue(maxsize=1)  # Only 1 pending training
        self.last_training_result = None
        self.last_training_time = None
        self.training_count = 0
        self._shutdown = False
        self._current_future = None
        self._lock = threading.Lock()
        
    def submit_training(self, trainer, buffer, batch_size: int = 32, 
                       training_number: int = 0) -> bool:
        """
        Submit training job without blocking.
        Returns True if submitted, False if training already in progress.
        """
        if self.training_in_progress.is_set():
            return False  # Skip - training already running
            
        self.training_in_progress.set()
        self.training_count = training_number
        
        self._current_future = self.executor.submit(
            self._run_training_safe,
            trainer,
            buffer,
            batch_size,
            training_number
        )
        self._current_future.add_done_callback(self._on_training_complete)
        return True
    
    def _run_training_safe(self, trainer, buffer, batch_size, training_number):
        """Execute training with error handling - runs in SEPARATE THREAD."""
        import time
        start_time = time.time()
        
        try:
            # Print training start banner
            print("\n" + "ğŸ“"*40)
            print(f"TRAINING #{training_number} EXECUTING (BACKGROUND THREAD)")
            print(f"   Thread: {threading.current_thread().name}")
            print("ğŸ“"*40)
            
            # This is the actual training - runs without blocking main loop
            result = trainer.train_step(batch_size=batch_size)
            
            elapsed = time.time() - start_time
            
            # Print completion
            print("\n" + "="*80)
            print(f"ğŸ“ TRAINING #{training_number} COMPLETE (background)")
            print("="*80)
            
            # V5.0.6: Update exploration parameters after training
            exploration_mgr = get_exploration_manager()
            if exploration_mgr is not None:
                exploration_mgr.step()
                stats = exploration_mgr.get_stats()
                print(f"ğŸ² Exploration: temp={stats['temperature']:.3f}, mean_Îµ={stats['mean_epsilon']:.3f}, collapsed={stats['collapsed_agents']}")
            
            if result:
                actor_loss = result.get('actor_loss', result.get('total_loss', 'N/A'))
                critic_loss = result.get('critic_loss', 'N/A')
                
                if actor_loss != 'N/A':
                    try:
                        print(f"ğŸ“‰ ACTOR LOSS:  {actor_loss:.6f}")
                    except:
                        print(f"ğŸ“‰ ACTOR LOSS:  {actor_loss}")
                if critic_loss != 'N/A':
                    try:
                        print(f"ğŸ“‰ CRITIC LOSS: {critic_loss:.6f}")
                    except:
                        print(f"ğŸ“‰ CRITIC LOSS: {critic_loss}")
                
                # V5.0.6: Check per-agent collapse and log entropy bonus
                if 'entropy_bonus' in result:
                    print(f"ğŸ”¥ Entropy Bonus: {result['entropy_bonus']:.6f}")
                        
                print(f"â±ï¸  Training time: {elapsed:.1f}s")
                print(f"ğŸ“Š All metrics: {result}")
            else:
                print("âš ï¸  No training stats returned")
                
            print("="*80 + "\n")
            
            if result:
                result['training_time_background'] = elapsed
                result['training_number'] = training_number
            
            return result
            
        except Exception as e:
            elapsed = time.time() - start_time
            print("\n" + "âŒ"*40)
            print(f"TRAINING #{training_number} FAILED! (background)")
            print(f"Error: {e}")
            print(f"Time before failure: {elapsed:.1f}s")
            print("âŒ"*40 + "\n")
            
            import traceback
            traceback.print_exc()
            
            return {'error': str(e), 'training_number': training_number}
    
    def _on_training_complete(self, future):
        """Callback when training finishes."""
        with self._lock:
            self.training_in_progress.clear()
            self.last_training_time = time.time()
            try:
                self.last_training_result = future.result()
            except Exception as e:
                self.last_training_result = {'error': str(e)}
    
    def is_training(self) -> bool:
        """Check if training is currently in progress."""
        return self.training_in_progress.is_set()
    
    def get_last_result(self) -> Optional[Dict]:
        """Get the result from the last completed training."""
        with self._lock:
            return self.last_training_result
    
    def shutdown(self, wait: bool = True):
        """Shutdown the executor."""
        self._shutdown = True
        self.executor.shutdown(wait=wait)
        
    def get_status(self) -> Dict:
        """Get current status of the training executor."""
        return {
            'is_training': self.is_training(),
            'training_count': self.training_count,
            'last_training_time': self.last_training_time,
            'has_last_result': self.last_training_result is not None,
            'shutdown': self._shutdown
        }


# Global instance - will be initialized in quantum_integrated_main
_async_training_executor: Optional[AsyncTrainingExecutor] = None

def get_async_trainer() -> Optional[AsyncTrainingExecutor]:
    """Get the global async training executor instance."""
    global _async_training_executor
    return _async_training_executor

def init_async_trainer(max_workers: int = 1) -> AsyncTrainingExecutor:
    """Initialize the global async training executor."""
    global _async_training_executor
    if _async_training_executor is None:
        _async_training_executor = AsyncTrainingExecutor(max_workers=max_workers)
        print("âœ… AsyncTrainingExecutor initialized (non-blocking training enabled)")
    return _async_training_executor


print("âœ… Non-Blocking Training Executor loaded (V5.0.3)")
# ============================================================================
# END OF NON-BLOCKING TRAINING EXECUTOR
# ============================================================================


# ============================================================================
# ============================================================================
#                    V5.0.4: NON-BLOCKING SIGNAL PUBLISHER
# ============================================================================
# Solves the "Ably publish timeout" issue that blocks the async event loop.
# When signal publishing times out (30-240s), the entire event loop freezes,
# preventing feature message processing and causing state cache staleness.
# ============================================================================

class AsyncSignalPublisher:
    """
    Non-blocking signal publisher that prevents Ably timeouts from
    freezing the async event loop.
    
    Problem Solved:
    - await channel.publish() can timeout for 30-240+ seconds
    - During this time, ALL async operations are blocked
    - Feature messages queue but aren't processed
    - State cache goes stale
    
    Solution:
    - Signal publishing runs in a background thread
    - Main event loop continues processing features
    - Timeouts don't affect data ingestion
    - Failed publishes are logged but don't block
    """
    
    def __init__(self, timeout: float = 10.0, max_retries: int = 2):
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=1,
            thread_name_prefix="SignalPublisher"
        )
        self.timeout = timeout
        self.max_retries = max_retries
        self.publish_count = 0
        self.success_count = 0
        self.timeout_count = 0
        self.error_count = 0
        self._lock = threading.Lock()
        self._shutdown = False
        self._last_publish_time = None
        self._pending_future = None
        
    def submit_signal(self, channel, event_name: str, data: dict, 
                     on_success=None, on_failure=None) -> bool:
        """
        Submit signal for background publishing - returns immediately.
        
        Args:
            channel: Ably channel object
            event_name: Event name (e.g., "new-final-signal")
            data: Signal data dictionary
            on_success: Optional callback on success
            on_failure: Optional callback on failure
            
        Returns:
            True if submitted, False if publisher is shutdown
        """
        if self._shutdown:
            return False
            
        with self._lock:
            self.publish_count += 1
            count = self.publish_count
            
        # Submit to background thread
        self._pending_future = self.executor.submit(
            self._publish_with_retry,
            channel,
            event_name,
            data,
            count,
            on_success,
            on_failure
        )
        
        return True
    
    def _publish_with_retry(self, channel, event_name, data, count, 
                           on_success, on_failure):
        """Execute publish with retry logic in background thread."""
        import asyncio
        import time as time_module
        
        for attempt in range(self.max_retries + 1):
            try:
                start_time = time_module.time()
                
                # Create new event loop for this thread
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    # Run publish with timeout
                    loop.run_until_complete(
                        asyncio.wait_for(
                            channel.publish(event_name, data),
                            timeout=self.timeout
                        )
                    )
                    
                    elapsed = time_module.time() - start_time
                    
                    with self._lock:
                        self.success_count += 1
                        self._last_publish_time = time_module.time()
                    
                    print(f"[{time_module.strftime('%H:%M:%S')}] âœ… âœ… Signal published ({elapsed:.1f}s)")
                    
                    if on_success:
                        try:
                            on_success(data)
                        except:
                            pass
                    
                    return True
                    
                finally:
                    loop.close()
                    
            except asyncio.TimeoutError:
                elapsed = time_module.time() - start_time
                
                with self._lock:
                    self.timeout_count += 1
                
                if attempt < self.max_retries:
                    print(f"[{time_module.strftime('%H:%M:%S')}] âš ï¸ Signal publish timeout ({elapsed:.1f}s) - retry {attempt + 1}/{self.max_retries}")
                else:
                    print(f"[{time_module.strftime('%H:%M:%S')}] âŒ Signal publish FAILED after {self.max_retries + 1} attempts (timeout)")
                    print(f"   âš¡ Main loop NOT blocked - features continue flowing")
                    
                    if on_failure:
                        try:
                            on_failure(data, "timeout")
                        except:
                            pass
                    
                    return False
                    
            except Exception as e:
                with self._lock:
                    self.error_count += 1
                
                print(f"[{time_module.strftime('%H:%M:%S')}] âŒ Signal publish error: {e}")
                print(f"   âš¡ Main loop NOT blocked - features continue flowing")
                
                if on_failure:
                    try:
                        on_failure(data, str(e))
                    except:
                        pass
                
                return False
        
        return False
    
    def get_stats(self) -> Dict:
        """Get publishing statistics."""
        with self._lock:
            total = self.publish_count
            success_rate = (self.success_count / total * 100) if total > 0 else 0
            return {
                'total_published': total,
                'successful': self.success_count,
                'timeouts': self.timeout_count,
                'errors': self.error_count,
                'success_rate': f"{success_rate:.1f}%",
                'last_publish_time': self._last_publish_time
            }
    
    def is_publishing(self) -> bool:
        """Check if a publish is currently in progress."""
        if self._pending_future is None:
            return False
        return not self._pending_future.done()
    
    def shutdown(self, wait: bool = True):
        """Shutdown the publisher."""
        self._shutdown = True
        self.executor.shutdown(wait=wait)


# Global instance
_async_signal_publisher: Optional[AsyncSignalPublisher] = None

def get_signal_publisher() -> Optional[AsyncSignalPublisher]:
    """Get the global async signal publisher instance."""
    global _async_signal_publisher
    return _async_signal_publisher

def init_signal_publisher(timeout: float = 10.0, max_retries: int = 2) -> AsyncSignalPublisher:
    """Initialize the global async signal publisher."""
    global _async_signal_publisher
    if _async_signal_publisher is None:
        _async_signal_publisher = AsyncSignalPublisher(timeout=timeout, max_retries=max_retries)
        print("âœ… AsyncSignalPublisher initialized (non-blocking publishing enabled)")
    return _async_signal_publisher


print("âœ… Non-Blocking Signal Publisher loaded (V5.0.4)")
# ============================================================================
# END OF NON-BLOCKING SIGNAL PUBLISHER
# ============================================================================


# ============================================================================
# ============================================================================
#                    MULTI-INSTRUMENT CHECKPOINT SYSTEM
#                    INTEGRATED FROM checkpoint_class.txt
# ============================================================================
# This checkpoint system supports multiple trading instruments:
# Volatility: V10, V25, V50, V75, V100, V150, V250
# Jump: J10, J25, J50, J75, J100
# Boom: BOOM300, BOOM500, BOOM1000
# Crash: CRASH300, CRASH500, CRASH1000
# Step: STEP100, STEP200, STEP500
# Range Break: RB100, RB200
# ============================================================================
import torch
import torch.nn as nn
import json
import os
import time
import threading
import queue
from pathlib import Path
from typing import Dict, Optional, List, Any, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import logging

try:
    from google.cloud import storage
    GCS_AVAILABLE = True
except ImportError:
    GCS_AVAILABLE = False
    print("âš ï¸  google-cloud-storage not installed. GCS features disabled.")

logger = logging.getLogger(__name__)


# ============================================================================
# SUPPORTED INSTRUMENTS
# ============================================================================

class Instrument(Enum):
    """Supported trading instruments."""
    # Volatility Indices
    V10 = "V10"
    V25 = "V25"
    V50 = "V50"
    V75 = "V75"
    V100 = "V100"
    V150 = "V150"
    V250 = "V250"

    # Jump Indices
    J10 = "J10"
    J25 = "J25"
    J50 = "J50"
    J75 = "J75"
    J100 = "J100"

    # Boom Indices
    BOOM300 = "BOOM300"
    BOOM500 = "BOOM500"
    BOOM1000 = "BOOM1000"

    # Crash Indices
    CRASH300 = "CRASH300"
    CRASH500 = "CRASH500"
    CRASH1000 = "CRASH1000"

    # Step Indices
    STEP100 = "STEP100"
    STEP200 = "STEP200"
    STEP500 = "STEP500"

    # Range Break
    RB100 = "RB100"
    RB200 = "RB200"


# Shorthand mapping for convenience
INSTRUMENT_ALIASES = {
    'volatility75': Instrument.V75,
    'volatility100': Instrument.V100,
    'vol75': Instrument.V75,
    'vol100': Instrument.V100,
    'jump25': Instrument.J25,
    'jump50': Instrument.J50,
    'boom1000': Instrument.BOOM1000,
    'crash1000': Instrument.CRASH1000,
}


def get_instrument(name: str) -> Instrument:
    """Get instrument from string name."""
    # Check direct enum match
    name_upper = name.upper().replace(' ', '').replace('_', '')

    for inst in Instrument:
        if inst.value == name_upper or inst.name == name_upper:
            return inst

    # Check aliases
    name_lower = name.lower().replace(' ', '').replace('_', '')
    if name_lower in INSTRUMENT_ALIASES:
        return INSTRUMENT_ALIASES[name_lower]

    raise ValueError(f"Unknown instrument: {name}. Supported: {[i.value for i in Instrument]}")


# ============================================================================
# TIMEFRAME DEFINITIONS
# ============================================================================

TIMEFRAME_LENGTHS = {
    # === High-Frequency Zone (Volatility Capture) ===
    'xs': 5,     # tick
    's': 10,     # ultra
    'm': 20,     # fast

    # === Critical Trading Zones ===
    'l': 30,     # scalp
    'xl': 60,    # 1min
    'xxl': 120,  # 2min

    # === Structure & Regime Detection ===
    '5m': 300,   # 5min
    '10m': 600,  # 10min
}
TIMEFRAMES =['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']


# ============================================================================
# STATUS ENUMS AND DATACLASSES
# ============================================================================

class LoadStatus(Enum):
    SUCCESS = "success"
    FAILED = "failed"
    FALLBACK = "fallback"
    SKIPPED = "skipped"
    REINITIALIZED = "reinitialized"
    PARTIAL = "partial"
    NOT_FOUND = "not_found"
    LOADED = "loaded"  # Add this missing value


class RecoveryMode(Enum):
    STRICT = "strict"
    FALLBACK = "fallback"
    SKIP = "skip"
    REINIT = "reinit"
    PARTIAL = "partial"


@dataclass
@dataclass
class ComponentLoadResult:
    name: str
    status: LoadStatus
    success: bool = None  # Add this line
    source_checkpoint: Optional[str] = None
    source_step: Optional[int] = None
    error: Optional[str] = None
    keys_loaded: int = 0
    keys_total: int = 0
    fallback_attempts: int = 0
    load_time_ms: float = 0
    
    def __post_init__(self):
        # Auto-set success based on status if not provided
        if self.success is None:
            self.success = self.status in [LoadStatus.SUCCESS, LoadStatus.FALLBACK, 
                                         LoadStatus.REINITIALIZED, LoadStatus.LOADED]

@dataclass
class CheckpointLoadResult:
    success: bool
    instrument: str
    voyage_number: int
    training_steps: int
    episode_number: int
    timestamp: str
    components: Dict[str, ComponentLoadResult] = field(default_factory=dict)
    total_load_time: float = 0
    file_size_mb: float = 0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    checkpoints_used: Dict[str, str] = field(default_factory=dict)

    def get_summary(self) -> Dict[str, Any]:
        succeeded = sum(1 for c in self.components.values() if c.status == LoadStatus.SUCCESS)
        fallback = sum(1 for c in self.components.values() if c.status == LoadStatus.FALLBACK)
        failed = sum(1 for c in self.components.values() if c.status == LoadStatus.FAILED)
        return {
            'total_components': len(self.components),
            'succeeded': succeeded,
            'fallback': fallback,
            'failed': failed,
            'success_rate': (succeeded + fallback) / max(1, len(self.components)) * 100
        }


@dataclass
class ComponentSaveResult:
    name: str
    success: bool
    error: Optional[str] = None
    size_bytes: int = 0


@dataclass
class CheckpointSaveResult:
    success: bool
    instrument: str
    voyage_number: int
    training_steps: int
    local_path: str
    gcs_path: Optional[str] = None
    components: Dict[str, ComponentSaveResult] = field(default_factory=dict)
    total_save_time: float = 0
    file_size_mb: float = 0
    errors: List[str] = field(default_factory=list)


# ============================================================================
# COMPONENT REINITIALIZER
# ============================================================================

class ComponentReinitializer:
    @staticmethod
    def reinit_module(module: nn.Module, name: str) -> bool:
        try:
            for m in module.modules():
                if hasattr(m, 'reset_parameters'):
                    m.reset_parameters()
            print(f"         ğŸ”„ Reinitialized {name}")
            return True
        except Exception as e:
            print(f"         âŒ Failed to reinit {name}: {e}")
            return False


# ============================================================================
# MULTI-INSTRUMENT CHECKPOINT MANAGER
# ============================================================================
from huggingface_hub import HfApi, login, hf_hub_download, list_repo_files
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import json
import time
import queue
import threading
import os
from datetime import datetime
import torch
import torch.nn as nn


class MultiInstrumentCheckpointManager:
    """
    Checkpoint manager supporting multiple trading instruments.
    HF version - maintains exact same interface as GCS version.

    Structure:
        hf://repo_id/
        â”œâ”€â”€ master_index.json
        â”œâ”€â”€ V75/
        â”‚   â”œâ”€â”€ instrument_index.json
        â”‚   â”œâ”€â”€ v1/
        â”‚   â”‚   â””â”€â”€ K1RL_V75_v1_step_*.pt
        â”‚   â””â”€â”€ v2/
        â”œâ”€â”€ V100/
        â”‚   â””â”€â”€ ...
        â””â”€â”€ J25/
            â””â”€â”€ ...
    """

    MASTER_INDEX_FILENAME = "master_index.json"
    INSTRUMENT_INDEX_FILENAME = "instrument_index.json"
    VOYAGE_INDEX_FILENAME = "voyage_index.json"

    def __init__(
        self,
        local_checkpoint_dir: str = "./saves",
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CHANGED: GCS params â†’ HF params
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        hf_repo_id: Optional[str] = None,      # Was: gcs_bucket_name
        hf_token: Optional[str] = None,        # Was: gcs_credential_path
        # (gcs_prefix removed - not needed for HF)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        autosave_interval_minutes: int = 5,
        max_checkpoints_per_voyage: int = 5,
        enable_async_save: bool = True,
        enable_compression: bool = True,
        current_instrument: str = "V75",
        current_voyage: int = 1,
        max_fallback_attempts: int = 3,
        allow_cross_voyage_fallback: bool = True,
        default_recovery_mode = None,  # RecoveryMode.FALLBACK
        verbose: bool = True
    ):
        """Initialize multi-instrument checkpoint manager with HF backend."""

        self.local_base_dir = Path(local_checkpoint_dir).resolve()
        self.local_base_dir.mkdir(parents=True, exist_ok=True)

        # HF settings (replaces GCS settings)
        self.hf_repo_id = hf_repo_id
        self.hf_api = None

        self.max_checkpoints_per_voyage = max_checkpoints_per_voyage
        self.enable_async_save = enable_async_save
        self.enable_compression = enable_compression
        self.max_fallback_attempts = max_fallback_attempts
        self.allow_cross_voyage_fallback = allow_cross_voyage_fallback
        self.default_recovery_mode = default_recovery_mode
        self.verbose = verbose

        self._current_instrument = get_instrument(current_instrument)
        self._current_voyage = current_voyage

        # HF setup (replaces GCS setup)
        if hf_repo_id:
            self._setup_hf(hf_token)

        self.master_index = self._load_or_create_master_index()
        self._ensure_instrument_folder(self._current_instrument)
        self._ensure_voyage_folder(self._current_instrument, self._current_voyage)

        # Async save queue
        self.save_queue = queue.Queue() if enable_async_save else None
        self.save_worker_thread = None
        if enable_async_save:
            self._start_save_worker()

        # Autosave
        self.autosave_interval = autosave_interval_minutes * 60
        self.last_save_time = time.time()
        self.autosave_enabled = False
        self.autosave_thread = None

        # Stats
        self.save_count = 0
        self.load_count = 0
        self.fallback_count = 0

        # Checkpoint cache
        self._checkpoint_cache: Dict[str, Dict] = {}
        self._cache_max_size = 3

        # Reinitializer
        self.reinitializer = ComponentReinitializer()

        self._print_init_banner()

    def _print_init_banner(self):
        print(f"\n{'='*80}")
        print(f"âš¡ K1RL MULTI-INSTRUMENT CHECKPOINT MANAGER V7.0 (Hugging Face)")
        print(f"{'='*80}")
        print(f"   Local Base: {self.local_base_dir}")
        print(f"   HF Repo: {self.hf_repo_id or 'Not configured'}")
        print(f"   Current Instrument: {self._current_instrument.value}")
        print(f"   Current Voyage: v{self._current_voyage}")
        if self.default_recovery_mode:
            print(f"   Recovery Mode: {self.default_recovery_mode.value}")
        print(f"   Known Instruments: {self.list_all_instruments()}")
        print(f"{'='*80}\n")

    # ========================================================================
    # HF SETUP (Replaces _setup_gcs)
    # ========================================================================

    def _setup_hf(self, token: Optional[str] = None):
        """Setup Hugging Face (replaces _setup_gcs)."""
        try:
            if token:
                login(token=token, add_to_git_credential=False)
                print(f"   âœ… HF: Logged in with token")
            else:
                print(f"   ğŸ” HF: Please login...")
                login(add_to_git_credential=False)
                print(f"   âœ… HF: Logged in interactively")

            self.hf_api = HfApi()
            print(f"   âœ… HF Connected: {self.hf_repo_id}")
            print(f"      View at: https://huggingface.co/datasets/{self.hf_repo_id}")

        except Exception as e:
            print(f"   âŒ HF Setup Failed: {e}")
            import traceback
            traceback.print_exc()
            self.hf_api = None

    def set_gcs_bucket(self, hf_api, bucket_name: str = None):
        """Set HF API (maintains compatibility with old method name)."""
        self.hf_api = hf_api
        if bucket_name:
            self.hf_repo_id = bucket_name
        print(f"   âœ… HF API injected directly: {self.hf_repo_id}")

    # ========================================================================
    # PATH HELPERS (Keep exact same interface)
    # ========================================================================

    def _get_instrument_folder_name(self, instrument) -> str:
        """Get folder name for instrument."""
        return instrument.value

    def _get_voyage_folder_name(self, voyage_number: int) -> str:
        return f"v{voyage_number}"

    def _get_local_instrument_dir(self, instrument) -> Path:
        """Get local directory for an instrument."""
        return self.local_base_dir / self._get_instrument_folder_name(instrument)

    def _get_local_voyage_dir(self, instrument, voyage_number: int) -> Path:
        """Get local directory for a voyage within an instrument."""
        return self._get_local_instrument_dir(instrument) / self._get_voyage_folder_name(voyage_number)

    def _get_gcs_instrument_prefix(self, instrument) -> str:
        """Get HF prefix for instrument (renamed internally but keeps method name)."""
        return f"{self._get_instrument_folder_name(instrument)}"

    def _get_gcs_voyage_prefix(self, instrument, voyage_number: int) -> str:
        """Get HF prefix for voyage (renamed internally but keeps method name)."""
        return f"{self._get_gcs_instrument_prefix(instrument)}/{self._get_voyage_folder_name(voyage_number)}"

    def _get_checkpoint_filename(self, instrument, voyage_number: int, training_steps: int) -> str:
        """Get checkpoint filename."""
        return f"K1RL_{instrument.value}_v{voyage_number}_step_{training_steps}.pt"

    def _get_checkpoint_id(self, instrument, voyage_number: int, training_steps: int) -> str:
        """Get unique checkpoint identifier."""
        return f"{instrument.value}_v{voyage_number}_step_{training_steps}"

    def _ensure_instrument_folder(self, instrument):
        """Ensure instrument folder exists."""
        self._get_local_instrument_dir(instrument).mkdir(parents=True, exist_ok=True)

    def _ensure_voyage_folder(self, instrument, voyage_number: int):
        """Ensure voyage folder exists within instrument."""
        self._get_local_voyage_dir(instrument, voyage_number).mkdir(parents=True, exist_ok=True)

    # ========================================================================
    # INDEX MANAGEMENT (Adapted for HF)
    # ========================================================================

    def _get_master_index_local_path(self) -> Path:
        return self.local_base_dir / self.MASTER_INDEX_FILENAME

    def _get_instrument_index_local_path(self, instrument) -> Path:
        return self._get_local_instrument_dir(instrument) / self.INSTRUMENT_INDEX_FILENAME

    def _get_voyage_index_local_path(self, instrument, voyage_number: int) -> Path:
        return self._get_local_voyage_dir(instrument, voyage_number) / self.VOYAGE_INDEX_FILENAME

    def _load_or_create_master_index(self) -> Dict[str, Any]:
        local_path = self._get_master_index_local_path()
        if local_path.exists():
            try:
                with open(local_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {
            "version": "7.0-hf",
            "created": datetime.now().isoformat(),
            "instruments": {}
        }

    def save_master_index(self, upload_to_gcs: bool = True):
        """Save master index (param name kept for compatibility)."""
        self.master_index["last_updated"] = datetime.now().isoformat()

        with open(self._get_master_index_local_path(), 'w') as f:
            json.dump(self.master_index, f, indent=2)

        if upload_to_gcs and self.hf_api:  # upload_to_gcs works for HF too
            try:
                self.hf_api.upload_file(
                    path_or_fileobj=str(self._get_master_index_local_path()),
                    path_in_repo=self.MASTER_INDEX_FILENAME,
                    repo_id=self.hf_repo_id,
                    repo_type="dataset"
                )
            except:
                pass

    def _load_instrument_index(self, instrument) -> Dict[str, Any]:
        local_path = self._get_instrument_index_local_path(instrument)
        if local_path.exists():
            try:
                with open(local_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {
            "instrument": instrument.value,
            "created": datetime.now().isoformat(),
            "current_voyage": 1,
            "voyages": {}
        }

    def _save_instrument_index(self, instrument, index: Dict, upload_to_gcs: bool = True):
        """Save instrument index (param name kept for compatibility)."""
        index["last_updated"] = datetime.now().isoformat()
        self._ensure_instrument_folder(instrument)

        with open(self._get_instrument_index_local_path(instrument), 'w') as f:
            json.dump(index, f, indent=2)

        if upload_to_gcs and self.hf_api:
            try:
                path_in_repo = f"{self._get_gcs_instrument_prefix(instrument)}/{self.INSTRUMENT_INDEX_FILENAME}"
                self.hf_api.upload_file(
                    path_or_fileobj=str(self._get_instrument_index_local_path(instrument)),
                    path_in_repo=path_in_repo,
                    repo_id=self.hf_repo_id,
                    repo_type="dataset"
                )
            except:
                pass

    def _rebuild_voyage_index_from_gcs(self, instrument, voyage_number: int) -> Dict[str, Any]:
        """Rebuild voyage index by scanning HF (method name kept for compatibility)."""
        if not self.hf_api:
            return {"instrument": instrument.value, "voyage": voyage_number, "checkpoints": []}

        try:
            import re as re_module
            prefix = f"{self._get_gcs_voyage_prefix(instrument, voyage_number)}/"
            files = list_repo_files(self.hf_repo_id, repo_type="dataset")

            checkpoints = []
            for file_path in files:
                if not file_path.startswith(prefix):
                    continue

                filename = file_path.split('/')[-1]

                # Match checkpoint files
                pattern = rf'K1RL_{re_module.escape(instrument.value)}_v{voyage_number}_step_(\d+)\.pt$'
                match = re_module.match(pattern, filename)

                if match:
                    step = int(match.group(1))
                    checkpoints.append({
                        "step": step,
                        "file": filename,
                        "hf_path": file_path,
                        "updated": None
                    })

            checkpoints.sort(key=lambda x: x['step'])

            return {
                "instrument": instrument.value,
                "voyage": voyage_number,
                "checkpoints": checkpoints,
                "rebuilt": True,
                "rebuilt_at": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"   âŒ Error rebuilding index from HF: {e}")
            return {"instrument": instrument.value, "voyage": voyage_number, "checkpoints": []}

    def _load_voyage_index(self, instrument, voyage_number: int) -> Dict[str, Any]:
        """Load voyage index, syncing from HF if needed."""
        local_path = self._get_voyage_index_local_path(instrument, voyage_number)

        # Try local first
        if local_path.exists():
            try:
                with open(local_path, 'r') as f:
                    return json.load(f)
            except:
                pass

        # Try downloading from HF if available
        if self.hf_api:
            try:
                hf_path = f"{self._get_gcs_voyage_prefix(instrument, voyage_number)}/{self.VOYAGE_INDEX_FILENAME}"

                downloaded = hf_hub_download(
                    repo_id=self.hf_repo_id,
                    filename=hf_path,
                    repo_type="dataset",
                    local_dir=str(self.local_base_dir),
                    local_dir_use_symlinks=False
                )

                with open(downloaded, 'r') as f:
                    index_data = json.load(f)
                print(f"   âœ… Loaded voyage index with {len(index_data.get('checkpoints', []))} checkpoints")
                return index_data

            except:
                # Index doesn't exist, try rebuilding from files
                print(f"   ğŸ“‹ No voyage index found, scanning HF for checkpoints...")
                rebuilt_index = self._rebuild_voyage_index_from_gcs(instrument, voyage_number)
                if rebuilt_index.get("checkpoints"):
                    print(f"   âœ… Rebuilt index with {len(rebuilt_index['checkpoints'])} checkpoints")
                    self._save_voyage_index(instrument, voyage_number, rebuilt_index, upload_to_gcs=True)
                    return rebuilt_index

        return {"instrument": instrument.value, "voyage": voyage_number, "checkpoints": []}

    def _save_voyage_index(self, instrument, voyage_number: int, voyage_index: Dict, upload_to_gcs: bool = True):
        """Save voyage index (param name kept for compatibility)."""
        voyage_index["last_updated"] = datetime.now().isoformat()
        self._ensure_voyage_folder(instrument, voyage_number)

        with open(self._get_voyage_index_local_path(instrument, voyage_number), 'w') as f:
            json.dump(voyage_index, f, indent=2)

        if upload_to_gcs and self.hf_api:
            try:
                path_in_repo = f"{self._get_gcs_voyage_prefix(instrument, voyage_number)}/{self.VOYAGE_INDEX_FILENAME}"
                self.hf_api.upload_file(
                    path_or_fileobj=str(self._get_voyage_index_local_path(instrument, voyage_number)),
                    path_in_repo=path_in_repo,
                    repo_id=self.hf_repo_id,
                    repo_type="dataset"
                )
            except:
                pass

    # ========================================================================
    # INSTRUMENT & VOYAGE MANAGEMENT (Keep exact same interface)
    # ========================================================================

    def get_current_instrument(self):
        """Get current instrument."""
        return self._current_instrument

    def get_current_instrument_name(self) -> str:
        """Get current instrument name as string."""
        return self._current_instrument.value

    def set_current_instrument(self, instrument: str, voyage: int = 1, save_index: bool = True):
        """Change to a different instrument."""
        self._current_instrument = get_instrument(instrument)
        self._current_voyage = voyage

        self._ensure_instrument_folder(self._current_instrument)
        self._ensure_voyage_folder(self._current_instrument, self._current_voyage)

        inst_name = self._current_instrument.value
        if inst_name not in self.master_index.get("instruments", {}):
            self.master_index.setdefault("instruments", {})[inst_name] = {
                "created": datetime.now().isoformat(),
                "current_voyage": voyage,
                "total_voyages": 1
            }
        else:
            self.master_index["instruments"][inst_name]["current_voyage"] = voyage

        if save_index:
            self.save_master_index()

        print(f"   ğŸ”„ Switched to: {self._current_instrument.value} / v{self._current_voyage}")

    def get_current_voyage(self) -> int:
        return self._current_voyage

    def set_current_voyage(self, voyage_number: int, save_index: bool = True):
        """Set current voyage within current instrument."""
        self._current_voyage = voyage_number
        self._ensure_voyage_folder(self._current_instrument, voyage_number)

        inst_index = self._load_instrument_index(self._current_instrument)
        inst_index["current_voyage"] = voyage_number
        self._save_instrument_index(self._current_instrument, inst_index, save_index)

        print(f"   ğŸ”¢ Voyage set to: {self._current_instrument.value} / v{voyage_number}")

    def list_all_instruments(self) -> List[str]:
        """List all instruments with checkpoints."""
        instruments = []
        for name in self.master_index.get("instruments", {}).keys():
            instruments.append(name)

        for folder in self.local_base_dir.iterdir():
            if folder.is_dir() and folder.name not in instruments:
                try:
                    get_instrument(folder.name)
                    instruments.append(folder.name)
                except:
                    pass

        return sorted(instruments)

    def list_voyages_for_instrument(self, instrument: Optional[str] = None) -> List[int]:
        """List all voyages for an instrument."""
        if instrument is None:
            inst = self._current_instrument
        else:
            inst = get_instrument(instrument)

        inst_index = self._load_instrument_index(inst)
        voyages = []

        for key in inst_index.get("voyages", {}).keys():
            try:
                voyages.append(int(key.replace('v', '')))
            except:
                pass

        inst_dir = self._get_local_instrument_dir(inst)
        if inst_dir.exists():
            for folder in inst_dir.iterdir():
                if folder.is_dir() and folder.name.startswith('v'):
                    try:
                        v_num = int(folder.name.replace('v', ''))
                        if v_num not in voyages:
                            voyages.append(v_num)
                    except:
                        pass

        return sorted(voyages)

    def list_checkpoints_in_voyage(self, instrument: Optional[str] = None, voyage_number: Optional[int] = None) -> List[Dict]:
        """List checkpoints in a voyage."""
        if instrument is None:
            inst = self._current_instrument
        else:
            inst = get_instrument(instrument)

        if voyage_number is None:
            voyage_number = self._current_voyage

        voyage_index = self._load_voyage_index(inst, voyage_number)
        checkpoints = voyage_index.get("checkpoints", [])
        return sorted(checkpoints, key=lambda x: x.get("step", 0))

    def get_latest_checkpoint_in_voyage(self, instrument: Optional[str] = None, voyage_number: Optional[int] = None) -> Optional[Dict]:
        checkpoints = self.list_checkpoints_in_voyage(instrument, voyage_number)
        return checkpoints[-1] if checkpoints else None

    # ========================================================================
    # FALLBACK CHAIN (Keep exact same interface)
    # ========================================================================

    def get_fallback_checkpoints(self, instrument, voyage_number: int, target_step: int) -> List[Tuple[int, int]]:
        """Get fallback checkpoints (voyage, step) to try."""
        fallbacks = []

        voyage_checkpoints = self.list_checkpoints_in_voyage(instrument.value, voyage_number)
        for cp in reversed(voyage_checkpoints):
            step = cp.get("step", 0)
            if step < target_step:
                fallbacks.append((voyage_number, step))
                if len(fallbacks) >= self.max_fallback_attempts:
                    break

        if self.allow_cross_voyage_fallback and len(fallbacks) < self.max_fallback_attempts:
            all_voyages = self.list_voyages_for_instrument(instrument.value)
            for prev_voyage in reversed(all_voyages):
                if prev_voyage < voyage_number:
                    prev_checkpoints = self.list_checkpoints_in_voyage(instrument.value, prev_voyage)
                    for cp in reversed(prev_checkpoints):
                        fallbacks.append((prev_voyage, cp.get("step", 0)))
                        if len(fallbacks) >= self.max_fallback_attempts:
                            break
                    if len(fallbacks) >= self.max_fallback_attempts:
                        break

        return fallbacks[:self.max_fallback_attempts]

# CONTINUED IN PART 2...
# PART 2 - Continuation of MultiInstrumentCheckpointManager
# Paste this after Part 1

    # ========================================================================
    # CHECKPOINT LOADING (Keep exact same interface)
    # ========================================================================

    def _load_checkpoint_file(self, instrument, voyage_number: int, training_steps: int, device: str = 'cpu') -> Optional[Dict]:
        """Load checkpoint file with caching and detailed diagnostics."""
        checkpoint_id = self._get_checkpoint_id(instrument, voyage_number, training_steps)

        if checkpoint_id in self._checkpoint_cache:
            return self._checkpoint_cache[checkpoint_id]

        checkpoint_path = (
            self._get_local_voyage_dir(instrument, voyage_number) /
            self._get_checkpoint_filename(instrument, voyage_number, training_steps)
        ).resolve()

        print(f"   ğŸ” Checkpoint path: {checkpoint_path}")

        if not checkpoint_path.exists():
            if self.hf_api:
                print(f"   ğŸ“¥ Downloading from HF...")
                download_success = self._download_checkpoint_from_gcs(instrument, voyage_number, training_steps)

                if download_success and checkpoint_path.exists():
                    size = checkpoint_path.stat().st_size
                    print(f"   âœ… File verified: {size:,} bytes")
                elif not checkpoint_path.exists():
                    print(f"   âŒ Download completed but file not found at: {checkpoint_path}")
                    if checkpoint_path.parent.exists():
                        files = [f.name for f in checkpoint_path.parent.iterdir()]
                        print(f"   ğŸ“‚ Directory contains {len(files)} files:")
                        for f in files[:5]:
                            print(f"      - {f}")
                    return None
            else:
                print(f"   âŒ File not found and no HF API configured")
                return None

        if not checkpoint_path.exists():
            print(f"   âŒ Checkpoint file still not found: {checkpoint_path.name}")
            return None

        try:
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

            if len(self._checkpoint_cache) >= self._cache_max_size:
                oldest_key = next(iter(self._checkpoint_cache))
                del self._checkpoint_cache[oldest_key]

            self._checkpoint_cache[checkpoint_id] = checkpoint
            return checkpoint

        except Exception as e:
            print(f"   âŒ torch.load() failed: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_checkpoint(
        self,
        quantum_system: nn.Module,
        instrument: Optional[str] = None,
        voyage_number: Optional[int] = None,
        training_steps: Optional[int] = None,
        quantum_advisor: Optional[nn.Module] = None,
        voting_trainer: Optional[Any] = None,
        device: str = 'cuda',
        recovery_mode = None,
        download_from_gcs: bool = True
    ):
        """Load checkpoint with cascading fallback."""
        start_time = time.time()

        if recovery_mode is None:
            recovery_mode = self.default_recovery_mode

        if instrument is None:
            inst = self._current_instrument
        else:
            inst = get_instrument(instrument)

        if voyage_number is None:
            voyage_number = self._current_voyage

        if training_steps is None:
            latest = self.get_latest_checkpoint_in_voyage(inst.value, voyage_number)
            if latest:
                training_steps = latest.get("step")
            else:
                raise ValueError(f"No checkpoints in {inst.value}/v{voyage_number}")

        result = CheckpointLoadResult(
            success=True,
            instrument=inst.value,
            voyage_number=voyage_number,
            training_steps=training_steps,
            episode_number=0,
            timestamp=""
        )

        print(f"\n{'='*80}")
        print(f"ğŸ“¥ LOADING CHECKPOINT")
        print(f"   Instrument: {inst.value}")
        print(f"   Voyage: v{voyage_number}")
        print(f"   Step: {training_steps}")
        if recovery_mode:
            print(f"   Recovery: {recovery_mode.value}")
        print(f"{'='*80}")

        primary_checkpoint = self._load_checkpoint_file(inst, voyage_number, training_steps, device)

        if primary_checkpoint is None:
            result.success = False
            result.errors.append(f"Checkpoint not found: {inst.value}/v{voyage_number}/step_{training_steps}")
            print(f"\n   âŒ CHECKPOINT NOT FOUND")
            return result

        result.timestamp = primary_checkpoint.get('timestamp', '')
        result.episode_number = primary_checkpoint.get('episode_number', 0)

        primary_id = self._get_checkpoint_id(inst, voyage_number, training_steps)
        print(f"\n   ğŸ“‚ Primary: {primary_id}")

        fallback_checkpoints = []
        if recovery_mode and hasattr(recovery_mode, 'FALLBACK'):
            fallback_checkpoints = self.get_fallback_checkpoints(inst, voyage_number, training_steps)
            if fallback_checkpoints:
                print(f"   ğŸ”„ Fallbacks: {[f'v{v}_step_{s}' for v, s in fallback_checkpoints]}")

        print(f"\n   ğŸ“¦ LOADING COMPONENTS:")
        print(f"   {'-'*70}")

        # Load agents
        if hasattr(quantum_system, 'agents') and 'agents' in primary_checkpoint:
            for agent_name in quantum_system.agents.keys():
                comp_result = self._load_component_with_fallback(
                    component=quantum_system.agents[agent_name],
                    component_name=f"agent_{agent_name}",
                    primary_checkpoint=primary_checkpoint,
                    primary_id=primary_id,
                    checkpoint_key='agents',
                    sub_key=agent_name,
                    fallback_checkpoints=fallback_checkpoints,
                    recovery_mode=recovery_mode,
                    instrument=inst,
                    device=device
                )
                result.components[f"agent_{agent_name}"] = comp_result

        # Load encoders
        for enc_name in ['latent_encoder', 'shared_latent_encoder']:
            if hasattr(quantum_system, enc_name) and getattr(quantum_system, enc_name) is not None:
                comp_result = self._load_component_with_fallback(
                    component=getattr(quantum_system, enc_name),
                    component_name=enc_name,
                    primary_checkpoint=primary_checkpoint,
                    primary_id=primary_id,
                    checkpoint_key=enc_name,
                    sub_key=None,
                    fallback_checkpoints=fallback_checkpoints,
                    recovery_mode=recovery_mode,
                    instrument=inst,
                    device=device
                )
                result.components[enc_name] = comp_result

        # Load quantum advisor
        advisor = quantum_advisor
        if advisor is None and hasattr(quantum_system, 'quantum_advisor'):
            advisor = quantum_system.quantum_advisor

        if advisor is not None:
            comp_result = self._load_component_with_fallback(
                component=advisor,
                component_name="quantum_advisor",
                primary_checkpoint=primary_checkpoint,
                primary_id=primary_id,
                checkpoint_key='quantum_advisor',
                sub_key=None,
                fallback_checkpoints=fallback_checkpoints,
                recovery_mode=recovery_mode,
                instrument=inst,
                device=device
            )
            result.components["quantum_advisor"] = comp_result

        # Load optimizers
        for opt_name in ['encoder_optimizer', 'shared_encoder_optimizer', 'agent_optimizer']:
            if hasattr(quantum_system, opt_name):
                optimizer = getattr(quantum_system, opt_name)
                if optimizer and opt_name in primary_checkpoint:
                    comp_result = self._load_optimizer_safe(optimizer, primary_checkpoint.get(opt_name), opt_name, primary_id)
                    result.components[opt_name] = comp_result

        # ====================================================================
        # V5.0.2: LOAD DIVERSITY PRESERVATION MODULES
        # ====================================================================
        # These trainable modules are critical for preventing agent homogenization
        # ====================================================================
        
        # Load latent_projector (AgentSpecificLatentProjector)
        if hasattr(quantum_system, 'latent_projector') and quantum_system.latent_projector is not None:
            if 'latent_projector' in primary_checkpoint:
                try:
                    quantum_system.latent_projector.load_state_dict(primary_checkpoint['latent_projector'])
                    print(f"   âœ… latent_projector [from {primary_id}]")
                    # FIX: Change 'source' to 'source_checkpoint'
                    result.components['latent_projector'] = ComponentLoadResult(
                        name='latent_projector', 
                        status=LoadStatus.SUCCESS,
                        success=True,
                        source_checkpoint=primary_id  # Changed from source to source_checkpoint
                    )
                except Exception as e:
                    print(f"   âŒ latent_projector FAILED: {e}")
                    result.components['latent_projector'] = ComponentLoadResult(
                        name='latent_projector', 
                        status=LoadStatus.FAILED,
                        success=False,
                        error=str(e)
                    )
            else:
                print(f"   âš ï¸  latent_projector not in checkpoint (keeping initialized weights)")
                result.components['latent_projector'] = ComponentLoadResult(
                    name='latent_projector',
                    status=LoadStatus.NOT_FOUND,
                    success=False
                )
        
        # Load coordination_modules (nn.ModuleList)  
        if hasattr(quantum_system, 'coordination_modules') and quantum_system.coordination_modules is not None:
            if 'coordination_modules' in primary_checkpoint:
                try:
                    quantum_system.coordination_modules.load_state_dict(primary_checkpoint['coordination_modules'])
                    print(f"   âœ… coordination_modules [from {primary_id}]")
                    result.components['coordination_modules'] = ComponentLoadResult(
                        name='coordination_modules', 
                        status=LoadStatus.SUCCESS,
                        success=True,
                        source_checkpoint=primary_id  # Changed from source to source_checkpoint
                    )
                except Exception as e:
                    print(f"   âŒ coordination_modules FAILED: {e}")
                    result.components['coordination_modules'] = ComponentLoadResult(
                        name='coordination_modules', 
                        status=LoadStatus.FAILED,
                        success=False,
                        error=str(e)
                    )
            else:
                print(f"   âš ï¸  coordination_modules not in checkpoint (keeping initialized weights)")
        
        # Load diversity_loss (DiversityLoss)
        if hasattr(quantum_system, 'diversity_loss') and quantum_system.diversity_loss is not None:
            if 'diversity_loss' in primary_checkpoint:
                try:
                    quantum_system.diversity_loss.load_state_dict(primary_checkpoint['diversity_loss'])
                    print(f"   âœ… diversity_loss [from {primary_id}]")
                    result.components['diversity_loss'] = ComponentLoadResult(
                        name='diversity_loss', 
                        status=LoadStatus.SUCCESS,
                        success=True,
                        source_checkpoint=primary_id  # Changed from source to source_checkpoint
                    )
                except Exception as e:
                    print(f"   âŒ diversity_loss FAILED: {e}")
                    result.components['diversity_loss'] = ComponentLoadResult(
                        name='diversity_loss', 
                        status=LoadStatus.FAILED,
                        success=False,
                        error=str(e)
                    )
            else:
                print(f"   âš ï¸  diversity_loss not in checkpoint (keeping initialized weights)")
        
        # Load state_augmenters (nn.ModuleDict)
        if hasattr(quantum_system, 'state_augmenters') and quantum_system.state_augmenters is not None:
            if 'state_augmenters' in primary_checkpoint:
                try:
                    quantum_system.state_augmenters.load_state_dict(primary_checkpoint['state_augmenters'])
                    print(f"   âœ… state_augmenters [from {primary_id}]")
                    result.components['state_augmenters'] = ComponentLoadResult(
                        name='state_augmenters', 
                        status=LoadStatus.SUCCESS,
                        success=True,
                        source_checkpoint=primary_id  # Changed from source to source_checkpoint
                    )
                except Exception as e:
                    print(f"   âŒ state_augmenters FAILED: {e}")
                    result.components['state_augmenters'] = ComponentLoadResult(
                        name='state_augmenters', 
                        status=LoadStatus.FAILED,
                        success=False,
                        error=str(e)
                    )
            else:
                print(f"   âš ï¸  state_augmenters not in checkpoint (keeping initialized weights)")
        
        # ====================================================================

        quantum_system.to(device)
        if advisor:
            advisor.to(device)

        self.clear_checkpoint_cache()
        result.total_load_time = time.time() - start_time
        self.load_count += 1

        self._print_load_summary(result)
        return result
    def _print_load_summary(self, result):
        """Print load summary."""
        summary = result.get_summary()
        print(f"\n   {'='*70}")
        print(f"   ğŸ“Š LOAD SUMMARY: {result.instrument}/v{result.voyage_number}")
        print(f"   {'='*70}")
        print(f"   Components: {summary['succeeded']}/{summary['total_components']} success")
        if summary.get('fallback', 0) > 0:
            print(f"   Fallbacks: {summary['fallback']}")
        print(f"   Time: {result.total_load_time:.2f}s")
        print(f"{'='*80}\n")

    def print_structure(self):
        """Print complete folder structure."""
        print(f"\n{'='*80}")
        print(f"ğŸ“ MULTI-INSTRUMENT CHECKPOINT STRUCTURE")
        print(f"{'='*80}")
        print(f"   Current: {self._current_instrument.value} / v{self._current_voyage}")

        for inst_name in self.list_all_instruments():
            inst = get_instrument(inst_name)
            voyages = self.list_voyages_for_instrument(inst_name)

            current_marker = "â†’" if inst == self._current_instrument else " "
            print(f"\n   {current_marker} {inst_name}/ ({len(voyages)} voyages)")

            for voyage_num in voyages[-3:]:
                checkpoints = self.list_checkpoints_in_voyage(inst_name, voyage_num)
                v_marker = "â†’" if (inst == self._current_instrument and voyage_num == self._current_voyage) else " "
                print(f"      {v_marker} v{voyage_num}/ ({len(checkpoints)} checkpoints)")

                for cp in checkpoints[-2:]:
                    print(f"         â””â”€ step_{cp.get('step', '?'):>7} | {cp.get('size_mb', 0):>6.1f} MB")

        print(f"{'='*80}\n")

    def _load_component_with_fallback(
        self,
        component: nn.Module,
        component_name: str,
        primary_checkpoint: Dict,
        primary_id: str,
        checkpoint_key: str,
        sub_key: Optional[str],
        fallback_checkpoints: List[Tuple[int, int]],
        recovery_mode,
        instrument,
        device: str
    ):
        """Load component with fallback to previous checkpoints."""
        comp_start = time.time()
        result = ComponentLoadResult(name=component_name, status=LoadStatus.FAILED)

        if sub_key:
            state_dict = primary_checkpoint.get(checkpoint_key, {}).get(sub_key)
        else:
            state_dict = primary_checkpoint.get(checkpoint_key)

        if state_dict is not None:
            success, error = self._try_load_state_dict(component, state_dict, component_name)
            if success:
                result.status = LoadStatus.SUCCESS
                result.source_checkpoint = primary_id
                result.keys_loaded = len(state_dict) if isinstance(state_dict, dict) else 0
                result.load_time_ms = (time.time() - comp_start) * 1000
                return result
            else:
                result.error = error
        else:
            print(f"   âš ï¸  {component_name} not in {primary_id}")

        # Fallback logic
        if recovery_mode and hasattr(recovery_mode, 'FALLBACK') and fallback_checkpoints:
            print(f"      ğŸ”„ Trying fallbacks...")

            for fb_voyage, fb_step in fallback_checkpoints:
                result.fallback_attempts += 1
                fb_id = self._get_checkpoint_id(instrument, fb_voyage, fb_step)

                fb_checkpoint = self._load_checkpoint_file(instrument, fb_voyage, fb_step, device)
                if fb_checkpoint is None:
                    continue

                if sub_key:
                    fb_state_dict = fb_checkpoint.get(checkpoint_key, {}).get(sub_key)
                else:
                    fb_state_dict = fb_checkpoint.get(checkpoint_key)

                if fb_state_dict is None:
                    continue

                success, error = self._try_load_state_dict(component, fb_state_dict, component_name)
                if success:
                    result.status = LoadStatus.FALLBACK
                    result.source_checkpoint = fb_id
                    result.source_step = fb_step
                    result.keys_loaded = len(fb_state_dict)
                    self.fallback_count += 1
                    print(f"         âœ… {component_name} â† {fb_id}")
                    result.load_time_ms = (time.time() - comp_start) * 1000
                    return result

        # Last resort
        if recovery_mode and hasattr(recovery_mode, 'REINIT'):
            if self.reinitializer.reinit_module(component, component_name):
                result.status = LoadStatus.REINITIALIZED
                result.source_checkpoint = "fresh_init"
        else:
            result.status = LoadStatus.SKIPPED

        result.load_time_ms = (time.time() - comp_start) * 1000
        return result

    def _try_load_state_dict(self, component: nn.Module, state_dict: Dict, component_name: str = "") -> Tuple[bool, str]:
        """Enhanced state dict loading that handles both formats."""
        try:
            if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                actual_state_dict = state_dict['state_dict']
                component.load_state_dict(actual_state_dict, strict=True)

                loaded_extras = []

                if 'actor_optimizer' in state_dict and hasattr(component, 'actor_optimizer'):
                    if component.actor_optimizer is not None:
                        try:
                            component.actor_optimizer.load_state_dict(state_dict['actor_optimizer'])
                            loaded_extras.append('actor_opt')
                        except: pass

                if 'critic_optimizer' in state_dict and hasattr(component, 'critic_optimizer'):
                    if component.critic_optimizer is not None:
                        try:
                            component.critic_optimizer.load_state_dict(state_dict['critic_optimizer'])
                            loaded_extras.append('critic_opt')
                        except: pass

                if 'optimizer' in state_dict and hasattr(component, 'optimizer'):
                    if component.optimizer is not None:
                        try:
                            component.optimizer.load_state_dict(state_dict['optimizer'])
                            loaded_extras.append('opt')
                        except: pass

                if 'train_step_counter' in state_dict and hasattr(component, 'train_step_counter'):
                    component.train_step_counter = state_dict['train_step_counter']
                    loaded_extras.append('train_step')

                if 'epsilon' in state_dict and hasattr(component, 'epsilon'):
                    component.epsilon = state_dict['epsilon']
                    loaded_extras.append('epsilon')

                if loaded_extras:
                    print(f"         âœ… {component_name} extras: {'+'.join(loaded_extras)}")

                return True, ""
            else:
                component.load_state_dict(state_dict, strict=True)
                return True, ""

        except RuntimeError as e:
            try:
                if isinstance(state_dict, dict) and 'state_dict' in state_dict:
                    component.load_state_dict(state_dict['state_dict'], strict=False)
                else:
                    component.load_state_dict(state_dict, strict=False)
                return True, f"(non-strict: {e})"
            except Exception as e2:
                return False, str(e)
        except Exception as e:
            return False, str(e)

    def _load_optimizer_safe(self, optimizer, state_dict, name, source_id):
        result = ComponentLoadResult(name=name, status=LoadStatus.FAILED)
        if state_dict is None:
            result.status = LoadStatus.NOT_FOUND
            return result
        try:
            optimizer.load_state_dict(state_dict)
            result.status = LoadStatus.SUCCESS
            result.source_checkpoint = source_id
            print(f"   âœ… {name} â† {source_id}")
        except:
            try:
                optimizer.state.clear()
                result.status = LoadStatus.SKIPPED
                print(f"   âš ï¸  {name} cleared")
            except:
                result.status = LoadStatus.FAILED
        return result

    def clear_checkpoint_cache(self):
        self._checkpoint_cache.clear()

    # ========================================================================
    # SAVE CHECKPOINT (Keep exact same interface)
    # ========================================================================

    def save_checkpoint(
        self,
        quantum_system: nn.Module,
        training_steps: int,
        instrument: Optional[str] = None,
        voyage_number: Optional[int] = None,
        quantum_advisor: Optional[nn.Module] = None,
        voting_trainer: Optional[Any] = None,
        episode_number: int = 0,
        upload_to_gcs: bool = True,  # Param name kept for compatibility
        notes: str = ""
    ):
        """Save checkpoint for specified instrument."""
        start_time = time.time()

        if instrument is None:
            inst = self._current_instrument
        else:
            inst = get_instrument(instrument)

        if voyage_number is None:
            voyage_number = self._current_voyage

        self._ensure_voyage_folder(inst, voyage_number)

        result = CheckpointSaveResult(
            success=True,
            instrument=inst.value,
            voyage_number=voyage_number,
            training_steps=training_steps,
            local_path=""
        )

        print(f"\n{'='*80}")
        print(f"ğŸ’¾ SAVING CHECKPOINT")
        print(f"   Instrument: {inst.value}")
        print(f"   Voyage: v{voyage_number}")
        print(f"   Step: {training_steps}")
        print(f"{'='*80}")

        checkpoint = {
            'instrument': inst.value,
            'voyage_number': voyage_number,
            'training_steps': training_steps,
            'episode_number': episode_number,
            'timestamp': datetime.now().isoformat(),
            'pytorch_version': torch.__version__,
            'config': {
                'state_dim': getattr(quantum_system, 'state_dim', 58),
                'action_dim': getattr(quantum_system, 'action_dim', 2),
                'agent_names': getattr(quantum_system, 'agent_names', []),
            },
            'agents': {},
        }

        print(f"\n   ğŸ“¦ SAVING COMPONENTS:")
        print(f"   {'-'*70}")

        # Save agents
        if hasattr(quantum_system, 'agents'):
            for name, agent in quantum_system.agents.items():
                comp_result = ComponentSaveResult(name=f"agent_{name}", success=False)
                try:
                    agent_state = {'state_dict': agent.state_dict()}

                    if hasattr(agent, 'actor_optimizer') and agent.actor_optimizer is not None:
                        try:
                            agent_state['actor_optimizer'] = agent.actor_optimizer.state_dict()
                        except: pass

                    if hasattr(agent, 'critic_optimizer') and agent.critic_optimizer is not None:
                        try:
                            agent_state['critic_optimizer'] = agent.critic_optimizer.state_dict()
                        except: pass

                    if hasattr(agent, 'optimizer') and agent.optimizer is not None:
                        try:
                            agent_state['optimizer'] = agent.optimizer.state_dict()
                        except: pass

                    if hasattr(agent, 'train_step_counter'):
                        agent_state['train_step_counter'] = agent.train_step_counter
                    if hasattr(agent, 'epsilon'):
                        agent_state['epsilon'] = agent.epsilon

                    checkpoint['agents'][name] = agent_state
                    comp_result.success = True
                    comp_result.size_bytes = sum(t.numel() * t.element_size() for t in agent.state_dict().values())

                    saved_parts = ['weights']
                    if 'actor_optimizer' in agent_state: saved_parts.append('actor_opt')
                    if 'critic_optimizer' in agent_state: saved_parts.append('critic_opt')
                    if 'optimizer' in agent_state: saved_parts.append('opt')

                    print(f"   âœ… agent_{name} ({comp_result.size_bytes / 1024:.1f} KB) [{'+'.join(saved_parts)}]")
                except Exception as e:
                    comp_result.error = str(e)
                    print(f"   âŒ agent_{name} FAILED: {e}")
                result.components[f"agent_{name}"] = comp_result

        # Save encoders
        for enc_name in ['latent_encoder', 'shared_latent_encoder']:
            if hasattr(quantum_system, enc_name):
                encoder = getattr(quantum_system, enc_name)
                if encoder is not None:
                    try:
                        checkpoint[enc_name] = encoder.state_dict()
                        print(f"   âœ… {enc_name}")
                    except Exception as e:
                        print(f"   âŒ {enc_name} FAILED: {e}")

        # Save quantum advisor
        advisor = quantum_advisor
        if advisor is None and hasattr(quantum_system, 'quantum_advisor'):
            advisor = quantum_system.quantum_advisor
        if advisor is not None:
            try:
                checkpoint['quantum_advisor'] = advisor.state_dict()
                print(f"   âœ… quantum_advisor")
            except Exception as e:
                print(f"   âŒ quantum_advisor FAILED: {e}")

        # Save optimizers
        for opt_name in ['encoder_optimizer', 'shared_encoder_optimizer', 'agent_optimizer']:
            if hasattr(quantum_system, opt_name):
                optimizer = getattr(quantum_system, opt_name)
                if optimizer is not None:
                    try:
                        checkpoint[opt_name] = optimizer.state_dict()
                        print(f"   âœ… {opt_name}")
                    except:
                        print(f"   âš ï¸  {opt_name} FAILED")

        # ====================================================================
        # V5.0.2: SAVE DIVERSITY PRESERVATION MODULES
        # ====================================================================
        # These trainable modules are critical for preventing agent homogenization
        # ====================================================================
        
        # Save latent_projector (AgentSpecificLatentProjector)
        if hasattr(quantum_system, 'latent_projector') and quantum_system.latent_projector is not None:
            try:
                checkpoint['latent_projector'] = quantum_system.latent_projector.state_dict()
                size_kb = sum(t.numel() * t.element_size() for t in quantum_system.latent_projector.state_dict().values()) / 1024
                print(f"   âœ… latent_projector ({size_kb:.1f} KB)")
            except Exception as e:
                print(f"   âŒ latent_projector FAILED: {e}")
        
        # Save coordination_modules (nn.ModuleList)
        if hasattr(quantum_system, 'coordination_modules') and quantum_system.coordination_modules is not None:
            try:
                checkpoint['coordination_modules'] = quantum_system.coordination_modules.state_dict()
                size_kb = sum(t.numel() * t.element_size() for t in quantum_system.coordination_modules.state_dict().values()) / 1024
                print(f"   âœ… coordination_modules ({size_kb:.1f} KB)")
            except Exception as e:
                print(f"   âŒ coordination_modules FAILED: {e}")
        
        # Save diversity_loss (DiversityLoss)
        if hasattr(quantum_system, 'diversity_loss') and quantum_system.diversity_loss is not None:
            try:
                checkpoint['diversity_loss'] = quantum_system.diversity_loss.state_dict()
                size_kb = sum(t.numel() * t.element_size() for t in quantum_system.diversity_loss.state_dict().values()) / 1024
                print(f"   âœ… diversity_loss ({size_kb:.1f} KB)")
            except Exception as e:
                print(f"   âŒ diversity_loss FAILED: {e}")
        
        # Save state_augmenters (nn.ModuleDict)
        if hasattr(quantum_system, 'state_augmenters') and quantum_system.state_augmenters is not None:
            try:
                checkpoint['state_augmenters'] = quantum_system.state_augmenters.state_dict()
                size_kb = sum(t.numel() * t.element_size() for t in quantum_system.state_augmenters.state_dict().values()) / 1024
                print(f"   âœ… state_augmenters ({size_kb:.1f} KB)")
            except Exception as e:
                print(f"   âŒ state_augmenters FAILED: {e}")
        # ====================================================================

        # Write to disk
        print(f"\n   ğŸ’½ WRITING TO DISK:")
        print(f"   {'-'*70}")

        checkpoint_filename = self._get_checkpoint_filename(inst, voyage_number, training_steps)
        checkpoint_path = self._get_local_voyage_dir(inst, voyage_number) / checkpoint_filename

        try:
            if self.enable_compression:
                torch.save(checkpoint, checkpoint_path, _use_new_zipfile_serialization=True)
            else:
                torch.save(checkpoint, checkpoint_path)

            result.file_size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)
            result.local_path = str(checkpoint_path)
            print(f"   âœ… Saved: {checkpoint_filename} ({result.file_size_mb:.2f} MB)")
        except Exception as e:
            result.success = False
            result.errors.append(f"Write failed: {e}")
            print(f"   âŒ WRITE FAILED: {e}")
            return result

        # Update indices
        self._update_indices_after_save(inst, voyage_number, training_steps, episode_number, result.file_size_mb, notes, upload_to_gcs)

        result.total_save_time = time.time() - start_time
        self.save_count += 1
        self.last_save_time = time.time()

        # HF upload (uses upload_to_gcs param name for compatibility)
        if upload_to_gcs and self.hf_api:
            if self.enable_async_save and self.save_queue:
                self.save_queue.put(('upload', checkpoint_path, inst, voyage_number, training_steps))
                print(f"   â˜ï¸  HF upload queued")
            else:
                hf_path = self._upload_checkpoint_to_gcs(checkpoint_path, inst, voyage_number, training_steps)
                if hf_path:
                    result.gcs_path = hf_path

        print(f"\n   ğŸ“Š SAVED: {inst.value}/v{voyage_number}/step_{training_steps}")
        print(f"{'='*80}\n")

        return result

    def _update_indices_after_save(self, instrument, voyage_number: int, training_steps: int,
                                   episode_number: int, file_size_mb: float, notes: str, upload_to_gcs: bool):
        """Update all indices after save."""
        voyage_index = self._load_voyage_index(instrument, voyage_number)
        entry = {
            "step": training_steps,
            "episode": episode_number,
            "file": self._get_checkpoint_filename(instrument, voyage_number, training_steps),
            "size_mb": round(file_size_mb, 2),
            "timestamp": datetime.now().isoformat(),
            "notes": notes
        }

        existing_idx = next((i for i, cp in enumerate(voyage_index.get("checkpoints", []))
                           if cp.get("step") == training_steps), None)
        if existing_idx is not None:
            voyage_index["checkpoints"][existing_idx] = entry
        else:
            voyage_index.setdefault("checkpoints", []).append(entry)
            voyage_index["checkpoints"].sort(key=lambda x: x.get("step", 0))

        self._save_voyage_index(instrument, voyage_number, voyage_index, upload_to_gcs)

        inst_index = self._load_instrument_index(instrument)
        voyage_key = self._get_voyage_folder_name(voyage_number)
        inst_index.setdefault("voyages", {})[voyage_key] = {
            "checkpoints": len(voyage_index["checkpoints"]),
            "last_step": training_steps
        }
        self._save_instrument_index(instrument, inst_index, upload_to_gcs)

        inst_name = instrument.value
        self.master_index.setdefault("instruments", {})[inst_name] = {
            "current_voyage": voyage_number,
            "total_voyages": len(inst_index.get("voyages", {})),
            "last_updated": datetime.now().isoformat()
        }
        self.save_master_index(upload_to_gcs)

        self._cleanup_old_voyage_checkpoints(instrument, voyage_number)

    # ========================================================================
    # BRIDGE CONVENIENCE METHODS (Keep exact same interface)
    # ========================================================================

    def save_from_bridge(self, bridge, training_steps: int, instrument: Optional[str] = None,
                         voyage_number: Optional[int] = None, episode_number: int = 0,
                         upload_to_gcs: bool = True, notes: str = ""):
        quantum_system = getattr(bridge, 'quantum_system', None)
        quantum_advisor = getattr(bridge, 'quantum_advisor', None)
        voting_trainer = getattr(bridge, 'voting_trainer', None)
        if quantum_system is None:
            raise ValueError("Bridge has no quantum_system!")
        return self.save_checkpoint(quantum_system, training_steps, instrument, voyage_number,
                                   quantum_advisor, voting_trainer, episode_number, upload_to_gcs, notes)

    def load_to_bridge(self, bridge, instrument: Optional[str] = None, voyage_number: Optional[int] = None,
                       training_steps: Optional[int] = None, device: str = 'cuda',
                       recovery_mode = None):
        quantum_system = getattr(bridge, 'quantum_system', None)
        quantum_advisor = getattr(bridge, 'quantum_advisor', None)
        voting_trainer = getattr(bridge, 'voting_trainer', None)
        if quantum_system is None:
            raise ValueError("Bridge has no quantum_system!")
        return self.load_checkpoint(quantum_system, instrument, voyage_number, training_steps,
                                   quantum_advisor, voting_trainer, device, recovery_mode)

    # ========================================================================
    # HF UPLOAD/DOWNLOAD (Replaces GCS methods but keeps method names)
    # ========================================================================

    def _upload_checkpoint_to_gcs(self, local_path: Path, instrument, voyage_number: int, training_steps: int):
        """Upload checkpoint to HF (method name kept for compatibility)."""
        if not self.hf_api:
            print(f"   âš ï¸  HF API not available, skipping upload")
            return None
        
        try:
            filename = self._get_checkpoint_filename(instrument, voyage_number, training_steps)
            path_in_repo = f"{instrument.value}/v{voyage_number}/{filename}"
            
            print(f"   â˜ï¸  Uploading to HF...")
            
            # FIX: Convert Path to string and open file in binary mode
            file_path_str = str(local_path)
            if not os.path.isfile(file_path_str):
                print(f"   âŒ File not found: {file_path_str}")
                return None
            
            # Open file in binary mode for upload
            with open(file_path_str, "rb") as file_obj:
                url = self.hf_api.upload_file(
                    path_or_fileobj=file_obj,  # Pass file object instead of path string
                    path_in_repo=path_in_repo,
                    repo_id=self.hf_repo_id,
                    repo_type="dataset"
                )
            
            hf_path = f"hf://datasets/{self.hf_repo_id}/{path_in_repo}"
            print(f"   âœ… HF: {hf_path}")
            return hf_path
            
        except Exception as e:
            print(f"   âŒ HF Upload Failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _download_checkpoint_from_gcs(self, instrument, voyage_number: int, training_steps: int) -> bool:
        """Download checkpoint from HF (method name kept for compatibility)."""
        if not self.hf_api:
            return False

        try:
            filename = self._get_checkpoint_filename(instrument, voyage_number, training_steps)
            hf_path = f"{instrument.value}/v{voyage_number}/{filename}"

            # FIX: Use parent directory, not the voyage directory
            local_base = self.local_base_dir.resolve()  # Use base dir, not voyage dir

            print(f"   â˜ï¸  Downloading from HF...")
            print(f"   ğŸ¯ Base directory: {local_base}")
            print(f"   â˜ï¸  HF path: {hf_path}")

            # Download to base dir - HF will create the full path structure
            downloaded_path = hf_hub_download(
                repo_id=self.hf_repo_id,
                filename=hf_path,
                repo_type="dataset",
                local_dir=str(local_base),  # Use BASE dir, not voyage dir
                local_dir_use_symlinks=False
            )

            # Verify the file is where we expect it
            expected_path = self._get_local_voyage_dir(instrument, voyage_number) / filename
            expected_path = expected_path.resolve()

            if expected_path.exists():
                print(f"   âœ… Downloaded and verified: {filename}")
                return True
            else:
                print(f"   âš ï¸  File downloaded but not at expected location")
                print(f"   Expected: {expected_path}")
                print(f"   Downloaded to: {downloaded_path}")

                # Try to find and move it
                downloaded_file = Path(downloaded_path)
                if downloaded_file.exists():
                    print(f"   ğŸ”„ Moving file to correct location...")
                    expected_path.parent.mkdir(parents=True, exist_ok=True)
                    downloaded_file.rename(expected_path)
                    print(f"   âœ… File moved successfully")
                    return True
                return False

        except Exception as e:
            print(f"   âŒ HF Download Failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _cleanup_old_voyage_checkpoints(self, instrument, voyage_number: int):
        voyage_index = self._load_voyage_index(instrument, voyage_number)
        checkpoints = voyage_index.get("checkpoints", [])

        if len(checkpoints) <= self.max_checkpoints_per_voyage:
            return

        to_remove = checkpoints[:-self.max_checkpoints_per_voyage]
        voyage_dir = self._get_local_voyage_dir(instrument, voyage_number)

        for cp in to_remove:
            filename = cp.get("file")
            if filename:
                local_path = voyage_dir / filename
                if local_path.exists():
                    local_path.unlink()

        voyage_index["checkpoints"] = checkpoints[-self.max_checkpoints_per_voyage:]
        self._save_voyage_index(instrument, voyage_number, voyage_index)

    def _start_save_worker(self):
        def worker():
            while True:
                try:
                    task = self.save_queue.get(timeout=1)
                    if task is None:
                        break
                    task_type, *args = task
                    if task_type == 'upload':
                        self._upload_checkpoint_to_gcs(*args)
                    self.save_queue.task_done()
                except queue.Empty:
                    continue
                except:
                    pass
        self.save_worker_thread = threading.Thread(target=worker, daemon=True)
        self.save_worker_thread.start()


# ============================================================================
# END OF INTEGRATED CHECKPOINT SYSTEM
# ============================================================================

class RobustConfig:
    """
    Central configuration for robustness features.
    All thresholds and limits in one place for easy tuning.
    """
    # State validation thresholds
    MAX_STATE_MAGNITUDE = 1000.0          # Trigger normalization above this
    MAX_FORECAST_MAGNITUDE = 10.0          # Clip forecasts to this range
    MAX_Q_ADJUSTMENT = 1.0                 # Maximum Q-value adjustment
    MIN_VALID_MAGNITUDE = 1e-8            # Minimum non-zero magnitude

    # Error handling configuration
    MAX_RETRIES = 3                        # Maximum retry attempts
    RETRY_DELAY = 1.0                      # Base delay between retries (seconds)
    FALLBACK_ENABLED = True                # Enable fallback mechanisms
    SUPPRESS_ERRORS = False                # Whether to suppress non-critical errors

    # Logging configuration
    LOG_NAN_DETECTION = True               # Log NaN/Inf detections
    LOG_BOUNDS_VIOLATIONS = True           # Log when values exceed bounds
    LOG_FALLBACK_TRIGGERS = True           # Log when fallbacks are used
    LOG_NORMALIZATION = True               # Log normalization events

    # Monitoring configuration
    HEALTH_CHECK_ENABLED = True            # Enable health monitoring
    ALERT_ON_CONSECUTIVE_ERRORS = 5        # Alert threshold for errors
    HEALTH_CHECK_INTERVAL = 60.0           # Seconds between health checks

# Global health monitor instance
_HEALTH_MONITOR = None

def get_health_monitor():
    """Get or create the global health monitor."""
    global _HEALTH_MONITOR
    if _HEALTH_MONITOR is None:
        _HEALTH_MONITOR = HealthMonitor()
    return _HEALTH_MONITOR

def validate_tensor_safe(tensor, name='tensor', max_magnitude=None):
    """
    Comprehensive tensor validation with automatic fixing.

    Args:
        tensor: Tensor to validate
        name: Name for logging
        max_magnitude: Maximum allowed magnitude (None = use config default)

    Returns:
        tuple: (validated_tensor, needs_normalization)
    """
    if max_magnitude is None:
        max_magnitude = RobustConfig.MAX_STATE_MAGNITUDE

    try:
        # Check for None
        if tensor is None:
            if RobustConfig.LOG_BOUNDS_VIOLATIONS:
                logging.error(f"âŒ {name} is None - creating zero tensor")
            return torch.zeros(1), True

        # Check type
        if not isinstance(tensor, torch.Tensor):
            if RobustConfig.LOG_BOUNDS_VIOLATIONS:
                logging.warning(f"âš ï¸ {name} is not a tensor (type: {type(tensor)})")
            try:
                tensor = torch.tensor(tensor)
            except:
                return torch.zeros(1), True

        # Check for empty
        if tensor.numel() == 0:
            if RobustConfig.LOG_BOUNDS_VIOLATIONS:
                logging.warning(f"âš ï¸ {name} is empty")
            return torch.zeros(1), True

        # Check for NaN
        if torch.isnan(tensor).any():
            if RobustConfig.LOG_NAN_DETECTION:
                logging.warning(f"âš ï¸ {name} contains NaN - replacing with zeros")
            tensor = torch.nan_to_num(tensor, nan=0.0)
            get_health_monitor().record_error(f'{name}_nan')

        # Check for Inf
        if torch.isinf(tensor).any():
            if RobustConfig.LOG_NAN_DETECTION:
                logging.warning(f"âš ï¸ {name} contains Inf - replacing with zeros")
            tensor = torch.nan_to_num(tensor, posinf=0.0, neginf=0.0)
            get_health_monitor().record_error(f'{name}_inf')

        # Check magnitude
        magnitude = tensor.abs().max().item()
        if magnitude > max_magnitude:
            if RobustConfig.LOG_BOUNDS_VIOLATIONS:
                logging.warning(
                    f"âš ï¸ {name} magnitude {magnitude:.2f} exceeds limit {max_magnitude:.2f}"
                )
            get_health_monitor().record_error(f'{name}_magnitude')
            return tensor, True  # Needs normalization

        # All checks passed
        get_health_monitor().record_success(f'{name}_validation')
        return tensor, False

    except Exception as e:
        logging.error(f"âŒ Validation error for {name}: {e}")
        get_health_monitor().record_error(f'{name}_validation_exception')
        # Return safe fallback
        return torch.zeros_like(tensor) if torch.is_tensor(tensor) else torch.zeros(1), True

def safe_normalize(tensor, name='tensor', epsilon=None):
    """
    Safely normalize tensor with comprehensive error handling.

    Args:
        tensor: Tensor to normalize
        name: Name for logging
        epsilon: Small value to prevent division by zero

    Returns:
        Normalized tensor (or zero tensor on failure)
    """
    if epsilon is None:
        epsilon = RobustConfig.MIN_VALID_MAGNITUDE

    try:
        # Validate first
        tensor, needs_norm = validate_tensor_safe(tensor, name)

        # Calculate magnitude
        magnitude = tensor.norm().item()

        # Check if magnitude is too small
        if magnitude < epsilon:
            if RobustConfig.LOG_NORMALIZATION:
                logging.warning(
                    f"âš ï¸ {name} magnitude too small ({magnitude:.2e}) - returning zeros"
                )
            get_health_monitor().record_error(f'{name}_zero_magnitude')
            return torch.zeros_like(tensor)

        # Normalize
        normalized = tensor / (magnitude + epsilon)

        # Verify normalization
        new_magnitude = normalized.norm().item()
        if abs(new_magnitude - 1.0) > 0.1:  # Allow 10% tolerance
            if RobustConfig.LOG_NORMALIZATION:
                logging.warning(
                    f"âš ï¸ {name} normalization check failed: {new_magnitude:.4f} â‰  1.0"
                )

        if RobustConfig.LOG_NORMALIZATION:
            logging.info(f"âœ“ {name} normalized: {magnitude:.2f} â†’ {new_magnitude:.4f}")

        get_health_monitor().record_success(f'{name}_normalization')
        return normalized

    except Exception as e:
        logging.error(f"âŒ Normalization error for {name}: {e}")
        get_health_monitor().record_error(f'{name}_normalization_exception')
        return torch.zeros_like(tensor)

def with_error_handling(fallback_value=None, log_error=True, component_name=None):
    """
    Decorator for robust error handling with automatic fallback.

    Args:
        fallback_value: Value to return on error (None = re-raise)
        log_error: Whether to log errors
        component_name: Component name for health monitoring
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            comp_name = component_name or func.__name__
            try:
                result = func(*args, **kwargs)
                get_health_monitor().record_success(comp_name)
                return result
            except Exception as e:
                if log_error:
                    logging.error(
                        f"âŒ Error in {func.__name__}: {e}",
                        exc_info=True if not RobustConfig.SUPPRESS_ERRORS else False
                    )
                get_health_monitor().record_error(comp_name)

                if fallback_value is not None:
                    if RobustConfig.LOG_FALLBACK_TRIGGERS:
                        logging.warning(f"â†©ï¸ {func.__name__} using fallback value")
                    return fallback_value
                raise
        return wrapper
    return decorator

def retry_on_failure(max_retries=None, delay=None, exceptions=(Exception,), backoff=True):
    """
    Decorator for automatic retry on failure with exponential backoff.

    Args:
        max_retries: Maximum number of retry attempts (None = use config)
        delay: Base delay between retries in seconds (None = use config)
        exceptions: Tuple of exceptions to catch
        backoff: Whether to use exponential backoff
    """
    if max_retries is None:
        max_retries = RobustConfig.MAX_RETRIES
    if delay is None:
        delay = RobustConfig.RETRY_DELAY

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    result = func(*args, **kwargs)
                    if attempt > 0:
                        logging.info(f"âœ“ {func.__name__} succeeded on attempt {attempt + 1}")
                    return result
                except exceptions as e:
                    if attempt == max_retries - 1:
                        logging.error(
                            f"âŒ {func.__name__} failed after {max_retries} attempts"
                        )
                        raise

                    wait_time = delay * (2 ** attempt if backoff else (attempt + 1))
                    logging.warning(
                        f"âš ï¸ {func.__name__} attempt {attempt + 1}/{max_retries} failed: {e}"
                        f" - retrying in {wait_time:.1f}s..."
                    )
                    time.sleep(wait_time)
            return None
        return wrapper
    return decorator

class HealthMonitor:
    """
    System health monitoring for tracking errors and system status.
    """

    def __init__(self):
        self.errors = defaultdict(int)
        self.successes = defaultdict(int)
        self.last_success = defaultdict(float)
        self.last_error = defaultdict(float)
        self.start_time = time.time()
        self._alerts_sent = set()

    def record_success(self, component: str):
        """Record successful operation for a component."""
        self.last_success[component] = time.time()
        self.successes[component] += 1
        # Reset error counter on success
        if self.errors[component] > 0:
            logging.info(f"âœ“ {component} recovered after {self.errors[component]} errors")
        self.errors[component] = 0
        # Clear alert flag on recovery
        if component in self._alerts_sent:
            self._alerts_sent.remove(component)

    def record_error(self, component: str):
        """Record error for a component."""
        self.errors[component] += 1
        self.last_error[component] = time.time()

        # Check if we should alert
        if (self.errors[component] >= RobustConfig.ALERT_ON_CONSECUTIVE_ERRORS and
            component not in self._alerts_sent):
            logging.critical(
                f"ğŸš¨ ALERT: {component} has {self.errors[component]} consecutive errors!"
            )
            self._alerts_sent.add(component)

    def get_error_count(self, component: str) -> int:
        """Get error count for a component."""
        return self.errors[component]

    def get_success_count(self, component: str) -> int:
        """Get success count for a component."""
        return self.successes[component]

    def get_health_status(self) -> dict:
        """
        Get overall system health status.

        Returns:
            Dict with uptime, component statuses, and error counts
        """
        uptime = time.time() - self.start_time
        total_errors = sum(self.errors.values())
        total_successes = sum(self.successes.values())

        # Calculate health score (0-100)
        if total_errors + total_successes > 0:
            health_score = (total_successes / (total_errors + total_successes)) * 100
        else:
            health_score = 100.0

        return {
            'uptime_seconds': uptime,
            'health_score': health_score,
            'total_errors': total_errors,
            'total_successes': total_successes,
            'component_errors': dict(self.errors),
            'component_successes': dict(self.successes),
            'last_error_time': dict(self.last_error),
            'last_success_time': dict(self.last_success),
            'alerts_active': list(self._alerts_sent)
        }

    def print_health_report(self):
        """Print a formatted health report."""
        status = self.get_health_status()
        print("\n" + "="*80)
        print("SYSTEM HEALTH REPORT")
        print("="*80)
        print(f"Uptime: {status['uptime_seconds']/3600:.2f} hours")
        print(f"Health Score: {status['health_score']:.1f}%")
        print(f"Total Operations: {status['total_successes'] + status['total_errors']}")
        print(f"  âœ“ Successes: {status['total_successes']}")
        print(f"  âœ— Errors: {status['total_errors']}")

        if status['component_errors']:
            print("\nComponent Errors:")
            for comp, count in sorted(status['component_errors'].items(),
                                     key=lambda x: x[1], reverse=True):
                if count > 0:
                    print(f"  {comp}: {count} errors")

        if status['alerts_active']:
            print("\nğŸš¨ ACTIVE ALERTS:")
            for alert in status['alerts_active']:
                print(f"  - {alert}")

        print("="*80 + "\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# END ROBUSTNESS UTILITIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VotingEnsembleTrainer:
    """
    SOPHISTICATED ENSEMBLE TRAINER WITH VOTING MECHANISM
    ====================================================

    **KEY CONCEPT:**
    Instead of blindly applying every gradient update, we:
    1. Generate multiple candidate updates from different strategies
    2. Have them "vote" on which direction to update
    3. Only apply consensus updates that multiple strategies agree on
    4. Reject outlier/bad updates automatically

    **BENEFITS:**
    - More stable training (bad updates get voted out)
    - Faster convergence (multiple strategies find good directions faster)
    - Robust to hyperparameter choices (ensemble averages out mistakes)
    - Adaptive (strategies that work better get more voting power)

    **CLASS ARCHITECTURE:**

    VotingEnsembleTrainer (meta-trainer)
        â”œâ”€â”€ ConservativeTrainer (voter #1 - safe, small updates)
        â”œâ”€â”€ AggressiveTrainer  (voter #2 - large, exploratory updates)
        â””â”€â”€ AdaptiveTrainer    (voter #3 - dynamically adjusts learning rate)

    Each voter processes the same batch and proposes parameter updates.
    The ensemble then votes on which updates to actually apply.
    """

    def __init__(
        self,
        base_trainer,  # Your existing QuantumSystemTrainer
        voting_threshold: float = 0.6,  # Need 60% agreement to apply update
        max_gradient_norm: float = 1.0,
        performance_window: int = 100,
        device: Optional[torch.device] = None
    ):
        """
        Initialize voting ensemble trainer.

        Args:
            base_trainer: Your existing QuantumSystemTrainer instance
            voting_threshold: Minimum fraction of voters that must agree (0.0-1.0)
            max_gradient_norm: Maximum allowed gradient norm (for safety)
            performance_window: Number of recent steps to track performance
            device: Torch device
        """
        self.base_trainer = base_trainer
        self.voting_threshold = voting_threshold
        self.max_gradient_norm = max_gradient_norm
        self.device = device or base_trainer.device

        # Performance tracking
        self.performance_window = performance_window
        self.strategy_performance = {
            'conservative': deque(maxlen=performance_window),
            'aggressive': deque(maxlen=performance_window),
            'adaptive': deque(maxlen=performance_window),
        }

        # Voting weights (initially equal, then adaptive)
        self.voting_weights = {
            'conservative': 1.0,
            'aggressive': 1.0,
            'adaptive': 1.0,
        }

        # Training statistics
        self.stats = {
            'total_votes': 0,
            'consensus_reached': 0,
            'updates_applied': 0,
            'updates_rejected': 0,
            'best_strategy': None,
        }

        # Create strategy-specific optimizers
        self._create_strategy_optimizers()

        logger.critical("âœ… VotingEnsembleTrainer initialized with 3 voting strategies")
        logger.critical(f"   Voting threshold: {voting_threshold:.1%}")
        logger.critical(f"   Gradient clip: {max_gradient_norm}")

    @property
    def buffer(self):
        """Expose the base trainer's buffer for diagnostic access."""
        return self.base_trainer.buffer if hasattr(self.base_trainer, 'buffer') else None

    @buffer.setter
    def buffer(self, value):
        """Allow setting the buffer on the base trainer."""
        if hasattr(self.base_trainer, 'buffer'):
            self.base_trainer.buffer = value
        else:
            self.base_trainer.buffer = value

    def _create_strategy_optimizers(self):
        """Create separate optimizers for each training strategy."""
        params = self.base_trainer.trainable_params

        # Strategy 1: Conservative (small LR, high stability)
        self.conservative_optimizer = torch.optim.Adam(params, lr=5e-5, betas=(0.9, 0.999))

        # Strategy 2: Aggressive (large LR, fast exploration)
        self.aggressive_optimizer = torch.optim.Adam(params, lr=5e-4, betas=(0.9, 0.99))

        # Strategy 3: Adaptive (medium LR with scheduler)
        self.adaptive_optimizer = torch.optim.Adam(params, lr= 1e-6, betas=(0.9, 0.999))
        self.adaptive_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.adaptive_optimizer, mode='min', factor=0.5, patience=10
        )

        logger.info("ğŸ—³ï¸  Created 3 voting strategies:")
        logger.info("   1. Conservative: LR=5e-5 (stable)")
        logger.info("   2. Aggressive:   LR=5e-4 (exploratory)")
        logger.info("   3. Adaptive:     LR=1e-4 (balanced)")

    def train_step(self, batch_size=None) -> Optional[Dict[str, Any]]:
        """
        VOTING-BASED TRAINING STEP
        ==========================

        Process:
        1. Sample batch from buffer
        2. Compute loss
        3. Generate 3 candidate gradient updates (one per strategy)
        4. Vote on which gradients to apply
        5. Apply consensus gradients
        6. Update strategy voting weights based on performance

        Args:
            batch_size: Optional batch size (uses base_trainer.batch_size if None)

        Returns:
            Dictionary with training metrics and voting statistics
        """
        try:
            # Use passed batch_size or fall back to base_trainer's batch_size
            actual_batch_size = batch_size if batch_size is not None else self.base_trainer.batch_size

            # Check buffer has enough samples
            if len(self.base_trainer.buffer) < actual_batch_size:
                return None

            # ================================================================
            # SIMPLIFIED: Delegate directly to base_trainer.train_step()
            # Note: base_trainer uses its own self.batch_size internally
            # ================================================================
            result = self.base_trainer.train_step()

            # Update voting statistics (for future enhancement)
            if result:
                self.stats['total_votes'] += 1
                self.stats['updates_applied'] += 1

            return result

        except Exception as e:
            logger.error(f"[VOTING TRAINER ERROR] {e}")
            import traceback
            traceback.print_exc()
            return None

    def _compute_strategy_gradients(
        self,
        loss: torch.Tensor,
        optimizer: torch.optim.Optimizer,
        strategy_name: str
    ) -> List[torch.Tensor]:
        """
        Compute gradients for a specific strategy.

        Returns:
            List of gradient tensors (one per parameter)
        """
        # Zero gradients
        optimizer.zero_grad()

        # Backward pass
        loss.backward(retain_graph=True)

        # Clip gradients for safety
        torch.nn.utils.clip_grad_norm_(
            self.base_trainer.trainable_params,
            self.max_gradient_norm
        )

        # Extract gradients
        gradients = []
        for param in self.base_trainer.trainable_params:
            if param.grad is not None:
                # Clone gradient to avoid modification
                gradients.append(param.grad.clone())
            else:
                # No gradient for this parameter
                gradients.append(torch.zeros_like(param))

        return gradients

    def _vote_on_gradients(
        self,
        candidate_gradients: Dict[str, List[torch.Tensor]]
    ) -> Tuple[Optional[List[torch.Tensor]], Dict[str, Any]]:
        """
        CORE VOTING MECHANISM
        =====================

        Determines consensus gradients by voting across strategies.

        Voting algorithm:
        1. For each parameter, compute cosine similarity between candidate gradients
        2. Count how many strategies agree (similarity > threshold)
        3. If enough strategies agree, use weighted average gradient
        4. If not, reject the update

        Returns:
            (consensus_gradients, voting_stats)
            - consensus_gradients: None if no consensus, else list of gradient tensors
            - voting_stats: Dictionary with voting metadata
        """
        self.stats['total_votes'] += 1

        num_params = len(self.base_trainer.trainable_params)
        num_strategies = len(candidate_gradients)

        # Track agreement per parameter
        param_agreements = []
        consensus_grads = []

        for param_idx in range(num_params):
            # Extract this parameter's gradients from all strategies
            grads = {
                strategy: candidate_gradients[strategy][param_idx]
                for strategy in candidate_gradients
            }

            # Compute pairwise cosine similarities
            similarities = self._compute_gradient_similarities(grads)

            # Count agreements (similarity > 0.5 = same direction)
            agreements = sum(1 for sim in similarities.values() if sim > 0.5)
            total_pairs = len(similarities)

            agreement_rate = agreements / total_pairs if total_pairs > 0 else 0.0
            param_agreements.append(agreement_rate)

            # Compute weighted average gradient
            weighted_grad = self._weighted_gradient_average(grads)
            consensus_grads.append(weighted_grad)

        # Overall agreement
        overall_agreement = np.mean(param_agreements) if param_agreements else 0.0

        # Voting statistics
        voting_stats = {
            'agreement': overall_agreement,
            'agreeing_strategies': int(overall_agreement * num_strategies),
            'param_agreements': param_agreements,
        }

        # Decision: Apply update if agreement exceeds threshold
        if overall_agreement >= self.voting_threshold:
            self.stats['consensus_reached'] += 1
            return consensus_grads, voting_stats
        else:
            return None, voting_stats

    def _compute_gradient_similarities(
        self,
        gradients: Dict[str, torch.Tensor]
    ) -> Dict[Tuple[str, str], float]:
        """
        Compute cosine similarity between all pairs of gradients.

        Returns:
            Dictionary mapping (strategy1, strategy2) -> similarity
        """
        similarities = {}
        strategies = list(gradients.keys())

        for i, strat1 in enumerate(strategies):
            for j, strat2 in enumerate(strategies):
                if i < j:  # Only compute each pair once
                    g1 = gradients[strat1].flatten()
                    g2 = gradients[strat2].flatten()

                    # Cosine similarity
                    if g1.norm() > 0 and g2.norm() > 0:
                        sim = torch.dot(g1, g2) / (g1.norm() * g2.norm())
                        similarities[(strat1, strat2)] = sim.item()
                    else:
                        similarities[(strat1, strat2)] = 0.0

        return similarities

    def _weighted_gradient_average(
        self,
        gradients: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        Compute weighted average of gradients using voting weights.

        Better-performing strategies get more influence.
        """
        weighted_sum = torch.zeros_like(list(gradients.values())[0])
        total_weight = 0.0

        for strategy, grad in gradients.items():
            weight = self.voting_weights[strategy]
            weighted_sum += weight * grad
            total_weight += weight

        return weighted_sum / (total_weight + 1e-8)

    def _apply_gradients(self, gradients: List[torch.Tensor]):
        """
        Apply consensus gradients to model parameters.
        """
        for param, grad in zip(self.base_trainer.trainable_params, gradients):
            param.grad = grad

        # Use conservative optimizer to apply (most stable)
        self.conservative_optimizer.step()

    def _update_strategy_weights(self, loss: float):
        """
        Update voting weights based on recent performance.

        Strategies that lead to lower losses get higher voting power.
        """
        # Record performance
        # (In practice, you'd track which strategy's gradients were used
        # and whether loss decreased. This is simplified.)
        for strategy in self.strategy_performance:
            self.strategy_performance[strategy].append(loss)

        # Compute average recent loss per strategy
        avg_losses = {}
        for strategy, losses in self.strategy_performance.items():
            if len(losses) > 0:
                avg_losses[strategy] = np.mean(losses)
            else:
                avg_losses[strategy] = float('inf')

        # Convert to weights (inverse of loss)
        min_loss = min(avg_losses.values())
        for strategy in self.voting_weights:
            if avg_losses[strategy] < float('inf'):
                # Weight = 1 / relative_loss
                relative_loss = avg_losses[strategy] / (min_loss + 1e-8)
                self.voting_weights[strategy] = 1.0 / (relative_loss + 1e-8)
            else:
                self.voting_weights[strategy] = 1.0

        # Normalize weights to sum to 3.0 (so average is 1.0)
        total = sum(self.voting_weights.values())
        for strategy in self.voting_weights:
            self.voting_weights[strategy] = 3.0 * self.voting_weights[strategy] / (total + 1e-8)

        # Log best strategy
        best_strategy = min(avg_losses, key=avg_losses.get)
        self.stats['best_strategy'] = best_strategy

        if self.stats['total_votes'] % 10 == 0:
            logger.info(f"ğŸ† [VOTING] Best strategy: {best_strategy}")
            logger.info(f"   Weights: {self.voting_weights}")

    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive voting statistics."""
        consensus_rate = (
            self.stats['consensus_reached'] / self.stats['total_votes']
            if self.stats['total_votes'] > 0 else 0.0
        )

        return {
            **self.stats,
            'consensus_rate': consensus_rate,
            'current_weights': self.voting_weights.copy(),
        }


# ============================================================================
# INTEGRATION HELPER FUNCTIONS
# ============================================================================

def wrap_trainer_with_voting(
    base_trainer,
    voting_threshold: float = 0.6,
    **kwargs
) -> VotingEnsembleTrainer:
    """
    Convenience function to wrap your existing trainer with voting mechanism.

    Usage:
        # Your existing code
        trainer = QuantumSystemTrainer(system, buffer, ...)

        # Upgrade to voting trainer
        voting_trainer = wrap_trainer_with_voting(trainer, voting_threshold=0.6)

        # Use voting trainer in your training loop
        metrics = voting_trainer.train_step()

    Args:
        base_trainer: Your QuantumSystemTrainer instance
        voting_threshold: Minimum agreement needed (0.6 = 60%)
        **kwargs: Additional arguments for VotingEnsembleTrainer

    Returns:
        VotingEnsembleTrainer instance
    """
    return VotingEnsembleTrainer(
        base_trainer=base_trainer,
        voting_threshold=voting_threshold,
        **kwargs
    )


def enable_quantum_advisor_logging(system):
    """
    Ensure quantum advisor logs are visible every prediction.
    """
    print("\n" + "="*80)
    print("ğŸ“ ENABLING QUANTUM ADVISOR LOGGING")
    print("="*80)

    # Set logger level to CRITICAL to ensure logs show
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.CRITICAL)

    # Add a handler if none exists
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(logging.CRITICAL)
        formatter = logging.Formatter('%(levelname)s - %(name)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        print("âœ… Added logging handler")

    # Verify quantum advisor will log
    if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
        print("âœ… Quantum advisor exists and will log on predictions")
    else:
        print("âŒ Quantum advisor missing - logs will not appear")

    print("="*80 + "\n")


# ================================================================
# USAGE INSTRUCTIONS
# ================================================================
"""
Add this to your code after the system is initialized:

# After system initialization
from quantum_advisor_fix import diagnose_quantum_advisor, fix_quantum_advisor_integration, enable_quantum_advisor_logging

# Run diagnostic
diagnostic_results = diagnose_quantum_advisor(system)

# If issues found, apply fix
if diagnostic_results['issues']:
    fix_quantum_advisor_integration(system)

    # Re-run diagnostic to verify
    diagnostic_results = diagnose_quantum_advisor(system)

# Enable logging
enable_quantum_advisor_logging(system)

# Now predictions should show quantum advisor logs!
"""

# ================================================================
# ALTERNATIVE: Patch the predict method directly
# ================================================================

def patch_agent_predict_with_quantum_advisor_logging(agent, agent_name):
    """
    Wrap an agent's predict method to ensure quantum advisor logging.
    """
    import functools
    import torch
    import numpy as np

    if not hasattr(agent, 'predict'):
        return

    original_predict = agent.predict

    @functools.wraps(original_predict)
    def predict_with_quantum_logging(*args, **kwargs):
        # Call original predict
        q_values = original_predict(*args, **kwargs)

        # Apply quantum advisor if available
        if hasattr(agent, 'quantum_advisor') and agent.quantum_advisor is not None:
            try:
                # Get current state
                state = agent.get_current_state_sequence() if hasattr(agent, 'get_current_state_sequence') else None

                if state is not None:
                    # Convert to tensor
                    if isinstance(state, np.ndarray):
                        state_tensor = torch.tensor(state, dtype=torch.float32, device=agent.device)
                        if state_tensor.dim() == 2:
                            state_tensor = state_tensor.unsqueeze(0)
                    else:
                        state_tensor = state

                    # Get quantum forecast
                    with torch.no_grad():
                        forecast, confidence = agent.quantum_advisor(state_tensor)
                        forecast = float(forecast.cpu().numpy())
                        confidence = float(confidence.cpu().numpy())

                    # LOG IT!
                    print(f"\nğŸ”® [QuantumAdvisor] {agent_name}")
                    print(f"    Original Q-values: BUY={q_values[0]:.6f} | SELL={q_values[1]:.6f}")
                    print(f"    Forecast: {forecast:+.6f} | Confidence: {confidence:.6f}")

                    # Apply adjustment
                    adjustment = forecast * confidence
                    q_values = q_values + adjustment

                    print(f"    Adjustment Applied: {adjustment:+.6f}")
                    print(f"    Adjusted Q-values: BUY={q_values[0]:.6f} | SELL={q_values[1]:.6f}")
                    print("    " + "-"*57)

            except Exception as e:
                print(f"âŒ [{agent_name}] Quantum advisor error: {e}")

        return q_values

    # Replace the method
    agent.predict = predict_with_quantum_logging
    print(f"âœ… Patched predict method for {agent_name}")


def patch_all_agents_with_quantum_logging(system):
    """
    Patch all agents to show quantum advisor logs.
    """
    print("\n" + "="*80)
    print("ğŸ”§ PATCHING ALL AGENTS WITH QUANTUM ADVISOR LOGGING")
    print("="*80 + "\n")

    if not hasattr(system, 'agents'):
        print("âŒ System has no agents")
        return

    patched_count = 0
    for name, agent in system.agents.items():
        try:
            patch_agent_predict_with_quantum_advisor_logging(agent, name)
            patched_count += 1
        except Exception as e:
            print(f"âŒ Failed to patch {name}: {e}")

    print(f"\nâœ… Patched {patched_count}/{len(system.agents)} agents")
    print("="*80 + "\n")


# ================================================================
# MAIN FIX FUNCTION - USE THIS!
# ================================================================

def apply_complete_quantum_advisor_fix(system):
    """
    Complete fix for quantum advisor integration and logging.
    Call this once after system initialization.
    """
    print("\n" + "="*120)
    print(" " * 40 + "ğŸ”® QUANTUM ADVISOR COMPLETE FIX")
    print("="*120 + "\n")

    # Step 1: Diagnose
    print("STEP 1: DIAGNOSTIC")
    diagnostic = diagnose_quantum_advisor(system)

    # Step 2: Fix if needed
    if diagnostic['issues']:
        print("\nSTEP 2: APPLYING FIXES")
        fixes = fix_quantum_advisor_integration(system)

        # Re-diagnose
        print("\nSTEP 3: RE-DIAGNOSTIC")
        diagnostic = diagnose_quantum_advisor(system)
    else:
        print("\nSTEP 2: No fixes needed")

    # Step 3: Enable logging
    print("\nSTEP 3: ENABLING LOGGING")
    enable_quantum_advisor_logging(system)

    # Step 4: Patch predict methods
    print("\nSTEP 4: PATCHING PREDICT METHODS")
    patch_all_agents_with_quantum_logging(system)

    # Final status
    print("\n" + "="*120)
    if not diagnostic['issues']:
        print(" " * 35 + "âœ… QUANTUM ADVISOR FULLY OPERATIONAL")
        print("\n" + " " * 25 + "Predictions will now show quantum advisor influence!")
    else:
        print(" " * 40 + "âš ï¸ SOME ISSUES REMAIN")
        print("\nRemaining issues:")
        for issue in diagnostic['issues']:
            print(f"   - {issue}")
    print("="*120 + "\n")

    return diagnostic


# -----------------------------------------------------------------------------
# 1ï¸âƒ£ Qiskit Import Fix
# -----------------------------------------------------------------------------
def patch_qiskit_imports():
    """Check Qiskit availability - execute() no longer needed (using transpile API)."""
    try:
        from qiskit import transpile
        from qiskit_aer import AerSimulator
        logger.critical("âœ… Qiskit available with new API (transpile + run)")
        return True
    except ImportError:
        logger.critical("âš ï¸ Qiskit not available â€” using classical fallbacks")
        return False
    except Exception as e:
        logger.critical(f"âš ï¸ Qiskit import check failed: {e}")
        return False


# -----------------------------------------------------------------------------
# 2ï¸âƒ£ Quantum Advisor Tensor Dimension Fix
# -----------------------------------------------------------------------------
def fix_quantum_advisor_tensor_dimensions(system):
    """
    Validate that quantum advisor tensor handling is correct.
    The actual fixes are in the _extract_quantum_forecast_features method.
    """
    if not hasattr(system, 'quantum_advisor') or system.quantum_advisor is None:
        logger.critical("âš ï¸ No quantum advisor found in system")
        return False

    advisor = system.quantum_advisor
    if not hasattr(advisor, '_extract_quantum_forecast_features'):
        logger.critical("âš ï¸ Advisor missing '_extract_quantum_forecast_features'")
        return False

    # The method is already fixed in the class definition
    # Just validate it exists and log success
    logger.critical("âœ… Quantum advisor tensor dimension handling validated")
    return True


# -----------------------------------------------------------------------------
# 3ï¸âƒ£ Apply All Hotfixes (No Fallback Creation)
# -----------------------------------------------------------------------------
def apply_quantum_advisor_hotfixes(system):
    """Apply all quantum advisor hotfixes (without fallback creation)."""
    print("\n" + "="*80)
    print("ğŸ”§ APPLYING QUANTUM ADVISOR HOTFIXES (CRITICAL MODE)")
    print("="*80)

    fixes_applied = []

    # Step 1: Patch Qiskit imports
    print("\n1ï¸âƒ£ Checking Qiskit availability...")
    if patch_qiskit_imports():
        fixes_applied.append("Qiskit available")
        print("   âœ… Qiskit available with new API")
    else:
        print("   âš ï¸ Qiskit not available (using classical fallbacks)")

    # Step 2: Fix tensor dimensions
    print("\n2ï¸âƒ£ Validating quantum advisor tensor dimensions...")
    if fix_quantum_advisor_tensor_dimensions(system):
        fixes_applied.append("Tensor dimensions validated")
        print("   âœ… Tensor dimension handling validated")
    else:
        print("   âš ï¸ Could not validate tensor handling (check advisor class)")

    # Step 3: Verify advisor functionality
    print("\n3ï¸âƒ£ Verifying quantum advisor...")
    try:
        qa = system.quantum_advisor
        # Use correct 2D shape: [batch_size, state_dim]
        test_state = torch.randn(2, getattr(system, "state_dim", 58))
        device = getattr(qa, "device", torch.device("cpu"))
        test_state = test_state.to(device)

        with torch.no_grad():
            forecast, confidence = qa(test_state)

        # Check shapes are correct
        expected_forecast_shape = (2, qa.forecast_horizon)
        expected_confidence_shape = (2, qa.forecast_horizon)

        if forecast.shape == expected_forecast_shape and confidence.shape == expected_confidence_shape:
            logger.critical(f"âœ… Quantum advisor verified | forecast shape={forecast.shape}, confidence shape={confidence.shape}")
            fixes_applied.append("Quantum advisor verified")
        else:
            logger.critical(f"âš ï¸ Shape mismatch | forecast={forecast.shape} (expected {expected_forecast_shape}), confidence={confidence.shape} (expected {expected_confidence_shape})")

    except Exception as e:
        logger.critical(f"âŒ Advisor verification failed: {e}")
        import traceback
        traceback.print_exc()

    # Summary
    print("\n" + "="*80)
    print(f"âœ… HOTFIXES COMPLETE: {len(fixes_applied)} applied")
    for fix in fixes_applied:
        print(f"   - {fix}")
    print("="*80 + "\n")

    return len(fixes_applied) > 0


# -----------------------------------------------------------------------------
# 4ï¸âƒ£ Emergency Fix - Disable Qiskit Inside Existing Advisor
# -----------------------------------------------------------------------------
def emergency_fix_qiskit_in_advisor(advisor):
    """Disable Qiskit usage within an existing advisor (uses classical trig features)."""
    if not advisor:
        logger.critical("âš ï¸ No advisor provided for emergency fix")
        return False

    try:
        def classical_feature_extraction(state):
            batch_size = state.size(0)
            seq_len = state.size(1) if state.dim() == 3 else 1
            state_dim = state.size(-1)
            if state.dim() == 2:
                state = state.unsqueeze(1)

            features = torch.zeros(batch_size, seq_len, 64, device=state.device)
            for i in range(seq_len):
                s = state[:, i, :]
                classical = torch.tanh(s)
                quantum = torch.sin(s * np.pi) * torch.cos(s * np.pi / 2)
                combined = torch.cat([classical, quantum], dim=-1)
                features[:, i, :] = combined
            return features

        advisor._extract_quantum_forecast_features = classical_feature_extraction
        logger.critical("âœ… Emergency fix applied: Disabled Qiskit inside advisor")
        return True

    except Exception as e:
        logger.critical(f"âŒ Emergency fix failed: {e}")
        return False


# 1ï¸âƒ£ Define the health check function first
def training_health_check(system):
    try:
        print(f"ğŸ”§ [HEALTH CHECK] Running at {time.ctime()}")
        if hasattr(system, "agents"):
            for agent_name, agent in system.agents.items():
                buffer_len = len(agent.buffer) if hasattr(agent, "buffer") else "N/A"
                print(f"Agent {agent_name} buffer length: {buffer_len}")
    except Exception as e:
        print(f"âš ï¸ [HEALTH CHECK] Error: {e}")

# 2ï¸âƒ£ Then define periodic_health_check
def periodic_health_check(system, interval=300):
    if not system._running:
        return
    training_health_check(system)
    threading.Timer(interval, periodic_health_check, args=(system, interval)).start()
# --- Asyncio error display ---
# ============================================================================
# CRITICAL FIXES FOR QUANTUM SYSTEM
# ============================================================================

# Fix 1: Ensure LogEntry is available globally
if 'LogEntry' not in dir():
    from dataclasses import dataclass
    from typing import Optional, Dict, Any

    @dataclass
    class LogEntry:
        timestamp: float
        level: str
        component: str
        message: str
        metadata: Optional[Dict[str, Any]] = None

# Fix 2: Safe logging helper
def safe_discord_log(message: str, level: str = 'INFO'):
    """Safe Discord logging that won't crash if batcher missing"""
    try:
        if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
            entry = {
                'timestamp': time.time(),
                'level': level,
                'component': 'QUANTUM_SYSTEM',
                'message': message,
                'metadata': None
            }
            if hasattr(logger.discord_batcher, 'add_log'):
                logger.discord_batcher.add_log(entry)
    except Exception:
        pass  # Silent fail for Discord logging

logger.info("Critical fixes loaded")

# REMOVED DUPLICATE: import asyncio
def safe_async_exception_handler(loop, context):
    msg = context.get("exception", context.get("message", ""))
    logger.error(f"[ASYNC ERROR] {msg}")
loop = asyncio.get_event_loop()
loop.set_exception_handler(safe_async_exception_handler)

logger.info("âœ… Colab Logging Booster initialized: Showing only INFO, ERROR & CRITICAL logs.")
# ============================================================================

# Create or get the logger
logger = logging.getLogger("QuantumSystemLogger")
logger.setLevel(logging.CRITICAL)

# Add console handler if none exists
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setLevel(logging.CRITICAL)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)

# 2. Fix asyncio issues
def fix_event_loop():
    """Fix event loop for Jupyter compatibility"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    # Set exception handler to suppress some errors
    def exception_handler(loop, context):
        exception = context.get('exception')
        if isinstance(exception, asyncio.CancelledError):
            return  # Ignore cancelled tasks
        print(f"Async error: {context.get('message', str(exception))}")

    loop.set_exception_handler(exception_handler)
    return loop

# Apply loop fix
fix_event_loop()


# ============================================================================
# SIGNAL VALIDATION - Prevent Fallback Signals
# ============================================================================


def extract_q_tensor_from_qvalue(q_value, preferred_tf='m', agent_name='agent'):
    """
    Extract a Q-value tensor from either a simple tensor or a multi-timeframe dict.

    Your multi-timeframe system returns Q-values as dicts like:
        {'xs': tensor, 's': tensor, 'm': tensor, 'l': tensor, 'xl': tensor, '5m': tensor}

    This function safely extracts the tensor for processing.

    Args:
        q_value: Either torch.Tensor or Dict[str, torch.Tensor]
        preferred_tf: Preferred timeframe key (default: 'm')
        agent_name: Agent name for logging

    Returns:
        tuple: (tensor, timeframe_used or None, is_dict_format)

    Examples:
        >>> # Dict format
        >>> q = {'m': torch.tensor([0.5, 0.5]), 's': torch.tensor([0.4, 0.6])}
        >>> tensor, tf, is_dict = extract_q_tensor_from_qvalue(q, preferred_tf='m')
        >>> print(tensor)  # torch.tensor([0.5, 0.5])
        >>> print(tf)      # 'm'

        >>> # Simple tensor format
        >>> q = torch.tensor([0.5, 0.5])
        >>> tensor, tf, is_dict = extract_q_tensor_from_qvalue(q)
        >>> print(tensor)  # torch.tensor([0.5, 0.5])
        >>> print(tf)      # None
    """
    if isinstance(q_value, dict):
        # Multi-timeframe dict format
        # Try preferred timeframe first
        if preferred_tf in q_value and q_value[preferred_tf] is not None:
            return q_value[preferred_tf], preferred_tf, True

        # Try other timeframes in priority order
        for tf in ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']:
            if tf in q_value and q_value[tf] is not None:
                logger.info(f"[{agent_name}] Preferred TF '{preferred_tf}' not available, using '{tf}'")
                return q_value[tf], tf, True

        # Fallback: first available (shouldn't reach here normally)
        if len(q_value) > 0:
            first_key = next(iter(q_value.keys()))
            logger.warning(f"[{agent_name}] No standard TF found, using first available: '{first_key}'")
            return q_value[first_key], first_key, True
        else:
            raise ValueError(f"[{agent_name}] Q-value dict is empty!")

    elif isinstance(q_value, torch.Tensor):
        # Simple tensor format
        return q_value, None, False

    else:
        raise TypeError(
            f"[{agent_name}] Q-value must be torch.Tensor or Dict[str, torch.Tensor], "
            f"got {type(q_value)}"
        )

def validate_prediction_quality(metadata):
    """
    Validate that prediction is not from fallback mechanism.
    Returns True if safe to publish, False if should block.
    """
    # Check for error in metadata (indicates fallback)
    if 'error' in metadata:
        logger.warning(f"ğŸš« BLOCKING FALLBACK SIGNAL: {metadata['error']}")
        return False

    # Check for zero coordination (another fallback indicator)
    avg_coord = metadata.get('avg_coordination', -1)
    if avg_coord == 0.0:
        logger.warning("ğŸš« BLOCKING LOW-QUALITY SIGNAL: Zero coordination")
        return False

    # Check for all coordination strengths being zero
    coord_strengths = metadata.get('coordination_strengths', {})
    if coord_strengths and all(v == 0.0 for v in coord_strengths.values()):
        logger.warning("ğŸš« BLOCKING SIGNAL: All agents have zero coordination")
        return False

    # Signal appears valid
    logger.info("âœ… Signal quality validated - OK to publish")
    return True

# === CRITICAL DIRECTORY FIX ===
def emergency_create_directories():
    """Create ALL directories that the trading system needs - PASTE AT TOP"""
    directories = [
        './saves',
        './saves/plots',
        './saves/models',
        './saves/data',
        './saves/agents',
        '/tmp/agents',
        '/tmp/rl_data',
        '/tmp/gpu_processing',
        '/tmp/batch_data'
    ]

    for directory in directories:
        try:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"Directory created/verified: {directory}")
        except Exception as e:
            logger.error(f"Failed to create {directory}: {e}")

    # Test critical file paths
    test_files = [
        './saves/rolling_ratio_plot.png',
        './saves/reward_plot.png'
    ]

    for test_file in test_files:
        try:
            Path(test_file).touch()
            os.remove(test_file)
            logger.info(f"Path verified: {test_file}")
        except Exception as e:
            logger.error(f"Path issue: {test_file} - {e}")

# === SAFE PLOTTING WRAPPER ===
def safe_plot_wrapper(original_plot_func):
    """Wrap plotting functions to prevent directory errors - PASTE AT TOP"""
    def wrapper(*args, **kwargs):
        try:
            # Always ensure directories exist before plotting
            os.makedirs('./saves', exist_ok=True)
            os.makedirs('./saves/plots', exist_ok=True)
            return original_plot_func(*args, **kwargs)
        except FileNotFoundError as e:
            if './saves' in str(e) or 'rolling_ratio_plot' in str(e):
                logger.warning(f"Creating missing directory and retrying plot...")
                os.makedirs('./saves', exist_ok=True)
                try:
                    return original_plot_func(*args, **kwargs)
                except Exception as retry_error:
                    logger.error(f"Plot retry failed: {retry_error}")
                    return None
            else:
                logger.warning(f"Plot file error (skipping): {e}")
                return None
        except Exception as e:
            logger.warning(f"Plot error (skipping): {e}")
            return None
    return wrapper

# --- Configuration ---
@dataclass
class RateLimitConfig:
    per_chat_interval: float = 1.5
    global_bulk_per_second: int = 20
    max_retries: int = 2
    retry_delay: float = 2.0
    max_queue_size: int = 1000

class MessagePriority(Enum):
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class DiscordMessage:
    content: str
    priority: MessagePriority
    timestamp: float
    embed: Optional[Dict[str, Any]] = None
    retry_count: int = 0

# --- Rate Limiter ---
class RateLimiter:
    def __init__(self, config: RateLimitConfig):
        self.config = config
        self.last_send_time = 0
        self.bulk_send_times = deque(maxlen=config.global_bulk_per_second)
        self.lock = threading.Lock()

    def can_send(self) -> tuple[bool, float]:
        with self.lock:
            current_time = time.time()
            time_since_last = current_time - self.last_send_time

            if time_since_last < self.config.per_chat_interval:
                return False, self.config.per_chat_interval - time_since_last

            while self.bulk_send_times and current_time - self.bulk_send_times[0] > 1.0:
                self.bulk_send_times.popleft()

            if len(self.bulk_send_times) >= self.config.global_bulk_per_second:
                wait_time = 1.0 - (current_time - self.bulk_send_times[0])
                return False, max(wait_time, 0.1)

            return True, 0.0

    def record_send(self):
        with self.lock:
            current_time = time.time()
            self.last_send_time = current_time
            self.bulk_send_times.append(current_time)
# REPLACE THE ENTIRE DiscordWebhookSender CLASS (around line 90-200 in your code):

class DiscordWebhookSender:
    """Discord webhook sender with rate limiting and priority queue"""

    def __init__(self, webhook_url: str, config: RateLimitConfig):
        self.webhook_url = webhook_url
        self.config = config
        self.rate_limiter = RateLimiter(config)
        self.message_queue = queue.PriorityQueue(maxsize=config.max_queue_size)
        self.running = False
        self.worker_thread = None
        self.stats = {
            'messages_sent': 0,
            'messages_failed': 0,
            'messages_dropped': 0,
            'queue_size': 0,
            'avg_send_time': 0.0
        }
        self.stats_lock = threading.Lock()

    def start(self):
        """Start the webhook sender worker thread"""
        if not self.running:
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logger.info("Discord webhook sender started")

    def stop(self):
        """Stop the webhook sender with proper cleanup"""
        self.running = False

        # Send poison pills with priority tuple format
        try:
            # Use priority 0 (highest) and current timestamp for poison pill
            self.message_queue.put((0, time.time(), None), timeout=1.0)
        except queue.Full:
            pass

        if self.worker_thread:
            self.worker_thread.join(timeout=5.0)

        logger.info("Discord webhook sender stopped")

    def _worker_loop(self):
        """Worker loop with fixed queue unpacking"""
        while self.running:
            try:
                try:
                    # Unpack three elements from priority queue
                    priority, timestamp, msg = self.message_queue.get(timeout=0.5)
                except queue.Empty:
                    continue

                if msg is None:  # Poison pill
                    break

                can_send, wait_time = self.rate_limiter.can_send()

                if not can_send:
                    try:
                        # Re-queue with same priority and timestamp
                        self.message_queue.put((priority, timestamp, msg), block=False)
                    except queue.Full:
                        self._update_stat('messages_dropped', 1)
                    time.sleep(wait_time)
                    continue

                success = self._send_direct(msg)

                if success:
                    self.rate_limiter.record_send()
                    self._update_stat('messages_sent', 1)
                elif msg.retry_count < self.config.max_retries:
                    msg.retry_count += 1
                    time.sleep(self.config.retry_delay * (2 ** msg.retry_count))
                    try:
                        # Re-queue with updated retry count
                        self.message_queue.put((priority, timestamp, msg), block=False)
                    except queue.Full:
                        self._update_stat('messages_dropped', 1)

                with self.stats_lock:
                    self.stats['queue_size'] = self.message_queue.qsize()

            except Exception as e:
                logger.error(f"Discord worker error: {e}")
                time.sleep(1.0)

    def _send_direct(self, msg: DiscordMessage) -> bool:
        """Send message directly to Discord webhook"""
        try:
            start_time = time.time()
            payload = {'embeds': [msg.embed]} if msg.embed else {'content': msg.content}
            response = requests.post(self.webhook_url, json=payload, timeout=10)

            send_time = time.time() - start_time
            with self.stats_lock:
                self.stats['avg_send_time'] = 0.1 * send_time + 0.9 * self.stats['avg_send_time']

            if response.status_code == 204:
                return True
            elif response.status_code == 429:
                time.sleep(response.json().get('retry_after', 5) / 1000.0)
                return False
            else:
                self._update_stat('messages_failed', 1)
                logger.warning(f"Discord webhook error: {response.status_code}")
                return False

        except Exception as e:
            logger.error(f"Discord send error: {e}")
            self._update_stat('messages_failed', 1)
            return False

    def _update_stat(self, key: str, value: int):
        """Update statistics with thread safety"""
        with self.stats_lock:
            self.stats[key] = self.stats.get(key, 0) + value

    def send(self, content: str = None, embed: Dict[str, Any] = None,
             priority: MessagePriority = MessagePriority.MEDIUM) -> bool:
        """Send message with fixed priority queue handling"""
        try:
            msg = DiscordMessage(
                content=content or "",
                priority=priority,
                timestamp=time.time(),
                embed=embed
            )
            # Add timestamp as tiebreaker to prevent comparison errors
            self.message_queue.put((priority.value, time.time(), msg), block=False)
            return True
        except queue.Full:
            self._update_stat('messages_dropped', 1)
            logger.warning("Discord message queue full, dropping message")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics"""
        with self.stats_lock:
            return self.stats.copy()

# --- Embed Builder ---
class DiscordEmbedBuilder:
    COLORS = {
        'signal': 0x3498db,
        'reward': 0x2ecc71,
        'mode': 0xe74c3c,
        'summary': 0x9b59b6,
        'error': 0xe67e22
    }

    @staticmethod
    def signal_change(signal: str, price: float = None, confidence: float = None):
        embed = {
            'title': 'Signal Change',
            'color': DiscordEmbedBuilder.COLORS['signal'],
            'fields': [{'name': 'New Signal', 'value': f'**{signal}**', 'inline': True}],
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }
        if price:
            embed['fields'].append({'name': 'Price', 'value': f'${price:,.2f}', 'inline': True})
        if confidence:
            embed['fields'].append({'name': 'Confidence', 'value': f'{confidence:.1%}', 'inline': True})
        return embed

    @staticmethod
    def reward_update(count: int, latest: float = None, avg: float = None):
        embed = {
            'title': 'Reward Update 2',
            'color': DiscordEmbedBuilder.COLORS['reward'],
            'fields': [{'name': 'Total', 'value': f'**{count:,}**', 'inline': True}],
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }
        if latest is not None:
            embed['fields'].append({'name': 'Latest', 'value': f'{latest:+.4f}', 'inline': True})
        if avg is not None:
            embed['fields'].append({'name': 'Avg', 'value': f'{avg:+.4f}', 'inline': True})
        return embed

    @staticmethod
    def mode_change(mode: str, steps: int, threshold: int):
        return {
            'title': 'Mode Change 2',
            'color': DiscordEmbedBuilder.COLORS['mode'],
            'fields': [
                {'name': 'New Mode', 'value': f'**{mode}**', 'inline': False},
                {'name': 'Progress', 'value': f'{steps:,}/{threshold:,}', 'inline': True},
                {'name': 'Complete', 'value': f'{steps/threshold*100:.1f}%', 'inline': True}
            ],
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }

    @staticmethod
    def signal_summary(buy: int, sell: int, total: int):
        return {
            'title': 'Signal Summary 2',
            'color': DiscordEmbedBuilder.COLORS['summary'],
            'fields': [
                {'name': 'BUY', 'value': f'{buy} ({buy/total*100:.1f}%)', 'inline': True},
                {'name': 'SELL', 'value': f'{sell} ({sell/total*100:.1f}%)', 'inline': True}
            ],
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S.000Z', time.gmtime())
        }

# --- High-Level Notifier ---
class DiscordNotifier:
    def __init__(self, sender: DiscordWebhookSender):
        self.sender = sender
        self.builder = DiscordEmbedBuilder()

    def notify_signal_change(self, signal: str, price: float = None, confidence: float = None):
        embed = self.builder.signal_change(signal, price, confidence)
        return self.sender.send(embed=embed, priority=MessagePriority.HIGH)

    def notify_reward_update(self, count: int, latest: float = None, avg: float = None):
        embed = self.builder.reward_update(count, latest, avg)
        return self.sender.send(embed=embed, priority=MessagePriority.MEDIUM)

    def notify_mode_change(self, mode: str, steps: int, threshold: int):
        embed = self.builder.mode_change(mode, steps, threshold)
        return self.sender.send(embed=embed, priority=MessagePriority.CRITICAL)

    def notify_signal_summary(self, buy: int, sell: int, total: int):
        embed = self.builder.signal_summary(buy, sell, total)
        return self.sender.send(embed=embed, priority=MessagePriority.LOW)
# ============================================================================
# DISCORD INTEGRATION FIX - PASTE THIS TO REPLACE EXISTING DISCORD CODE
# ============================================================================
def start_discord_monitoring(system, interval=300):
    """
    FIXED: Discord monitoring with proper attribute access and safety checks
    """
    def monitor():
        while True:
            time.sleep(interval)
            try:
                # FIX: Check if discord_batcher exists on logger
                if not hasattr(logger, 'discord_batcher'):
                    logger.debug("Discord batcher not available on logger object")
                    continue

                if logger.discord_batcher is None:
                    logger.debug("Discord batcher is None")
                    continue

                # Now safe to call get_stats()
                stats = logger.discord_batcher.get_stats()

                logger.info(
                    f"Discord Stats: {stats['messages_sent_success']} sent, "
                    f"{stats['messages_sent_failed']} failed, "
                    f"unsent: {stats['unsent_logs_count']}, "
                    f"queue: {stats.get('queue_size', 0)}"
                )

                # Check for issues
                if stats['messages_sent_failed'] > 10:
                    logger.warning(f"High Discord failure rate: {stats['messages_sent_failed']} failed")

                if stats['unsent_logs_count'] > 40:
                    logger.warning(f"Discord queue backing up: {stats['unsent_logs_count']} unsent")

            except AttributeError as e:
                logger.debug(f"Discord monitor attribute error: {e}")
            except Exception as e:
                logger.debug(f"Discord monitor error (non-critical): {e}")

    # Start monitor thread
    Thread(target=monitor, daemon=True).start()
    logger.info("Discord monitoring started with safety checks")

# Step 5: Fix Discord Stats Access (ADD this helper function)

def get_discord_stats(self):
    # Return empty dict or default stats
    return {"messages_sent": 0, "errors": 0}

# Step 6: Fix Force Send Discord Report (ADD this helper function)

def force_send_discord_report():
    """
    SAFE way to force Discord report - handles all error cases
    """
    try:
        if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
            logger.discord_batcher.force_send_report()
            logger.info("Discord report forced")
            return True
        else:
            logger.warning("Discord batcher not available for force send")
            return False
    except Exception as e:
        logger.error(f"Force send Discord report failed: {e}")
        return False

# Step 7: Fix Integration with System (REPLACE existing integrate_with_system)

def integrate_discord_with_system(system):
    """
    FIXED: Integrate Discord logger with trading system
    """
    try:
        # Enhance get_system_status to include Discord stats
        if hasattr(system, 'get_system_status'):
            original_get_status = system.get_system_status

            def enhanced_get_status():
                status = original_get_status()
                discord_stats = get_discord_stats()  # Use safe helper

                status['discord_logging'] = {
                    'reports_sent': discord_stats.get('total_reports_sent', 0),
                    'unsent_logs': discord_stats.get('unsent_logs_count', 0),
                    'send_threshold': 50,
                    'success_rate': (
                        discord_stats.get('messages_sent_success', 0) /
                        max(1, discord_stats.get('messages_sent_success', 0) +
                            discord_stats.get('messages_sent_failed', 0))
                    ) if discord_stats else 0.0
                }
                return status

            system.get_system_status = enhanced_get_status
            logger.info("âœ… Discord integrated with system.get_system_status()")

        # Add Discord force send to reward processing
        if hasattr(system, '_save_reward'):
            original_save_reward = system._save_reward

            def enhanced_save_reward(reward):
                original_save_reward(reward)
                # Force Discord report every 100 rewards
                if len(system.reward_history) % 100 == 0:
                    force_send_discord_report()

            system._save_reward = enhanced_save_reward
            logger.info("âœ… Discord integrated with reward processing")

        logger.info("âœ… Discord fully integrated with system")
        return True

    except Exception as e:
        logger.error(f"Discord integration failed: {e}")
        return False

# Step 8: Verify Discord Batcher is Running (ADD this check)

def verify_discord_health():
    """
    Check Discord system health and restart if needed
    """
    try:
        if not hasattr(logger, 'discord_batcher'):
            logger.critical("âŒ CRITICAL: Discord batcher not found on logger!")
            return False

        if not logger.discord_batcher:
            logger.critical("âŒ CRITICAL: Discord batcher is None!")
            return False

        # Check if batcher is running
        if not logger.discord_batcher.is_running:
            logger.warning("âš ï¸ Discord batcher not running - attempting restart...")
            try:
                logger.discord_batcher.start()
                logger.info("âœ… Discord batcher restarted")
            except Exception as e:
                logger.error(f"Failed to restart Discord batcher: {e}")
                return False

        # Test getting stats
        stats = logger.discord_batcher.get_stats()
        logger.info(f"âœ… Discord health check passed: {stats['total_reports_sent']} reports sent")
        return True

    except Exception as e:
        logger.error(f"Discord health check failed: {e}")
        return False

# Step 9: ADD Enhanced Auto-Restart Monitor

def start_discord_health_monitor(interval=60):
    """
    Monitor Discord health and auto-restart if needed
    """
    def health_monitor():
        consecutive_failures = 0
        max_failures = 3

        while True:
            time.sleep(interval)
            try:
                if verify_discord_health():
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    logger.warning(f"Discord health check failed ({consecutive_failures}/{max_failures})")

                    if consecutive_failures >= max_failures:
                        logger.critical("Discord system unhealthy - attempting full restart...")
                        try:
                            # Stop existing batcher
                            if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
                                logger.discord_batcher.stop()

                            # Create new batcher
                            logger.discord_batcher = DiscordLogBatcher(
                                webhook_url=WEBHOOK_URL,
                                batch_size=50
                            )
                            logger.discord_batcher.start()

                            logger.info("âœ… Discord system restarted")
                            consecutive_failures = 0

                        except Exception as e:
                            logger.error(f"Discord restart failed: {e}")

            except Exception as e:
                logger.debug(f"Health monitor error: {e}")

    Thread(target=health_monitor, daemon=True).start()
    logger.info("Discord health monitor started")

# === ASYNCIO TASK MANAGER ===
class SafeTaskManager:
    """Manage asyncio tasks safely to prevent conflicts - PASTE AT TOP"""

    def __init__(self):
        self.active_tasks = set()
        self.task_lock = threading.Lock()
        self.cleanup_counter = 0

    def create_safe_task(self, coro, name=None):
        """Create task with proper tracking and cleanup"""
        try:
            loop = asyncio.get_event_loop()
            task = loop.create_task(coro)
            if name:
                task.set_name(name)

            with self.task_lock:
                self.active_tasks.add(task)

            # Add cleanup callback
            task.add_done_callback(self._cleanup_task)
            return task
        except Exception as e:
            logger.error(f"Failed to create safe task {name}: {e}")
            return None

    def _cleanup_task(self, task):
        """Clean up completed tasks"""
        with self.task_lock:
            self.active_tasks.discard(task)

        # Suppress common harmless exceptions
        if task.exception():
            exception = task.exception()
            if not isinstance(exception, (asyncio.CancelledError, ConnectionError)):
                logger.debug(f"Task {task.get_name()} exception: {exception}")

        self.cleanup_counter += 1
        if self.cleanup_counter % 50 == 0:
            logger.info(f"Task cleanup: {self.cleanup_counter} tasks cleaned, {len(self.active_tasks)} active")

    def cancel_all_tasks(self):
        """Cancel all tracked tasks"""
        with self.task_lock:
            for task in list(self.active_tasks):
                if not task.done():
                    task.cancel()
        logger.info(f"Cancelled {len(self.active_tasks)} tasks")

    def get_task_count(self):
        """Get current active task count"""
        with self.task_lock:
            return len(self.active_tasks)

# === ABLY CONNECTION STABILIZER ===
class AblyConnectionStabilizer:
    """Stabilize Ably connections - PASTE AT TOP"""

    def __init__(self, ably_client):
        self.ably_client = ably_client
        self.connection_stable = False
        self.last_check = 0
        self.check_interval = 10  # seconds

    def is_connected(self):
        """Quick connection check"""
        if not self.ably_client:
            return False

        try:
            current_state = getattr(self.ably_client.connection, 'state', 'unknown')
            self.connection_stable = (current_state == 'connected')
            return self.connection_stable
        except Exception:
            self.connection_stable = False
            return False

    def should_check_connection(self):
        """Rate limit connection checks"""
        current_time = time.time()
        if current_time - self.last_check > self.check_interval:
            self.last_check = current_time
            return True
        return False

# === ENHANCED EXCEPTION HANDLER ===
def setup_safe_exception_handling(loop):
    """Setup safe exception handling for asyncio loop - PASTE AT TOP"""
    def safe_exception_handler(loop, context):
        exception = context.get('exception')

        # Suppress common harmless exceptions that flood logs
        if isinstance(exception, (
            asyncio.CancelledError,
            ConnectionError,
            OSError
        )):
            return

        # Suppress specific Ably connection errors
        if exception and 'ConnectionClosedOK' in str(type(exception)):
            return

        # Suppress task conflict errors (these are handled by task manager)
        message = context.get('message', '')
        if any(phrase in message for phrase in [
            'does not match the current task',
            'Task was destroyed but it is pending',
            'Cannot enter into task'
        ]):
            return

        # Log only genuinely important errors
        logger.debug(f"Asyncio: {context.get('message', 'Unknown error')}")

    loop.set_exception_handler(safe_exception_handler)




# ============================================================================
# QUANTUM COMPUTING IMPORTS
# ============================================================================
try:
    import qiskit
    import qiskit_aer  # âœ… Add this line
    from qiskit import QuantumCircuit, transpile
    from qiskit_aer import AerSimulator

    QISKIT_AVAILABLE = True
    _SV_AER = AerSimulator(method='statevector')
    _AER = AerSimulator(method='automatic')

    print("âœ… Qiskit available: Full quantum features enabled")
    print(f"   Qiskit version: {qiskit.__version__}")
    print(f"   Qiskit-Aer version: {qiskit_aer.__version__}")

except ImportError as e:
    QISKIT_AVAILABLE = False
    _SV_AER = None
    _AER = None
    print("âš ï¸  Qiskit not available: Using classical fallbacks for quantum features")
    print(f"   Import error: {e}")
    print("   Install with: pip install qiskit qiskit-aer")



# TensorFlow configuration
tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()

# Asyncio configuration
nest_asyncio.apply()

# ============================================================================
# LOGGING CONFIGURATION - Enhanced for V6
# ============================================================================
os.environ['PYTHONUNBUFFERED'] = '1'

# Force unbuffered output for real-time streaming
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(line_buffering=True)
except Exception:
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, line_buffering=True)

# Base logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('quantum_trading_system_v6.log', mode='a')
    ],
    force=True
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Critical logger for important system events
critical_logger = logging.getLogger('CRITICAL')
critical_logger.setLevel(logging.CRITICAL)

# Quantum-specific logger
quantum_logger = logging.getLogger('QUANTUM')
quantum_logger.setLevel(logging.INFO)

# Color formatting (if colorama available)
try:
    from colorama import Fore, Style, init
    init(autoreset=True)

    class ColorFormatter(logging.Formatter):
        COLORS = {
            'DEBUG': Fore.BLUE,
            'INFO': Fore.GREEN,
            'WARNING': Fore.YELLOW,
            'ERROR': Fore.RED,
            'CRITICAL': Fore.MAGENTA
        }

        def format(self, record):
            color = self.COLORS.get(record.levelname, '')
            timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S')
            msg = f"{timestamp} | {record.levelname:<8} | {record.name} | {record.getMessage()}"
            return f"{color}{msg}{Style.RESET_ALL}"

    for handler in logger.handlers:
        handler.setFormatter(ColorFormatter())
except Exception as e:
    print(f"[LoggingBooster] Color formatting disabled: {e}")

# Live flushing
for handler in logger.handlers:
    handler.flush = sys.stdout.flush

# TensorFlow verbosity
try:
    tf.get_logger().setLevel('ERROR')
    tf.autograph.set_verbosity(1)
except:
    pass

# ============================================================================
# SILENCE QISKIT VERBOSE LOGGING
# ============================================================================
# Qiskit produces extremely verbose INFO logs during transpilation
# Silence all qiskit loggers to WARNING level or above
qiskit_loggers = [
    'qiskit',
    'qiskit.compiler',
    'qiskit.compiler.transpiler',
    'qiskit.passmanager',
    'qiskit.passmanager.base_tasks',
    'qiskit.transpiler',
    'qiskit.providers',
    'qiskit.circuit',
]

for qiskit_logger_name in qiskit_loggers:
    qiskit_logger = logging.getLogger(qiskit_logger_name)
    qiskit_logger.setLevel(logging.WARNING)  # Only show WARNING and above
    qiskit_logger.propagate = False  # Don't propagate to root logger

# Also silence by pattern matching
for name in list(logging.root.manager.loggerDict.keys()):
    if 'qiskit' in name.lower():
        logging.getLogger(name).setLevel(logging.WARNING)
        logging.getLogger(name).propagate = False

logger.info("âœ… Qiskit logging silenced (WARNING level only)")
# ============================================================================

# ============================================================================
# SYSTEM CONSTANTS
# ============================================================================

# API Keys and Configuration
ABLY_API_KEY = "4vT80g.E0lfvg:reqqX942--QJVafOQsgRDWsBXIDtgDxg51szTmLkIeM"

# Trading timeframes
 # Extra Small to Extra Large
# ============================================================================
# HIGH-FREQUENCY TIMEFRAME CONFIGURATION (SECONDS)
# ============================================================================

FEATURE_WINDOW = 10  # Base lookback in number of candles

TIMEFRAMES = ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']

# Automatic scaling based on FEATURE_WINDOW

EXPECTED_TIMEFRAMES = sorted(TIMEFRAME_LENGTHS.keys())

# Model architecture
DEFAULT_STATE_DIM = 58
STATE_DIM = 64  # Legacy compatibility
DEFAULT_ACTION_DIM = 2
ACTION_DIM = 2  # Legacy compatibility
DEFAULT_HIDDEN_DIM = 256
DEFAULT_LATENT_DIM = 32
DEFAULT_NUM_HEADS = 8
DEFAULT_NUM_LAYERS = 3

# Training parameters
LEARNING_RATE = 3e-4
BATCH_SIZE = 32
BUFFER_SIZE = 100000000
GAMMA = 0.94          # â†“ Reduce from 0.97 â†’ 0.94 (more immediate)
TAU = 0.003           # â†“ Reduce from 0.01 â†’ 0.003 (more stable)
UPDATE_FREQUENCY = 4
GRADIENT_CLIP =  5.0
EPSILON_START = 1.0
EPSILON_END = 0.45      # 45% random at minimum
EPSILON_DECAY = 0.9995  # ~20k steps to reach END


# Quantum parameters
QUANTUM_N_QUBITS = 6
QUANTUM_N_LAYERS = 2
QUANTUM_ENTANGLEMENT_STRENGTH = 0.25 
MEASUREMENT_SHOTS = 512  
QUANTUM_WEIGHT = 0.15 # Weight for quantum vs classical

logger.info("="*80)
logger.info("QUANTUM TRADING SYSTEM V6 - INITIALIZATION")
logger.info("="*80)
logger.info(f"Qiskit Available: {QISKIT_AVAILABLE}")
logger.info(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
logger.info(f"Quantum Qubits: {QUANTUM_N_QUBITS}")
logger.info(f"State Dimension: {DEFAULT_STATE_DIM}")
logger.info(f"Action Dimension: {DEFAULT_ACTION_DIM}")
logger.info("="*80)

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: import logging
from typing import Any, Optional

logger = logging.getLogger(__name__)

def ensure_tensor(x: Any, dtype: torch.dtype = torch.float32,
                  device: Optional[torch.device] = None) -> torch.Tensor:
    """
    Convert any input to a proper torch.Tensor with logging.
    If conversion fails, returns a tensor of zeros with shape [1].

    Args:
        x: Input data (scalar, list, tuple, np.ndarray, or torch.Tensor)
        dtype: Desired torch dtype
        device: Desired device

    Returns:
        torch.Tensor of at least 1D
    """
    if x is None:
        return torch.zeros(1, dtype=dtype, device=device)

    if isinstance(x, torch.Tensor):
        tensor = x.to(dtype=dtype)
    else:
        try:
            if isinstance(x, (list, tuple)):
                tensor = torch.tensor(x, dtype=dtype)
            elif isinstance(x, np.ndarray):
                tensor = torch.from_numpy(x).to(dtype)
            else:
                tensor = torch.tensor([x], dtype=dtype)  # Wrap scalar in list
        except Exception as e:
            logger.warning(f"Failed to convert to tensor: {e}")
            tensor = torch.zeros(1, dtype=dtype)

    if device is not None:
        tensor = tensor.to(device)

    # Ensure at least 1D
    if tensor.dim() == 0:
        tensor = tensor.unsqueeze(0)

    return tensor




def safe_mean(tensor_list: List[torch.Tensor]) -> torch.Tensor:
    """
    Compute the mean of a list of tensors safely.
    Ignores None or empty tensors. Flattens tensors if shapes differ.

    Args:
        tensor_list: List of torch.Tensor

    Returns:
        torch.Tensor: mean tensor, or tensor([0.0]) if input is empty or fails
    """
    valid_tensors = [t for t in tensor_list if t is not None and t.numel() > 0]
    if not valid_tensors:
        return torch.tensor([0.0])

    try:
        # Ensure all tensors have same shape
        shapes = [t.shape for t in valid_tensors]
        if len(set(shapes)) > 1:
            # Flatten all tensors if shapes differ
            valid_tensors = [t.flatten() for t in valid_tensors]

        stacked = torch.stack(valid_tensors)
        return stacked.mean(dim=0)
    except Exception as e:
        logger.warning(f"Error in safe_mean: {e}")
        return torch.tensor([0.0])


def normalize_tensor(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:

    mean = x.mean()
    std = x.std() + eps
    return (x - mean) / std

def log_tensor_stats(name: str, tensor: torch.Tensor, level: int = logging.DEBUG):

    if logger.isEnabledFor(level):
        logger.log(level, f"{name} shape={tensor.shape}, "
                         f"mean={tensor.mean():.4f}, "
                         f"std={tensor.std():.4f}, "
                         f"min={tensor.min():.4f}, "
                         f"max={tensor.max():.4f}")

# ============================================================================
# DATA STRUCTURES
# ============================================================================


@dataclass
class Experience:

    state: torch.Tensor
    action: int
    reward: float
    next_state: torch.Tensor
    done: bool
    info: Dict[str, Any] = field(default_factory=dict)
    priority: float = 1.0

logger.info("âœ… Data structures and utilities initialized")



# ============================================================================
# ATTENTION MECHANISMS
# ============================================================================

class MultiHeadAttention(nn.Module):

    def __init__(self, d_model: int, n_heads: int = 6, dropout: float = 0.45):
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size = query.size(0)

        # Ensure 3D tensors
        if query.dim() == 2:
            query = query.unsqueeze(1)
        if key.dim() == 2:
            key = key.unsqueeze(1)
        if value.dim() == 2:
            value = value.unsqueeze(1)

        # Linear projections
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        context = torch.matmul(attention_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )

        output = self.W_o(context)
        output = self.layer_norm(output + query)

        return output

class CrossTimeframeAttention(nn.Module):


    def __init__(self, d_model: int, n_timeframes: int = len(TIMEFRAMES),
                 n_heads: int = 8):
        super().__init__()
        self.d_model = d_model
        self.n_timeframes = n_timeframes

        self.timeframe_embeddings = nn.Embedding(n_timeframes, d_model)
        self.attention = MultiHeadAttention(d_model, n_heads)
        self.output_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(0.1)

        logger.debug(f"CrossTimeframeAttention initialized: d_model={d_model}, "
                    f"n_timeframes={n_timeframes}")

    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:

        if not features:
            logger.debug("CrossTimeframeAttention: No features provided")
            return torch.zeros(1, self.d_model)

        # Collect valid features
        valid_features = []
        timeframe_indices = []

        for i, tf in enumerate(TIMEFRAMES):
            if tf in features and features[tf] is not None:
                feat = features[tf]
                if feat.dim() == 1:
                    feat = feat.unsqueeze(0)
                valid_features.append(feat)
                timeframe_indices.append(i)

        if not valid_features:
            logger.debug("CrossTimeframeAttention: No valid features")
            return torch.zeros(1, self.d_model)

        # Stack features
        stacked = torch.stack(valid_features, dim=1)
        batch_size = stacked.size(0)

        # Add timeframe embeddings
        indices = torch.tensor(timeframe_indices, dtype=torch.long)
        embeddings = self.timeframe_embeddings(indices)
        embeddings = embeddings.unsqueeze(0).expand(batch_size, -1, -1)

        embedded = stacked + embeddings

        # Apply attention
        attended = self.attention(embedded, embedded, embedded)

        # Aggregate
        aggregated = attended.mean(dim=1)

        # Output projection
        output = self.output_proj(aggregated)
        output = self.dropout(output)

        return output

# ============================================================================
# PRIORITIZED REPLAY BUFFER
# ============================================================================

class PrioritizedReplayBuffer:

    def __init__(self, capacity: int = BUFFER_SIZE, alpha: float = 0.6,
                 beta: float = 0.4):
        self.capacity = capacity
        self.alpha = alpha  # Priority exponent
        self.beta = beta    # Importance sampling exponent
        self.beta_increment = 0.001

        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.position = 0

        logger.info(f"âœ… PrioritizedReplayBuffer initialized (capacity={capacity})")

    def push(self, experience: Experience, priority: Optional[float] = None):

        if priority is None:
            priority = max(self.priorities) if self.priorities else 1.0

        self.buffer.append(experience)
        self.priorities.append(priority)

    def sample(self, batch_size: int) -> Tuple[List[Experience], torch.Tensor, torch.Tensor]:

        if len(self.buffer) < batch_size:
            logger.warning(f"Buffer too small: {len(self.buffer)} < {batch_size}")
            return [], torch.zeros(0), torch.zeros(0)

        # Calculate sampling probabilities
        priorities = np.array(list(self.priorities))
        probs = priorities ** self.alpha
        probs /= probs.sum()

        # Sample indices
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)

        # Calculate importance weights
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        weights = torch.tensor(weights, dtype=torch.float32)

        # Get experiences
        experiences = [self.buffer[i] for i in indices]

        # Update beta
        self.beta = min(1.0, self.beta + self.beta_increment)

        return experiences, weights, torch.tensor(indices, dtype=torch.long)

    def update_priorities(self, indices: torch.Tensor, priorities: torch.Tensor):

        for idx, priority in zip(indices.tolist(), priorities.tolist()):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = max(abs(priority), 1e-6)

    def __len__(self):
        return len(self.buffer)

# ============================================================================
# NEURAL NETWORK COMPONENTS
# ============================================================================

class Actor(nn.Module):


    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = DEFAULT_HIDDEN_DIM):
        super().__init__()

        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim)
        )

        logger.debug(f"Actor initialized: state_dim={state_dim}, action_dim={action_dim}")

    def forward(self, state: torch.Tensor) -> torch.Tensor:

        logits = self.net(state)
        return F.softmax(logits, dim=-1)

    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> int:

        probs = self.forward(state)

        if deterministic:
            action = torch.argmax(probs, dim=-1)
        else:
            dist = Categorical(probs)
            action = dist.sample()

        return action.item() if action.numel() == 1 else action

class Critic(nn.Module):


    def __init__(self, state_dim: int, action_dim: int = None,
                 hidden_dim: int = DEFAULT_HIDDEN_DIM):
        super().__init__()

        input_dim = state_dim + (action_dim if action_dim else 0)

        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        logger.debug(f"Critic initialized: state_dim={state_dim}, action_dim={action_dim}")

    def forward(self, state: torch.Tensor, action: Optional[torch.Tensor] = None) -> torch.Tensor:

        if action is not None:
            if action.dim() == 1:
                action = action.unsqueeze(-1)
            x = torch.cat([state, action], dim=-1)
        else:
            x = state
        return self.net(x)

logger.info("âœ… Neural network components initialized")


# ============================================================================
# ENHANCED QUANTUM AGENT WITH ACTOR-CRITIC
# ============================================================================

# ============================================================================
# MULTI-TIMEFRAME ENTANGLED AGENT
# ============================================================================


# ============================================================================
# Q-VALUE TEMPERING AND QUANTUM INTEGRATION
# ============================================================================
def apply_quantum_forecast(system, q_values: Dict[str, torch.Tensor],
                          states_dict: Dict[str, Any]) -> Tuple[Dict[str, torch.Tensor], Dict]:
    """
    Enhanced quantum forecast application with CRITICAL logging.

    FIXED VERSION: Now handles:
    - Simple tensors and multi-timeframe dict Q-values
    - Both PyTorch tensors AND numpy arrays

    Applies quantum advisor tempering to all agent Q-values and logs
    the entire process at CRITICAL level for visibility.
    """
    logger.critical("="*80)
    logger.critical("ğŸ”® QUANTUM FORECAST APPLICATION STARTED")
    logger.critical("="*80)

    metadata = {
        "quantum_forecast": {
            "applied": False,
            "error": None,
            "adjustments": {},
            "timestamp": time.time()
        }
    }

    # Check if quantum advisor exists
    if not hasattr(system, "quantum_advisor") or system.quantum_advisor is None:
        metadata["quantum_forecast"]["error"] = "No quantum advisor"
        logger.critical("âŒ [QUANTUM FORECAST] Quantum advisor not available - skipping tempering")
        return q_values, metadata

    logger.critical(f"âœ… [QUANTUM FORECAST] Quantum advisor active | Processing {len(q_values)} agents")

    adjusted_q_values = {}
    tempering_stats = {
        'total_agents': len(q_values),
        'successful': 0,
        'failed': 0,
        'skipped': 0
    }

    try:
        for agent_name, agent_q in q_values.items():
            if agent_q is None:
                adjusted_q_values[agent_name] = None
                tempering_stats['skipped'] += 1
                logger.critical(f"âš ï¸  [QUANTUM FORECAST] {agent_name}: No Q-values to adjust")
                continue

            # Extract state for this agent
            state_tensor = None

            if agent_name in states_dict:
                agent_states = states_dict[agent_name]

                if isinstance(agent_states, dict):
                    # Multi-timeframe: CRITICAL FIX - use agent's OWN timeframe first
                    # Extract timeframe from agent name (e.g., 'xl' from 'xl', '5m' from '5m')
                    agent_timeframe = agent_name  # Agent name IS the timeframe

                    # Build priority list: agent's own timeframe first, then fallbacks
                    timeframe_priority = [agent_timeframe]

                    # Add other timeframes as fallbacks (excluding agent's own)
                    all_timeframes = ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']
                    for tf in all_timeframes:
                        if tf != agent_timeframe and tf not in timeframe_priority:
                            timeframe_priority.append(tf)

                    # Try timeframes in priority order
                    for tf in timeframe_priority:
                        if tf in agent_states and agent_states[tf] is not None:
                            state_tensor = ensure_tensor(agent_states[tf], device=system.device)
                            if tf == agent_timeframe:
                                logger.critical(f"ğŸ“Š [QUANTUM FORECAST] {agent_name}: Using {tf} timeframe state (MATCHED)")
                            else:
                                logger.critical(f"ğŸ“Š [QUANTUM FORECAST] {agent_name}: Using {tf} timeframe state (FALLBACK)")
                            break
                else:
                    state_tensor = ensure_tensor(agent_states, device=system.device)

            if state_tensor is None:
                adjusted_q_values[agent_name] = agent_q
                tempering_stats['skipped'] += 1
                logger.critical(f"âš ï¸  [QUANTUM FORECAST] {agent_name}: No state available - using original Q-values")
                continue

            # Ensure proper shape
            if isinstance(state_tensor, torch.Tensor):
                if state_tensor.dim() == 1:
                    state_tensor = state_tensor.unsqueeze(0)
            else:
                # Convert numpy to tensor if needed
                state_tensor = torch.as_tensor(state_tensor, device=system.device)
                if state_tensor.dim() == 1:
                    state_tensor = state_tensor.unsqueeze(0)

            # ================================================================
            # CRITICAL FIX: Handle both tensor/numpy and dict Q-values
            # Use agent's OWN timeframe for Q-values too
            # ================================================================
            if isinstance(agent_q, dict):
                # Multi-timeframe Q-values: extract agent's OWN timeframe first
                agent_q_value = None
                primary_tf = None

                # Try agent's own timeframe first
                agent_timeframe = agent_name  # Agent name IS the timeframe

                if agent_timeframe in agent_q and agent_q[agent_timeframe] is not None:
                    primary_tf = agent_timeframe
                    agent_q_value = agent_q[agent_timeframe]
                else:
                    # Fallback to other timeframes in priority order
                    for tf in ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']:
                        if tf != agent_timeframe and tf in agent_q and agent_q[tf] is not None:
                            primary_tf = tf
                            agent_q_value = agent_q[tf]
                            break

                if agent_q_value is None:
                    # Last resort: use first available
                    primary_tf = next(iter(agent_q.keys()))
                    agent_q_value = agent_q[primary_tf]

                # Convert to tensor if numpy
                if isinstance(agent_q_value, np.ndarray):
                    agent_q_tensor = torch.as_tensor(agent_q_value, dtype=torch.float32, device=system.device)
                elif isinstance(agent_q_value, torch.Tensor):
                    agent_q_tensor = agent_q_value.to(system.device)
                else:
                    agent_q_tensor = torch.tensor(agent_q_value, dtype=torch.float32, device=system.device)

                # Log before adjustment (dict format)
                logger.critical(
                    f"ğŸ”„ [QUANTUM FORECAST] {agent_name}: "
                    f"Original Q-values ({primary_tf} TF): BUY={agent_q_tensor[0]:.6f} SELL={agent_q_tensor[1]:.6f}"
                )

                agent_q_for_processing = agent_q_tensor
                is_dict_format = True

            else:
                # Simple tensor or numpy array format
                primary_tf = None

                # Convert to tensor if numpy
                if isinstance(agent_q, np.ndarray):
                    agent_q_tensor = torch.as_tensor(agent_q, dtype=torch.float32, device=system.device)
                elif isinstance(agent_q, torch.Tensor):
                    agent_q_tensor = agent_q.to(system.device)
                else:
                    agent_q_tensor = torch.tensor(agent_q, dtype=torch.float32, device=system.device)

                agent_q_for_processing = agent_q_tensor
                is_dict_format = False

                # Log before adjustment (tensor format)
                logger.critical(
                    f"ğŸ”„ [QUANTUM FORECAST] {agent_name}: "
                    f"Original Q-values: BUY={agent_q_tensor[0]:.6f} SELL={agent_q_tensor[1]:.6f}"
                )

            # ================================================================
            # Get quantum forecast and adjust
            # ================================================================
            with torch.no_grad():
                # Ensure proper shape for quantum advisor (PyTorch tensor now)
                if agent_q_for_processing.dim() == 1:
                    q_for_adjustment = agent_q_for_processing.unsqueeze(0)
                else:
                    q_for_adjustment = agent_q_for_processing

                # Call quantum advisor to get adjusted Q-values
                adjusted, adj_meta = system.quantum_advisor.predict_q_adjustment(
                    state_tensor,
                    q_for_adjustment
                )

            # ================================================================
            # Store adjusted Q-values (maintain original format)
            # ================================================================
            adjusted_tensor = adjusted.squeeze(0) if adjusted.dim() > 1 else adjusted

            if is_dict_format:
                # Original was dict: return dict with adjusted primary timeframe
                adjusted_q_values[agent_name] = {}
                for tf_key in agent_q.keys():
                    if tf_key == primary_tf:
                        # Use quantum-adjusted values for primary timeframe
                        # Convert back to original type if it was numpy
                        if isinstance(agent_q[tf_key], np.ndarray):
                            adjusted_q_values[agent_name][tf_key] = adjusted_tensor.detach().cpu().numpy()
                        else:
                            adjusted_q_values[agent_name][tf_key] = adjusted_tensor
                    else:
                        # Keep original values for other timeframes
                        adjusted_q_values[agent_name][tf_key] = agent_q[tf_key]

                # Log after adjustment (dict format)
                logger.critical(
                    f"âœ… [QUANTUM FORECAST] {agent_name}: "
                    f"Adjusted Q-values ({primary_tf} TF): BUY={adjusted_tensor[0]:.6f} "
                    f"SELL={adjusted_tensor[1]:.6f} | "
                    f"Delta: BUY={(adjusted_tensor[0] - agent_q_for_processing[0]):.6f}, "
                    f"SELL={(adjusted_tensor[1] - agent_q_for_processing[1]):.6f}"
                )
            else:
                # Original was tensor or numpy: return in same format
                if isinstance(agent_q, np.ndarray):
                    adjusted_q_values[agent_name] = adjusted_tensor.detach().cpu().numpy()
                else:
                    adjusted_q_values[agent_name] = adjusted_tensor

                # Log after adjustment (tensor format)
                logger.critical(
                    f"âœ… [QUANTUM FORECAST] {agent_name}: "
                    f"Adjusted Q-values: BUY={adjusted_tensor[0]:.6f} "
                    f"SELL={adjusted_tensor[1]:.6f} | "
                    f"Delta: BUY={(adjusted_tensor[0] - agent_q_for_processing[0]):.6f}, "
                    f"SELL={(adjusted_tensor[1] - agent_q_for_processing[1]):.6f}"
                )

            # Store adjustment metadata
            metadata["quantum_forecast"]["adjustments"][agent_name] = adj_meta
            tempering_stats['successful'] += 1

        metadata["quantum_forecast"]["applied"] = True
        metadata["quantum_forecast"]["stats"] = tempering_stats

        logger.critical("="*80)
        logger.critical(
            f"ğŸ”® QUANTUM FORECAST COMPLETE | "
            f"Success: {tempering_stats['successful']}/{tempering_stats['total_agents']} | "
            f"Skipped: {tempering_stats['skipped']} | "
            f"Failed: {tempering_stats['failed']}"
        )
        logger.critical("="*80)

    except Exception as e:
        logger.critical(f"âŒ [QUANTUM FORECAST] ERROR: {e}")
        import traceback
        traceback.print_exc()
        metadata["quantum_forecast"]["error"] = str(e)
        adjusted_q_values = q_values
        tempering_stats['failed'] = len(q_values)

    return adjusted_q_values, metadata


def quantum_enhanced_predict(system, states_dict: Dict[str, Any]) -> Tuple[Dict[str, torch.Tensor], Dict]:
    """
    Enhanced predict with quantum advisor integration and CRITICAL logging.

    This is the main prediction path that uses quantum tempering.
    NOTE: V8.1 - This function is no longer used in the main prediction flow.
    Prediction now happens via predict_all_agents_with_metadata calling base predict + apply_quantum_forecast.
    """
    logger.critical("ğŸ¯ [PREDICTION] Starting quantum-enhanced prediction")

    # Get base Q-values from system
    if hasattr(system, '_original_predict'):
        q_values, base_metadata = system._original_predict(states_dict)
    else:
        q_values, base_metadata = system.predict(states_dict)

    logger.critical(f"ğŸ“Š [PREDICTION] Base Q-values computed for {len(q_values)} agents")

    # Apply quantum forecast with enhanced logging
    adjusted_q_values, quantum_metadata = apply_quantum_forecast(system, q_values, states_dict)

    # Merge metadata
    metadata = {**base_metadata, **quantum_metadata}
    metadata["method"] = "quantum_enhanced_predict"
    metadata["timestamp"] = time.time()

    logger.critical(
        f"âœ… [PREDICTION] Quantum-enhanced prediction complete | "
        f"Agents: {len(adjusted_q_values)} | "
        f"Quantum applied: {quantum_metadata['quantum_forecast']['applied']}"
    )

    return adjusted_q_values, metadata


def train_quantum_advisor(system, states: torch.Tensor, actions: torch.Tensor,
                          rewards: torch.Tensor, next_states: torch.Tensor,
                          dones: torch.Tensor) -> float:



    if not hasattr(system, 'quantum_advisor') or system.quantum_advisor is None:
        return 0.0

    advisor = system.quantum_advisor
    advisor.train()

    try:
        # Generate forecasts for current states
        current_forecast, current_confidence = advisor(states)

        # Generate forecasts for next states
        with torch.no_grad():
            next_forecast, next_confidence = advisor(next_states)

        # Compute target forecast (should predict future rewards)
        target_forecast = rewards.unsqueeze(1).expand_as(current_forecast)

        # Forecast loss (MSE between forecast and actual rewards)
        forecast_loss = F.mse_loss(current_forecast, target_forecast)

        # Confidence regularization (encourage confident predictions)
        confidence_reg = -0.01 * current_confidence.mean()

        total_loss = forecast_loss + confidence_reg

        # Optimize
        advisor._optimizer.zero_grad()
        total_loss.backward()
        clip_grad_norm_(advisor.parameters(), GRADIENT_CLIP)
        advisor._optimizer.step()

        return total_loss.item()

    except Exception as e:
        logger.error(f"Quantum advisor training error: {e}")
        return 0.0


def train_agent_batch(agent, batch: Tuple, gamma: float = GAMMA) -> Dict[str, float]:


    try:
        # Q-learning update
        agent.model.train()
        current_q = agent.model(states).gather(1, actions.unsqueeze(1)).squeeze()

        with torch.no_grad():
            next_q = agent.target_model(next_states).max(1)[0]
            target_q = rewards + gamma * next_q * (1 - dones)

        q_loss = F.mse_loss(current_q, target_q)

        agent.optimizer.zero_grad()
        q_loss.backward()
        clip_grad_norm_(agent.model.parameters(), GRADIENT_CLIP)
        agent.optimizer.step()

        losses["q"] = q_loss.item()

        # Actor update (if actor exists)
        if hasattr(agent, 'actor') and hasattr(agent, 'actor_optimizer'):
            agent.actor.train()
            action_probs = agent.actor(states)
            log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-8)

            with torch.no_grad():
                advantages = target_q - current_q

            actor_loss = -(log_probs * advantages).mean()

            agent.actor_optimizer.zero_grad()
            actor_loss.backward()
            clip_grad_norm_(agent.actor.parameters(), GRADIENT_CLIP)
            agent.actor_optimizer.step()

            losses["actor"] = actor_loss.item()

        # Critic update (if critic exists)
        if hasattr(agent, 'critic') and hasattr(agent, 'critic_optimizer'):
            agent.critic.train()
            value = agent.critic(states).squeeze()

            with torch.no_grad():
                target_value = rewards + gamma * agent.critic(next_states).squeeze() * (1 - dones)

            critic_loss = F.mse_loss(value, target_value)

            agent.critic_optimizer.zero_grad()
            critic_loss.backward()
            clip_grad_norm_(agent.critic.parameters(), GRADIENT_CLIP)
            agent.critic_optimizer.step()

            losses["critic"] = critic_loss.item()

    except Exception as e:
        logger.error(f"Agent training error: {e}")

    return losses

# ============================================================================
# EXPERIENCE MANAGER
# ============================================================================

class ExperienceManager:


    def __init__(self, buffer_size: int = BUFFER_SIZE):
        self.buffer = PrioritizedReplayBuffer(capacity=buffer_size)
        self.temp_experiences = []

        logger.info(f"âœ… ExperienceManager initialized (buffer_size={buffer_size})")

    def add_experience(self, state, action, reward, next_state, done, info=None):

        state_tensor = ensure_tensor(state)
        next_state_tensor = ensure_tensor(next_state)

        exp = Experience(
            state=state_tensor,
            action=int(action),
            reward=float(reward),
            next_state=next_state_tensor,
            done=bool(done),
            info=info or {}
        )

        self.buffer.push(exp)

    def sample_batch(self, batch_size: int = BATCH_SIZE) -> Tuple:

        if len(self.buffer) < batch_size:
            return (None,) * 7

        experiences, weights, indices = self.buffer.sample(batch_size)

        if not experiences:
            return (None,) * 7

        # Collate batch
        states = torch.stack([exp.state for exp in experiences])
        actions = torch.tensor([exp.action for exp in experiences], dtype=torch.long)
        rewards = torch.tensor([exp.reward for exp in experiences], dtype=torch.float32)
        next_states = torch.stack([exp.next_state for exp in experiences])
        dones = torch.tensor([exp.done for exp in experiences], dtype=torch.float32)

        return states, actions, rewards, next_states, dones, weights, indices

    def update_priorities(self, indices: torch.Tensor, td_errors: torch.Tensor):

        priorities = torch.abs(td_errors) + 1e-6
        self.buffer.update_priorities(indices, priorities)

    def __len__(self):
        return len(self.buffer)

logger.info("âœ… Experience manager and training functions initialized")


# ============================================================================
# QUANTUM TRADING SYSTEM - MAIN CLASS
# ============================================================================

class QuantumTradingSystem:


    def __init__(self, config: Dict[str, Any] = None):

        config = config or {}

        # Configuration
        self.state_dim = config.get('state_dim', DEFAULT_STATE_DIM)
        self.action_dim = config.get('action_dim', DEFAULT_ACTION_DIM)
        self.hidden_dim = config.get('hidden_dim', DEFAULT_HIDDEN_DIM)
        self.latent_dim = config.get('latent_dim', DEFAULT_LATENT_DIM)
        self.n_agents = config.get('n_agents', 3)
        self.agent_names = config.get('agent_names',
                                      [f"agent_{i}" for i in range(self.n_agents)])
        self.timeframes = config.get('timeframes', TIMEFRAMES)
        self.device = torch.device(config.get('device',
                                              'cuda' if torch.cuda.is_available() else 'cpu'))

        logger.info("="*80)
        logger.info(f"Initializing QuantumTradingSystem")
        logger.info(f"  State Dim: {self.state_dim}")
        logger.info(f"  Action Dim: {self.action_dim}")
        logger.info(f"  Agents: {self.n_agents}")
        logger.info(f"  Device: {self.device}")
        logger.info("="*80)

        # Initialize components
        self.agents = self._initialize_agents()
        self.experience_manager = ExperienceManager(BUFFER_SIZE)

        # Latent encoder (will be wrapped with quantum during integration)
        self.latent_encoder = nn.Sequential(
            nn.Linear(self.state_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.latent_dim)
        ).to(self.device)

        # Quantum components (initialized during integration)
        self.quantum_advisor = None
        self._original_predict = None

        # Training statistics
        self.training_step = 0
        self.episode_count = 0
        self.best_reward = float('-inf')
        self.losses_history = []
        self._running = False

        logger.info(f"âœ… QuantumTradingSystem initialized with {self.n_agents} agents")

    def _initialize_agents(self) -> Dict[str, QuantumAgent]:

        agents = {}

        logger.info(f"Initializing {self.n_agents} quantum agents...")

        for name in self.agent_names:
            agent = QuantumAgent(
                name=name,
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim,
                n_qubits=QUANTUM_N_QUBITS,
                learning_rate=LEARNING_RATE,
                device=self.device
            ).to(self.device)
            agents[name] = agent

            # Verify agent has required attributes
            if hasattr(agent, 'actor') and hasattr(agent, 'critic'):
                logger.info(f"  âœ… {name}: Actor-Critic verified")
            else:
                logger.warning(f"  âš ï¸  {name}: Missing Actor or Critic!")

        return agents

    def predict(self, states_dict: Dict[str, Any]) -> Tuple[Dict[str, torch.Tensor], Dict]:


        q_values = {}

        for agent_name, agent in self.agents.items():
            if agent_name in states_dict:
                agent_states = states_dict[agent_name]

                # Extract state
                state = None
                if isinstance(agent_states, dict):
                    for tf in self.timeframes:
                        if tf in agent_states and agent_states[tf] is not None:
                            state = agent_states[tf]
                            break
                else:
                    state = agent_states

                if state is not None:
                    state_tensor = ensure_tensor(state, device=self.device)
                    if state_tensor.dim() == 1:
                        state_tensor = state_tensor.unsqueeze(0)

                    with torch.no_grad():
                        q_vals = agent.forward(state_tensor)

                    q_values[agent_name] = q_vals.squeeze(0)
                else:
                    q_values[agent_name] = torch.zeros(self.action_dim, device=self.device)
            else:
                q_values[agent_name] = torch.zeros(self.action_dim, device=self.device)

        metadata = {
            "method": "original_predict",
            "timestamp": time.time()
        }

        return q_values, metadata

    def quantum_enhanced_predict(
        self,
        state: torch.Tensor,
        agent_idx: int = 0
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        Predict action using base Q-values with optional quantum enhancement.

        Args:
            state: Input state tensor, can be 1D or batched
            agent_idx: Index of the agent to use

        Returns:
            action: Selected action tensor
            info: Dictionary with debug info including Q-values and forecasts
        """
        # Ensure state is batched
        if state.dim() == 1:
            state = state.unsqueeze(0)

        agent = self.agents[agent_idx]
        info = {}

        # Get base Q-values from agent
        with torch.no_grad():
            if hasattr(agent, 'actor'):
                base_q_values = agent.actor(state)
            else:
                base_q_values = agent.model(state)

        info['base_q_values'] = base_q_values.cpu().numpy()

        # Get quantum forecast if available
        if hasattr(self, 'quantum_advisor') and self.quantum_advisor is not None:
            with torch.no_grad():
                forecast, confidence = self.quantum_advisor(state)

            info['quantum_forecast'] = forecast.cpu().numpy()
            info['forecast_confidence'] = confidence.cpu().numpy()

            # Apply quantum adjustment
            adjusted_q_values = apply_quantum_forecast(
                base_q_values,
                forecast,
                confidence,
                alpha=0.3  # 30% quantum influence
            )
            info['adjusted_q_values'] = adjusted_q_values.cpu().numpy()
        else:
            # No quantum advisor, use base Q-values
            adjusted_q_values = base_q_values
            info['quantum_forecast'] = None
            info['forecast_confidence'] = None
            info['adjusted_q_values'] = base_q_values.cpu().numpy()

        # Select action (greedy or exploration)
        if np.random.random() < 0.1:  # 10% exploration
            action = torch.randint(0, adjusted_q_values.shape[-1], (1,))
            info['exploration'] = True
        else:
            action = adjusted_q_values.argmax(dim=-1)
            info['exploration'] = False

        info['selected_action'] = action.item()

        return action, info

    def verify_system_components(self):
        """
        Comprehensive system component verification (V8.5.2 FIX #2).
        Returns dict with component status.
        """
        checks = {}

        # Check 1: Quantum Advisor
        checks['quantum_advisor'] = (
            hasattr(self, 'quantum_advisor') and
            self.quantum_advisor is not None
        )

        # Check 2: Reward Processor
        checks['reward_processor'] = (
            hasattr(self, 'reward_processor') and
            self.reward_processor is not None and
            hasattr(self.reward_processor, 'start') and
            self.reward_processor.is_running
        )

        # Check 3: Quantum Trainer (V8.5+)
        if hasattr(self, 'quantum_bridge'):
            checks['quantum_trainer'] = (
                hasattr(self.quantum_bridge, 'quantum_trainer') and
                self.quantum_bridge.quantum_trainer is not None
            )
        else:
            checks['quantum_trainer'] = False

        # Check 4: Buffer
        checks['hybrid_buffer'] = (
            hasattr(self, 'hybrid_buffer') and
            self.hybrid_buffer is not None and
            hasattr(self.hybrid_buffer, 'maxlen')
        )

        # Check 5: Quantum System
        if hasattr(self, 'quantum_bridge'):
            checks['quantum_system'] = (
                hasattr(self.quantum_bridge, 'quantum_system') and
                self.quantum_bridge.quantum_system is not None
            )
        else:
            checks['quantum_system'] = False

        # Check 6: Voting System (V8.5+)
        if hasattr(self, 'quantum_system'):
            checks['voting_system'] = (
                hasattr(self.quantum_system, 'quantum_voting') and
                self.quantum_system.quantum_voting is not None
            )
        else:
            checks['voting_system'] = False

        # Count passed/failed
        passed = sum(checks.values())
        total = len(checks)

        # Detailed reporting
        print(f"\n{'='*80}")
        print(f"SYSTEM COMPONENT VERIFICATION - V8.5.2")
        print(f"{'='*80}")

        for component, status in checks.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"{status_icon} {component}: {'INITIALIZED' if status else 'MISSING'}")

        print(f"{'='*80}")
        print(f"RESULT: {passed}/{total} checks passed")

        if passed == total:
            print("âœ… ALL COMPONENTS OPERATIONAL!")
        else:
            print(f"âš ï¸  {total - passed} component(s) missing or failed")
            print("System will work with reduced functionality")
            print("\nMissing components:")
            for component, status in checks.items():
                if not status:
                    print(f"  âŒ {component}")

        print(f"{'='*80}\n")

        logger.critical(f"[V8.5.2] Component Verification: {passed}/{total} passed")

        return checks, passed, total

    def train_step(self, batch_size: int = BATCH_SIZE) -> Dict[str, float]:

        # Sample batch
        batch = self.experience_manager.sample_batch(batch_size)

        if batch[0] is None:
            return {"actor": 0.0, "critic": 0.0, "q": 0.0, "total": 0.0}

        total_losses = {"actor": [], "critic": [], "q": []}

        # Count agents updating
        actors_updating = 0
        critics_updating = 0

        # Train each agent
        for agent_name, agent in self.agents.items():
            losses = train_agent_batch(agent, batch, GAMMA)

            for key, value in losses.items():
                if value > 0:
                    total_losses[key].append(value)
                    if key == "actor":
                        actors_updating += 1
                    elif key == "critic":
                        critics_updating += 1

            # Update target network
            if self.training_step % UPDATE_FREQUENCY == 0:
                agent.update_target(TAU)

            # Update epsilon
            agent.update_epsilon(EPSILON_DECAY)

        # Train quantum advisor if available
        if self.quantum_advisor is not None:
            states, actions, rewards, next_states, dones, _, _ = batch
            quantum_loss = train_quantum_advisor(
                self, states, actions, rewards, next_states, dones
            )
            total_losses["quantum"] = [quantum_loss]

        # Update priorities
        states, actions, rewards, next_states, dones, weights, indices = batch

        with torch.no_grad():
            # Compute TD errors for priority updates
            q_values_list = []
            next_q_values_list = []

            for agent in self.agents.values():
                q_values_list.append(agent.model(states))
                next_q_values_list.append(agent.target_model(next_states))

            q_values = torch.stack(q_values_list).mean(dim=0)
            next_q_values = torch.stack(next_q_values_list).mean(dim=0)

            current_q = q_values.gather(1, actions.unsqueeze(1)).squeeze()
            next_v = next_q_values.max(dim=1)[0]
            target_q = rewards + GAMMA * next_v * (1 - dones)

            td_errors = target_q - current_q

        self.experience_manager.update_priorities(indices, td_errors)

        # Aggregate losses
        avg_losses = {}
        for key, values in total_losses.items():
            avg_losses[key] = np.mean(values) if values else 0.0
        avg_losses["total"] = sum(avg_losses.values())

        # Log training progress periodically
        if self.training_step % 100 == 0:
            logger.info(f"[TRAIN] Step {self.training_step}: "
                       f"Actor={avg_losses['actor']:.4f}, "
                       f"Critic={avg_losses['critic']:.4f}, "
                       f"Q={avg_losses['q']:.4f} | "
                       f"Updates: {actors_updating} actors, {critics_updating} critics")

        self.training_step += 1
        self.losses_history.append(avg_losses)

        return avg_losses

    def save_checkpoint(self, path: str):

        checkpoint = {
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'hidden_dim': self.hidden_dim,
                'latent_dim': self.latent_dim,
                'n_agents': self.n_agents,
                'agent_names': self.agent_names,
                'timeframes': self.timeframes
            },
            'agents': {name: agent.state_dict() for name, agent in self.agents.items()},
            'latent_encoder': self.latent_encoder.state_dict(),
            'quantum_advisor': self.quantum_advisor.state_dict() if self.quantum_advisor else None,
            'training_step': self.training_step,
            'episode_count': self.episode_count,
            'best_reward': self.best_reward
        }

        torch.save(checkpoint, path)
        logger.info(f"âœ… Checkpoint saved to {path}")

    def load_checkpoint(self, path: str):

        checkpoint = torch.load(path, map_location=self.device)

        # Load agent states
        for name, state_dict in checkpoint['agents'].items():
            if name in self.agents:
                self.agents[name].load_state_dict(state_dict)

        # Load encoder
        self.latent_encoder.load_state_dict(checkpoint['latent_encoder'])

        # Load quantum advisor if available
        if checkpoint.get('quantum_advisor') and self.quantum_advisor:
            self.quantum_advisor.load_state_dict(checkpoint['quantum_advisor'])

        # Load training state
        self.training_step = checkpoint.get('training_step', 0)
        self.episode_count = checkpoint.get('episode_count', 0)
        self.best_reward = checkpoint.get('best_reward', float('-inf'))

        logger.info(f"âœ… Checkpoint loaded from {path}")

logger.info("âœ… QuantumTradingSystem class initialized")




# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import torch.nn as nn
# REMOVED DUPLICATE: import torch.nn.functional as F
# REMOVED DUPLICATE: import numpy as np
from typing import Tuple, Optional

# ============================================================================
# COMPONENT 1: QUANTUM STATE ENCODING WRAPPER
# ============================================================================
# INSERT AFTER: Line ~1500 (after imports, before agent classes)
# PURPOSE: Augments classical state encoders with quantum circuit features
# ============================================================================

class QuantumStateEncodingWrapperModule(nn.Module):
    """
    Wraps a classical state encoder with quantum circuit simulation.

    This module takes your existing encoder and augments it with quantum-derived
    features, giving your agents access to quantum-enhanced state representations.

    Features:
    - Preserves original encoder functionality
    - Adds quantum circuit simulation (if Qiskit available)
    - Falls back to classical encoding gracefully
    - Minimal performance overhead
    """

    def __init__(self, classical_encoder: nn.Module, n_qubits: int = 6):
        """
        Args:
            classical_encoder: Your existing state encoder network
            n_qubits: Number of qubits for quantum circuit (default: 8)
        """
        super().__init__()
        self.classical_encoder = classical_encoder
        self.n_qubits = n_qubits
        self.quantum_available = QISKIT_AVAILABLE

        # Quantum circuit cache for performance
        self._circuit_cache = {}

        # Get output dimension from classical encoder
        test_input = torch.randn(1, self._infer_input_dim(classical_encoder))
        classical_output = classical_encoder(test_input)
        self.classical_output_dim = classical_output.shape[-1]

        # Quantum feature dimension (2^n_qubits amplitudes)
        self.quantum_dim = min(2 ** n_qubits, 256)  # Cap at 256 for performance

        # Projection layer to combine classical + quantum features
        self.fusion_layer = nn.Linear(
            self.classical_output_dim + self.quantum_dim,
            self.classical_output_dim
        )

        print(f"âœ… QuantumStateEncodingWrapper initialized:")
        print(f"   - Classical output dim: {self.classical_output_dim}")
        print(f"   - Quantum feature dim: {self.quantum_dim}")
        print(f"   - Qubits: {n_qubits}")
        print(f"   - Quantum circuits: {'ACTIVE' if self.quantum_available else 'DISABLED (using classical)'}")

    def _infer_input_dim(self, encoder: nn.Module) -> int:
        """Infer input dimension from encoder."""
        for module in encoder.modules():
            if isinstance(module, nn.Linear):
                return module.in_features
        return 128  # Default fallback

    def _create_quantum_circuit(self, state: torch.Tensor) -> Optional['QuantumCircuit']:
        """Create a quantum circuit encoding the state."""
        if not self.quantum_available:
            return None

        try:
            from qiskit import QuantumCircuit

            # Normalize state to [0, 2Ï€] for rotation angles
            state_np = state.detach().cpu().numpy().flatten()
            # Robust angle normalization with stability checks
            state_flat = state_np.flatten()
            state_range = state_np.max() - state_np.min()

            # Prevent division-by-near-zero explosion
            MIN_VARIANCE = 0.01
            if state_range < MIN_VARIANCE:
                # Low variance - use uniform distribution with perturbation
                angles = np.linspace(0, 2 * np.pi, len(state_flat))
                angles += (state_flat - state_flat.mean()) * 0.1
            else:
                # Normal normalization with robust epsilon
                angles = (state_flat - state_flat.min()) / (state_range + MIN_VARIANCE) * 2 * np.pi

            # Clip to valid range
            angles = np.clip(angles, 0, 2 * np.pi)

            # Create circuit
            qc = QuantumCircuit(self.n_qubits)

            # Apply rotation gates based on state values
            for i in range(min(len(angles), self.n_qubits)):
                qc.ry(angles[i], i)

            # Add entanglement
            for i in range(self.n_qubits - 1):
                qc.cx(i, i + 1)

            # More rotations for complexity
            for i in range(min(len(angles), self.n_qubits)):
                idx = (i + self.n_qubits // 2) % len(angles)
                qc.rz(angles[idx], i)

            return qc

        except Exception as e:
            print(f"âš ï¸  Quantum circuit creation failed: {e}")
            return None

    def _extract_quantum_features(self, state: torch.Tensor) -> torch.Tensor:
        """Extract quantum features from state via circuit simulation."""
        batch_size = state.shape[0]
        device = state.device

        if not self.quantum_available:
            # Classical fallback: use FFT-based features
            features = torch.fft.fft(state, dim=-1).real
            features = features[:, :self.quantum_dim]
            return features

        try:
            # Use new Qiskit API (transpile + run) instead of deprecated execute()
            from qiskit import transpile

            quantum_features_list = []

            for i in range(batch_size):
                # Create quantum circuit for this sample
                qc = self._create_quantum_circuit(state[i])

                if qc is None:
                    # Fallback for this sample
                    features = torch.fft.fft(state[i], dim=-1).real[:self.quantum_dim]
                    quantum_features_list.append(features)
                    continue

                # Execute circuit using new API
                qc.save_statevector()  # Save the statevector
                transpiled_qc = transpile(qc, _SV_AER)
                job = _SV_AER.run(transpiled_qc, shots=1)
                result = job.result()
                statevector = result.get_statevector(transpiled_qc)

                # Use statevector amplitudes as quantum features
                amplitudes = np.abs(statevector) ** 2
                amplitudes = amplitudes[:self.quantum_dim]  # Truncate if needed

                # Pad if needed
                if len(amplitudes) < self.quantum_dim:
                    amplitudes = np.pad(amplitudes, (0, self.quantum_dim - len(amplitudes)))

                features = torch.tensor(amplitudes, dtype=torch.float32, device=device)
                quantum_features_list.append(features)

            quantum_features = torch.stack(quantum_features_list).to(device)
            return quantum_features

        except Exception as e:
            print(f"âš ï¸  Quantum feature extraction failed: {e}")
            # Full fallback
            features = torch.fft.fft(state, dim=-1).real
            features = features[:, :self.quantum_dim]
            return features

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with quantum-enhanced encoding.

        Args:
            state: Input state tensor [batch_size, state_dim]

        Returns:
            Quantum-enhanced encoded state [batch_size, classical_output_dim]
        """
        # Classical encoding
        classical_features = self.classical_encoder(state)

        # Quantum features (or classical fallback)
        quantum_features = self._extract_quantum_features(state)

        # Combine features
        combined = torch.cat([classical_features, quantum_features], dim=-1)

        # Fuse via learned projection
        output = self.fusion_layer(combined)

        return output


# ============================================================================
# COMPONENT 2: QUANTUM FORECASTING ADVISOR
# ============================================================================
# INSERT AFTER: QuantumStateEncodingWrapperModule class
# PURPOSE: Provides quantum-based price forecasting and Q-value adjustments
# ============================================================================

class QuantumFeatureNormalizer(nn.Module):
    """
    Normalizes quantum features to prevent explosion in downstream networks.
    Added in V8.3 to fix forecast instability.
    """
    def __init__(self, feature_dim: int, max_norm: float = 5.0):
        super().__init__()
        self.feature_dim = feature_dim
        self.max_norm = max_norm

        # Learnable scale and bias for adaptive normalization
        self.scale = nn.Parameter(torch.ones(feature_dim))
        self.bias = nn.Parameter(torch.zeros(feature_dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Normalize quantum features with multiple safeguards."""
        # Layer normalization
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True) + 1e-6
        x_norm = (x - mean) / std

        # Clip to prevent extreme values
        x_clipped = torch.clamp(x_norm, -self.max_norm, self.max_norm)

        # Learnable affine transformation
        x_transformed = x_clipped * self.scale + self.bias

        return x_transformed


class QuantumForecastingAdvisor(nn.Module):
    """
    Quantum-based forecasting advisor for Q-value tempering.

    This module uses quantum circuits to forecast future price movements
    and provides adjustment factors for Q-values, making your agents
    more forward-looking.

    Features:
    - Multi-step ahead forecasting (default 5 steps)
    - Confidence estimation for predictions
    - Quantum circuit simulation (or classical fallback)
    - Learned fusion with classical predictions
    """

    def __init__(
        self,
        state_dim: int,
        forecast_horizon: int = 30,
        n_qubits: int = 6,  # V2.0 FIX: Changed from 60 to 6 (2^6=64 dims instead of 2^60)
        hidden_dim: int = 128
    ):
        """
        Args:
            state_dim: Dimension of input state
            forecast_horizon: Number of steps to forecast ahead
            n_qubits: Number of qubits for quantum circuits
            hidden_dim: Hidden layer dimension
        """
        super().__init__()
        self.state_dim = state_dim
        self.forecast_horizon = forecast_horizon
        self.n_qubits = n_qubits
        self.quantum_available = QISKIT_AVAILABLE

        # Classical preprocessing
        self.state_processor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Quantum feature dimension
        self.quantum_dim = 2 ** n_qubits

        # Fusion network (combines classical + quantum features)
        self.fusion_network = nn.Sequential(
            nn.Linear(hidden_dim + self.quantum_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # V8.3 FIX: Add quantum feature normalizer for stability
        self.quantum_feature_normalizer = nn.LayerNorm(self.quantum_dim)

        # V8.5.3 FIX: Add input state normalizer for BOTH quantum and classical paths
        self.input_state_normalizer = nn.LayerNorm(state_dim)

        # V8.5.3 FIX: Maximum forecast magnitude to prevent explosions
        self.max_forecast_magnitude = 12.0

        # Forecast heads - V8.5.3 FIX: Bounded forecast with Tanh activation
        self.forecast_head = nn.Sequential(
            nn.Linear(hidden_dim, forecast_horizon),
            nn.Tanh()  # Bounds raw output to [-1, +1]
        )
        self.confidence_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, forecast_horizon),
            nn.Sigmoid()  # Confidence in [0, 1]
        )

        print(f"âœ… QuantumForecastingAdvisor initialized:")
        print(f"   - State dim: {state_dim}")
        print(f"   - Forecast horizon: {forecast_horizon} steps")
        print(f"   - Qubits: {n_qubits}")
        print(f"   - Quantum forecasting: {'ACTIVE' if self.quantum_available else 'DISABLED (using classical)'}")
        print(f"   - Quantum feature normalizer: ACTIVE")
        print(f"   - Input state normalizer: ACTIVE (V8.5.3)")
        print(f"   - Forecast bounds: Â±{self.max_forecast_magnitude}")


    def _normalize_state_for_quantum(self, state: torch.Tensor) -> torch.Tensor:
        """
        Normalize state for quantum circuit processing (V8.5.2 FIX).

        Quantum circuits require states with magnitude ~1.0 for numerical stability.
        This function scales states to appropriate range while preserving information.

        Args:
            state: Input state tensor

        Returns:
            Normalized and scaled state tensor ready for quantum processing
        """
        # Get state magnitude
        magnitude = torch.norm(state)

        # If magnitude too small, add small epsilon
        if magnitude < 1e-8:
            magnitude = torch.tensor(1e-8, device=state.device, dtype=state.dtype)

        # Normalize to unit magnitude
        normalized_state = state / magnitude

        # Scale to reasonable range for quantum encoding (0.1 to 1.0)
        # This prevents numerical issues while preserving relative relationships
        scaled_state = normalized_state * 0.5  # Scale to ~0.5 magnitude

        return scaled_state
    def _create_forecasting_circuit(self, state: torch.Tensor) -> Optional['QuantumCircuit']:
        """Create quantum circuit for price forecasting."""
        if not self.quantum_available:
            return None

        try:
            from qiskit import QuantumCircuit

            # Normalize state for circuit angles
            state_np = state.detach().cpu().numpy().flatten()
            # Robust angle normalization with stability checks
            state_flat = state_np.flatten()
            state_range = state_np.max() - state_np.min()

            # Prevent division-by-near-zero explosion
            MIN_VARIANCE = 0.01
            if state_range < MIN_VARIANCE:
                # Low variance - use uniform distribution with perturbation
                angles = np.linspace(0, 2 * np.pi, len(state_flat))
                angles += (state_flat - state_flat.mean()) * 0.1
            else:
                # Normal normalization with robust epsilon
                angles = (state_flat - state_flat.min()) / (state_range + MIN_VARIANCE) * 2 * np.pi

            # Clip to valid range
            angles = np.clip(angles, 0, 2 * np.pi)

            qc = QuantumCircuit(self.n_qubits)

            # Encode state information
            for i in range(min(len(angles), self.n_qubits)):
                qc.rx(angles[i], i)
                qc.ry(angles[(i + 1) % len(angles)], i)

            # Create entanglement pattern
            for i in range(self.n_qubits - 1):
                qc.cx(i, i + 1)

            # Apply phase gates for temporal evolution
            for i in range(self.n_qubits):
                qc.rz(angles[i % len(angles)] * 0.5, i)

            # More entanglement
            for i in range(self.n_qubits - 1, 0, -1):
                qc.cx(i, i - 1)

            return qc

        except Exception as e:
            print(f"âš ï¸  Quantum forecasting circuit creation failed: {e}")
            return None

    def _extract_quantum_forecast_features(self, state: torch.Tensor) -> torch.Tensor:
        """Extract quantum features for forecasting."""
        batch_size = state.shape[0]
        device = state.device

        # FIX 3: Input validation to prevent instability
        max_val = state.abs().max().item()
        if torch.isnan(state).any() or torch.isinf(state).any():
            logger.critical("âŒ [QUANTUM] State contains NaN/Inf - using fallback")
        elif max_val > 100.0:
            logger.critical(f"âŒ [QUANTUM] State magnitude too large: {max_val:.4f} - using fallback")
        elif max_val > 10.0:
            logger.warning(f"âš ï¸  [QUANTUM] Unusually large state values: {max_val:.4f}")

        if state.numel() > 1:
            variance = state.var().item()
            if variance < 1e-8:
                logger.warning(f"âš ï¸  [QUANTUM] Very low variance: {variance:.2e}")

        if not self.quantum_available:
            # Classical fallback: wavelet-like features
            features = torch.zeros(batch_size, self.quantum_dim, device=device)
            for i in range(batch_size):
                # Use FFT and phase information
                fft_features = torch.fft.fft(state[i], dim=-1)
                amplitudes = torch.abs(fft_features).flatten()
                phases = torch.angle(fft_features).flatten()
                # Concatenate and pad/truncate to quantum_dim
                combined = torch.cat([amplitudes, phases])
                if combined.shape[0] > self.quantum_dim:
                    combined = combined[:self.quantum_dim]
                elif combined.shape[0] < self.quantum_dim:
                    padding = torch.zeros(self.quantum_dim - combined.shape[0], device=device)
                    combined = torch.cat([combined, padding])
                features[i] = combined
            return features

        try:
            # Use new Qiskit API (transpile + run) instead of deprecated execute()
            from qiskit import transpile

            quantum_features_list = []

            for i in range(batch_size):
                # V8.5.2 FIX: Normalize state BEFORE quantum processing
                normalized_state = self._normalize_state_for_quantum(state[i])
                qc = self._create_forecasting_circuit(normalized_state)

                if qc is None:
                    # Fallback for this sample
                    fft_features = torch.fft.fft(state[i], dim=-1)
                    amplitudes = torch.abs(fft_features).flatten()
                    phases = torch.angle(fft_features).flatten()
                    combined = torch.cat([amplitudes, phases])
                    if combined.shape[0] > self.quantum_dim:
                        combined = combined[:self.quantum_dim]
                    elif combined.shape[0] < self.quantum_dim:
                        padding = torch.zeros(self.quantum_dim - combined.shape[0], device=device)
                        combined = torch.cat([combined, padding])
                    quantum_features_list.append(combined)
                    continue

                # Execute quantum circuit using new API
                qc.save_statevector()  # Save the statevector
                transpiled_qc = transpile(qc, _SV_AER)
                job = _SV_AER.run(transpiled_qc, shots=1)
                result = job.result()
                statevector = result.get_statevector(transpiled_qc)

                # Bounded quantum feature extraction with stability checks
                amplitudes = np.abs(statevector) ** 2
                phases = np.angle(statevector)

                # Apply robust clipping to prevent explosion
                MAX_AMPLITUDE = 10.0
                amplitudes = np.clip(amplitudes, 0, MAX_AMPLITUDE)
                phases = np.clip(phases, -np.pi, np.pi)

                # Normalize amplitudes to prevent accumulation
                if np.max(amplitudes) > 0:
                    amplitudes = amplitudes / (np.max(amplitudes) + 1e-6)

                # Combine amplitude and phase information
                combined_np = np.concatenate([amplitudes, phases])

                # Pad or truncate to quantum_dim
                if len(combined_np) > self.quantum_dim:
                    combined_np = combined_np[:self.quantum_dim]
                elif len(combined_np) < self.quantum_dim:
                    combined_np = np.pad(combined_np, (0, self.quantum_dim - len(combined_np)))

                # Final safety clip before tensor conversion
                combined_np = np.clip(combined_np, -MAX_AMPLITUDE, MAX_AMPLITUDE)

                features = torch.tensor(combined_np, dtype=torch.float32, device=device)
                quantum_features_list.append(features)

            quantum_features = torch.stack(quantum_features_list).to(device)
            return quantum_features

        except Exception as e:
            print(f"âš ï¸  Quantum forecast feature extraction failed: {e}")
            import traceback
            traceback.print_exc()
            # Full fallback
            features = torch.zeros(batch_size, self.quantum_dim, device=device)
            for i in range(batch_size):
                fft_features = torch.fft.fft(state[i], dim=-1)
                amplitudes = torch.abs(fft_features).flatten()
                phases = torch.angle(fft_features).flatten()
                combined = torch.cat([amplitudes, phases])
                if combined.shape[0] > self.quantum_dim:
                    combined = combined[:self.quantum_dim]
                elif combined.shape[0] < self.quantum_dim:
                    padding = torch.zeros(self.quantum_dim - combined.shape[0], device=device)
                    combined = torch.cat([combined, padding])
                features[i] = combined
            return features

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Generate quantum-enhanced price forecasts.

        V8.5.3 FIX: Normalizes state at entry for BOTH classical and quantum paths.

        Args:
            state: Input state [batch_size, state_dim]

        Returns:
            forecast: Bounded price predictions [batch_size, forecast_horizon]
            confidence: Confidence for each prediction [batch_size, forecast_horizon]
        """
        # V8.5.3 FIX: Normalize state ONCE at entry
        # Get statistics before normalization
        pre_norm_magnitude = torch.norm(state, dim=-1).mean().item()
        pre_norm_max = state.abs().max().item()

        # Check for pathological states
        if torch.isnan(state).any() or torch.isinf(state).any():
            logger.critical(f"âŒ [QUANTUM] State contains NaN/Inf - replacing with zeros")
            state = torch.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)

        # Log if state is very large (indicates upstream issue)
        if pre_norm_magnitude > 1000.0:
            logger.critical(
                f"âŒ [QUANTUM] Extreme state magnitude: {pre_norm_magnitude:.2f} "
                f"(max element: {pre_norm_max:.2f}) - normalizing"
            )
        elif pre_norm_magnitude > 10.0:
            logger.warning(
                f"âš ï¸  [QUANTUM] Large state magnitude: {pre_norm_magnitude:.2f} - normalizing"
            )

        # Apply LayerNorm (per-feature normalization)
        state_normalized = self.input_state_normalizer(state)

        # Log normalization effect if significant
        post_norm_magnitude = torch.norm(state_normalized, dim=-1).mean().item()
        if pre_norm_magnitude > 10.0:
            logger.info(
                f"ğŸ”§ [QUANTUM] State normalized: {pre_norm_magnitude:.2f} â†’ "
                f"{post_norm_magnitude:.2f} "
                f"(factor: {pre_norm_magnitude/post_norm_magnitude:.2f}x)"
            )

        # Classical processing (NOW receives normalized state)
        classical_features = self.state_processor(state_normalized)

        # Quantum features (also receives normalized state)
        quantum_features = self._extract_quantum_forecast_features(state_normalized)
        quantum_features = self.quantum_feature_normalizer(quantum_features)  # V8.3 stability

        # Fuse classical and quantum
        combined = torch.cat([classical_features, quantum_features], dim=-1)
        fused_features = self.fusion_network(combined)

        # V8.5.3 FIX: Bounded forecast generation
        # Generate raw forecasts (Tanh already bounds to [-1, +1])
        forecast_raw = self.forecast_head(fused_features)

        # Scale from [-1, +1] to [-max_forecast_magnitude, +max_forecast_magnitude]
        # and apply extra clipping for safety
        forecast = torch.clamp(
            forecast_raw * self.max_forecast_magnitude,
            -self.max_forecast_magnitude,
            self.max_forecast_magnitude
        )

        # Generate confidence (already bounded by Sigmoid)
        confidence = self.confidence_head(fused_features)
        confidence = torch.clamp(confidence, 0.0, 1.0)  # Extra safety

        return forecast, confidence

    def predict_q_adjustment(
        self,
        state: torch.Tensor,
        q_values: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Adjust Q-values based on quantum-enhanced forecasting with CRITICAL logging.

        This is the CORE METHOD that enables quantum tempering throughout the system.

        Args:
            state: Current market state [batch_size, state_dim]
            q_values: Original Q-values from agent [batch_size, n_actions]

        Returns:
            adjusted_q_values: Quantum-tempered Q-values [batch_size, n_actions]
            metadata: Dictionary with comprehensive tempering information
        """
        with torch.no_grad():
            # Log entry
            logger.critical(f"ğŸ”® [QUANTUM ADVISOR] Processing {state.shape[0]} samples")

            # Generate quantum forecast and confidence
            forecast, confidence = self.forward(state)

            # ================================================================
            # V8.5.5: COMPREHENSIVE FORECAST DIAGNOSTICS
            # ================================================================
            forecast_mean = forecast.mean().item()
            forecast_std = forecast.std().item()
            forecast_min = forecast.min().item()
            forecast_max = forecast.max().item()
            positive_count = (forecast > 0).sum().item()
            negative_count = (forecast < 0).sum().item()
            total_forecast_elements = forecast.numel()
            positive_ratio = positive_count / total_forecast_elements if total_forecast_elements > 0 else 0.0

            logger.critical(
                f"ğŸ“Š [FORECAST STATS] "
                f"Mean: {forecast_mean:.4f}, Std: {forecast_std:.4f}, "
                f"Range: [{forecast_min:.4f}, {forecast_max:.4f}], "
                f"Positive: {positive_ratio*100:.1f}% ({positive_count}/{total_forecast_elements})"
            )

            # Alert if forecast is clearly biased
            if positive_ratio > 0.8:
                logger.warning(
                    f"âš ï¸  [FORECAST BIAS] >80% positive forecasts "
                    f"({positive_ratio*100:.1f}%) - may cause BUY-only behavior"
                )
            elif positive_ratio < 0.2:
                logger.warning(
                    f"âš ï¸  [FORECAST BIAS] >80% negative forecasts "
                    f"({(1-positive_ratio)*100:.1f}%) - may cause SELL-only behavior"
                )

            # State diagnostics
            state_mean = state.mean().item()
            state_std = state.std().item()
            state_min = state.min().item()
            state_max = state.max().item()

            logger.critical(
                f"ğŸ“Š [STATE STATS] Mean: {state_mean:.4f}, Std: {state_std:.4f}, "
                f"Range: [{state_min:.4f}, {state_max:.4f}]"
            )

            # Alert if state variance is too low
            if state_std < 0.1:
                logger.warning(
                    f"âš ï¸  [STATE VARIANCE] Very low state std ({state_std:.4f}) - "
                    f"states may be too similar, causing uniform forecasts"
                )

            # Confidence diagnostics
            conf_mean = confidence.mean().item()
            conf_std = confidence.std().item()

            logger.critical(
                f"ğŸ“Š [CONFIDENCE STATS] Mean: {conf_mean:.4f}, Std: {conf_std:.4f}"
            )

            logger.critical(
                f"ğŸ”® [QUANTUM ADVISOR] Forecast range: [{forecast.min().item():.4f}, {forecast.max().item():.4f}] | "
                f"Confidence: {confidence.mean().item():.4f}"
            )


            # V8.5.3 FIX: Apply bounded quantum forecast adjustment
            adjusted_q = self.apply_quantum_tempering_to_qvalues(
                q_values,
                forecast,
                confidence,
                alpha=QUANTUM_WEIGHT,      # Uses the 0.1 constant from line 2236
                max_adjustment=1.0         # V8.5.3 NEW: Cap adjustment magnitude
            )

            # Calculate adjustment delta
            adjustment = adjusted_q - q_values

            # Log tempering details for first sample
            if q_values.shape[0] > 0:
                logger.critical(
                    f"ğŸ”® [QUANTUM TEMPERING] Original Q: BUY={q_values[0, 0]:.6f} SELL={q_values[0, 1]:.6f} | "
                    f"Adjusted Q: BUY={adjusted_q[0, 0]:.6f} SELL={adjusted_q[0, 1]:.6f} | "
                    f"Delta: [{adjustment[0, 0]:.6f}, {adjustment[0, 1]:.6f}]"
                )

            # Build comprehensive metadata
            metadata = {
                'forecast': forecast.detach().cpu().numpy().tolist(),
                'confidence': confidence.detach().cpu().numpy().tolist(),
                'original_q': q_values.detach().cpu().numpy().tolist(),
                'adjusted_q': adjusted_q.detach().cpu().numpy().tolist(),
                'adjustment': adjustment.detach().cpu().numpy().tolist(),
                'quantum_weight': QUANTUM_WEIGHT,
                'forecast_mean': forecast.mean().item(),
                'confidence_mean': confidence.mean().item(),
                'adjustment_magnitude': adjustment.abs().mean().item()
            }

            logger.critical(
                f"ğŸ”® [QUANTUM ADVISOR] Tempering complete | "
                f"Avg adjustment: {metadata['adjustment_magnitude']:.6f} | "
                f"Weight: {QUANTUM_WEIGHT}"
            )

        return adjusted_q, metadata

    def apply_quantum_tempering_to_qvalues(self,
        q_values: torch.Tensor,
        forecast: torch.Tensor,
        confidence: torch.Tensor,
        alpha: float = 0.15,
        max_adjustment: float = 1.0  # V8.5.3 NEW: Maximum Q-value adjustment
    ) -> torch.Tensor:
        """
        Apply DIRECTIONAL and BOUNDED quantum forecast tempering to Q-values.

        V8.5.3 FIX: Clips adjustment magnitude to prevent extreme Q-value changes.

        **KEY CHANGE FROM V8.3**: Forecasts now have ECONOMIC MEANING:
            - Positive forecast (bullish) â†’ BUY favored, SELL disfavored
            - Negative forecast (bearish) â†’ SELL favored, BUY disfavored

        Args:
            q_values: Original Q-values [batch_size, 2] where 2 = [BUY, SELL]
            forecast: Quantum forecast [batch_size, forecast_horizon]
            confidence: Forecast confidence [batch_size, forecast_horizon]
            alpha: Mixing coefficient (default 0.1)
            max_adjustment: Maximum allowed adjustment magnitude (V8.5.3 NEW)

        Returns:
            adjusted_q_values: Directionally-adjusted and bounded Q-values [batch_size, 2]

        V8.3 Example (WRONG - symmetric):
            Original: BUY=0.50, SELL=0.40
            Forecast: +0.03 (bullish)
            V8.3 Result: BUY=0.503, SELL=0.403  âŒ Both increase!

        V8.4 Example (CORRECT - directional):
            Original: BUY=0.50, SELL=0.40
            Forecast: +0.03 (bullish - price expected to rise)
            V8.4 Result: BUY=0.503, SELL=0.397  âœ… BUY up, SELL down!
        """
        # Type validation
        if not isinstance(forecast, torch.Tensor):
            raise TypeError(f"forecast must be torch.Tensor, got {type(forecast)}")
        if not isinstance(confidence, torch.Tensor):
            raise TypeError(f"confidence must be torch.Tensor, got {type(confidence)}")
        if not isinstance(q_values, torch.Tensor):
            raise TypeError(f"q_values must be torch.Tensor, got {type(q_values)}")

        # Shape validation
        if forecast.dim() != 2:
            raise ValueError(f"forecast must be 2D [batch, horizon], got {forecast.shape}")
        if confidence.dim() != 2:
            raise ValueError(f"confidence must be 2D [batch, horizon], got {confidence.shape}")
        if q_values.dim() != 2:
            raise ValueError(f"q_values must be 2D [batch, actions], got {q_values.shape}")

        # V8.4: Verify exactly 2 actions
        if q_values.shape[1] != 2:
            raise ValueError(
                f"V8.4 directional tempering requires exactly 2 actions [BUY, SELL], "
                f"got {q_values.shape[1]}"
            )

        batch_size = q_values.shape[0]
        horizon = forecast.shape[1]
        device = q_values.device

        # STEP 1: Aggregate forecast into directional signal
        # Near-term forecasts weighted more than far-term
        decay_factors = torch.pow(0.95, torch.arange(horizon, device=device).float())
        weighted_forecast = (forecast * confidence * decay_factors).sum(dim=1)
        weight_sum = (confidence * decay_factors).sum(dim=1) + 1e-8
        directional_signal = weighted_forecast / weight_sum
        directional_signal = directional_signal.unsqueeze(1)

        # V8.5.5: Log directional signal statistics
        signal_mean = directional_signal.mean().item()
        signal_std = directional_signal.std().item()
        signal_min = directional_signal.min().item()
        signal_max = directional_signal.max().item()
        positive_signals = (directional_signal > 0).sum().item()
        total_signals = directional_signal.numel()

        logger.critical(
            f"ğŸ“Š [DIRECTIONAL SIGNAL] "
            f"Mean: {signal_mean:.4f}, Std: {signal_std:.4f}, "
            f"Range: [{signal_min:.4f}, {signal_max:.4f}], "
            f"Positive: {positive_signals}/{total_signals}"
        )

        if positive_signals == total_signals:
            logger.warning("âš ï¸  [SIGNAL BIAS] ALL signals positive - will favor BUY across all agents")
        elif positive_signals == 0:
            logger.warning("âš ï¸  [SIGNAL BIAS] ALL signals negative - will favor SELL across all agents")

        # V8.5.3 FIX: Clip directional signal BEFORE applying to Q-values
        # This prevents extreme forecasts from causing extreme Q-value changes
        # If alpha=0.1 and max_adjustment=1.0, then max_signal=10.0
        max_signal = max_adjustment / alpha
        directional_signal_clipped = torch.clamp(directional_signal, -max_signal, max_signal)

        # Log if clipping occurred
        signal_before_clip = directional_signal.abs().max().item()
        signal_after_clip = directional_signal_clipped.abs().max().item()
        if signal_before_clip > max_signal:
            logger.warning(
                f"âš ï¸  [QUANTUM TEMPERING] Signal clipped: {signal_before_clip:.2f} â†’ "
                f"{signal_after_clip:.2f} (max: {max_signal:.2f})"
            )

        # STEP 2: Apply DIRECTIONAL adjustment (V8.4)
        # Positive signal â†’ BUY gets +adjustment, SELL gets -adjustment
        # Negative signal â†’ BUY gets -adjustment, SELL gets +adjustment
        adjustment_vector = torch.stack([
            directional_signal_clipped.squeeze(1),   # BUY gets clipped signal as-is
            -directional_signal_clipped.squeeze(1)   # SELL gets negated clipped signal
        ], dim=1)

        # Scale by alpha
        adjustment_vector = adjustment_vector * alpha

        # V8.5.3 FIX: Final safety clip on adjustment magnitude
        adjustment_vector = torch.clamp(adjustment_vector, -max_adjustment, max_adjustment)

        # STEP 3: Apply to Q-values
        adjusted_q_values = q_values + adjustment_vector

        return adjusted_q_values
    # ============================================================================
    # COMPONENT 3: QUANTUM FORECAST APPLICATION
    # ============================================================================
    # INSERT AFTER: QuantumForecastingAdvisor class
    # PURPOSE: Helper functions to apply quantum forecasts to Q-values
    # ============================================================================



def integrate_quantum_advisor(system: 'QuantumTradingSystem') -> Dict[str, bool]:
    """
    Integrate quantum advisor into an existing quantum trading system.

    This function:
    1. Creates a QuantumForecastingAdvisor if not present
    2. Wraps agent encoders with QuantumStateEncodingWrapper
    3. Validates integration

    Args:
        system: Your QuantumTradingSystem instance

    Returns:
        status: Dictionary of integration status checks
    """
    print("\n" + "="*80)
    print("INTEGRATING QUANTUM ADVISOR")
    print("="*80)

    status = {
        'advisor_created': False,
        'encoders_wrapped': False,
        'agents_validated': False,
        'integration_complete': False,
        'error': None
    }

    try:
        # Step 1: Create quantum advisor if not present
        if not hasattr(system, 'quantum_advisor') or system.quantum_advisor is None:
            print("\n1ï¸âƒ£ Creating Quantum Forecasting Advisor...")
            # Fix: Use system.state_dim directly instead of system.config
            state_dim = getattr(system, 'state_dim', 128)
            system.quantum_advisor = QuantumForecastingAdvisor(
                state_dim=state_dim,
                forecast_horizon=30
            )
            status['advisor_created'] = True
            print("   âœ… Quantum advisor created")
            print(f"   State dimension: {state_dim}")
        else:
            status['advisor_created'] = True
            print("\n1ï¸âƒ£ Quantum advisor already exists")

        # Step 2: Wrap agent encoders (if agents have encoders)
        print("\n2ï¸âƒ£ Wrapping agent encoders with quantum enhancement...")
        wrapped_count = 0

        if hasattr(system, 'agents'):
            for i, agent in enumerate(system.agents):
                # Check if agent has an encoder to wrap
                if hasattr(agent, 'encoder') and not isinstance(
                    agent.encoder, QuantumStateEncodingWrapperModule
                ):
                    original_encoder = agent.encoder
                    agent.encoder = QuantumStateEncodingWrapperModule(
                        original_encoder, n_qubits=6
                    )
                    wrapped_count += 1
                    print(f"   âœ… Agent {i}: Encoder wrapped")

        if wrapped_count > 0:
            status['encoders_wrapped'] = True
            print(f"   âœ… Wrapped {wrapped_count} agent encoders")
        else:
            status['encoders_wrapped'] = True  # Not applicable or already wrapped
            print("   â„¹ï¸  No encoders needed wrapping")

        # Step 3: Validate agents have proper architecture
        print("\n3ï¸âƒ£ Validating agent architecture...")
        if hasattr(system, 'agents'):
            all_valid = True
            for i, agent in enumerate(system.agents):
                has_actor = hasattr(agent, 'actor') or hasattr(agent, 'model')
                has_critic = hasattr(agent, 'critic')

                if not has_critic:
                    print(f"   âš ï¸  Agent {i}: Missing critic (will use V5 fixes)")
                    all_valid = False
                else:
                    print(f"   âœ… Agent {i}: Has actor and critic")

            status['agents_validated'] = all_valid
        else:
            status['agents_validated'] = False
            print("   âš ï¸  No agents found in system")

        # Step 4: Final integration check
        print("\n4ï¸âƒ£ Final integration check...")
        checks = [
            hasattr(system, 'quantum_advisor'),
            system.quantum_advisor is not None,
            hasattr(system, 'agents')
        ]

        if all(checks):
            status['integration_complete'] = True
            print("   âœ… ALL CHECKS PASSED")
            print("\n" + "="*80)
            print("QUANTUM ADVISOR INTEGRATION COMPLETE")
            print("="*80)
            print("\nYour system now has:")
            print("   âœ… Quantum forecasting advisor")
            print("   âœ… Quantum-enhanced state encoding")
            print("   âœ… Forward-looking predictions")
            print("   âœ… Confidence-weighted decisions")
            print("="*80 + "\n")
        else:
            status['integration_complete'] = False
            print("   âš ï¸  Some checks failed, but system should still work")

    except Exception as e:
        status['error'] = str(e)
        print(f"\nâŒ Integration error: {e}")
        traceback.print_exc()

    return status


# Your existing LogLevel enum (keep this if you have it, otherwise use this)
class LogLevel(Enum):
    DEBUG = 1
    INFO = 2
    WARNING = 3
    ERROR = 4
    CRITICAL = 5
    FLOW = 6

@dataclass
class LogEntry:
    timestamp: float
    level: LogLevel
    component: str
    message: str
    metadata: Optional[Dict[str, Any]] = None
    importance_score: float = 0.0

    def __post_init__(self):
        self.importance_score = self._calculate_importance()

    def _calculate_importance(self) -> float:
        base_scores = {
            LogLevel.DEBUG: 1.0,
            LogLevel.INFO: 2.0,
            LogLevel.WARNING: 4.0,
            LogLevel.ERROR: 8.0,
            LogLevel.CRITICAL: 10.0,
            LogLevel.FLOW: 1.5
        }

        score = base_scores.get(self.level, 1.0)

        important_keywords = [
            'failed', 'error', 'exception', 'timeout', 'crash', 'fatal',
            'signal', 'trade', 'batch', 'gpu', 'connection', 'reward',
            'training', 'model', 'agent', 'processing'
        ]

        message_lower = self.message.lower()
        keyword_boost = sum(0.5 for keyword in important_keywords if keyword in message_lower)

        important_components = ['BATCH', 'GPU', 'AGENT', 'SIGNAL', 'TRADING', 'META']
        component_boost = 1.0 if self.component in important_components else 0.0

        return score + keyword_boost + component_boost

class DiscordEmbedGenerator:
    """Generate Discord embeds from batched logs"""

    def __init__(self):
        self.color_map = {
            LogLevel.DEBUG: 0x9B9B9B,      # Gray
            LogLevel.INFO: 0x3498DB,       # Blue
            LogLevel.WARNING: 0xF39C12,    # Orange
            LogLevel.ERROR: 0xE74C3C,      # Red
            LogLevel.CRITICAL: 0x992D22,   # Dark Red
            LogLevel.FLOW: 0x9B59B6        # Purple
        }

        self.emoji_map = {
            LogLevel.DEBUG: "ğŸ”",
            LogLevel.INFO: "â„¹ï¸",
            LogLevel.WARNING: "âš ï¸",
            LogLevel.ERROR: "âŒ",
            LogLevel.CRITICAL: "ğŸ’¥",
            LogLevel.FLOW: "ğŸ”„"
        }

    def generate_discord_payload(self, top_logs: List[LogEntry], stats: Dict[str, Any]) -> Dict[str, Any]:
        """Generate standard Discord webhook payload with embeds"""

        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        level_counts = Counter(log.level.name for log in top_logs)
        component_counts = Counter(log.component for log in top_logs)

        # Main embed
        main_embed = {
            "title": f"ğŸ¤– Trading Bot Log 2 - Top {len(top_logs)} Priority Logs",
            "description": f"Generated: {current_time}",
            "color": self._get_overall_color(level_counts),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "fields": []
        }

        # Add statistics fields
        stats_field = {
             "name": "ğŸ“Š Log Statistics",
            "value": self._format_stats(level_counts, stats),
            "inline": False
        }
        main_embed["fields"].append(stats_field)

        # Add component breakdown
        if component_counts:
            components_field = {
                "name": "ğŸ·ï¸ Component Breakdown",
                "value": self._format_components(component_counts),
                "inline": True
            }
            main_embed["fields"].append(components_field)

        # Add footer
        main_embed["footer"] = {
            "text": "Trading Bot Discord Logging System"
        }

        # Create payload
        payload = {
            "username": "Trading Bot Logger 2",
            "embeds": [main_embed]
        }

        # Add individual log entries as separate embeds (limit to 8)
        log_embeds = self._create_log_embeds(top_logs[:8])
        payload["embeds"].extend(log_embeds)

        # If more logs, add as text content
        if len(top_logs) > 8:
            remaining_logs = self._format_remaining_logs(top_logs[8:])
            payload["content"] = f"Additional Logs ({len(top_logs[8:])} more):\n```\n{remaining_logs}\n```"

        return payload

    def generate_enhanced_discord_payload(self, top_logs: List[LogEntry], stats: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhanced Discord payload generation that shows MORE logs efficiently.
        Uses mix of detailed embeds and compact text format.
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        level_counts = Counter(log.level.name for log in top_logs)
        component_counts = Counter(log.component for log in top_logs)

        # Main embed
        main_embed = {
            "title": f"ğŸ¤– Trading Bot Log 2 - Top {len(top_logs)} Priority Logs",
            "description": f"Generated: {current_time}",
            "color": self._get_overall_color(level_counts),
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "fields": []
        }

        # Add statistics
        stats_field = {
            "name": "ğŸ“Š Log Statistics",
            "value": self._format_stats(level_counts, stats),
            "inline": False
        }
        main_embed["fields"].append(stats_field)

        # Add component breakdown
        if component_counts:
            components_field = {
                "name": "ğŸ·ï¸ Component Breakdown",
                "value": self._format_components(component_counts),
                "inline": True
            }
            main_embed["fields"].append(components_field)

        main_embed["footer"] = {
            "text": "Enhanced Trading Bot Discord Logging System"
        }

        # Create payload
        payload = {
            "username": "Trading Bot Logger 2",
            "embeds": [main_embed]
        }

        # Add enhanced log embeds (showing up to 30 in detailed format)
        log_embeds = self._create_enhanced_log_embeds(top_logs[:30])
        payload["embeds"].extend(log_embeds)

        # Add remaining logs as enhanced text content
        if len(top_logs) > 30:
            remaining_logs = self._format_remaining_logs(top_logs[30:])
            payload["content"] = f"Additional Logs ({len(top_logs[30:])} more):\n```\n{remaining_logs}\n```"

        return payload

    def _get_overall_color(self, level_counts: Counter) -> int:
        """Determine overall embed color based on log levels"""
        if level_counts.get('CRITICAL', 0) > 0:
            return self.color_map[LogLevel.CRITICAL]
        elif level_counts.get('ERROR', 0) > 0:
            return self.color_map[LogLevel.ERROR]
        elif level_counts.get('WARNING', 0) > 0:
            return self.color_map[LogLevel.WARNING]
        else:
            return self.color_map[LogLevel.INFO]

    def _format_stats(self, level_counts: Counter, stats: Dict[str, Any]) -> str:
        """Format statistics for Discord"""
        stats_lines = []

        for level in ['CRITICAL', 'ERROR', 'WARNING', 'INFO']:
            count = level_counts.get(level, 0)
            emoji = self.emoji_map.get(LogLevel[level], "")
            if count > 0:
                stats_lines.append(f"{emoji} **{level}**: {count}")

        if stats.get('total_logs_buffered', 0) > 0:
            stats_lines.append(f"ğŸ“ **Total Buffered**: {stats['total_logs_buffered']}")

        if stats.get('current_hourly_rate', 0) > 0:
            stats_lines.append(f"â±ï¸ **Hourly Rate**: {stats['current_hourly_rate']:.1f}")

        return "\n".join(stats_lines) if stats_lines else "No statistics available"

    def _format_components(self, component_counts: Counter) -> str:
        """Format component breakdown for Discord"""
        top_components = component_counts.most_common(5)
        return "\n".join([f"**{comp}**: {count}" for comp, count in top_components])

    def _create_log_embeds(self, logs: List[LogEntry]) -> List[Dict[str, Any]]:
        """Create individual embeds for top priority logs"""
        embeds = []

        for i, log in enumerate(logs, 1):
            timestamp_str = datetime.fromtimestamp(log.timestamp).strftime("%H:%M:%S.%f")[:-3]
            emoji = self.emoji_map.get(log.level, "ğŸ“")

            embed = {
                "title": f"{emoji} #{i}: {log.level.name} - {log.component}",
                "description": f"```\n{log.message[:1000]}{'...' if len(log.message) > 1000 else ''}\n```",
                "color": self.color_map.get(log.level, 0x9B9B9B),
                "fields": [
                    {
                        "name": "â° Time",
                        "value": timestamp_str,
                        "inline": True
                    },
                    {
                        "name": "ğŸ¯ Priority Score",
                        "value": f"{log.importance_score:.1f}",
                        "inline": True
                    }
                ]
            }

            if log.metadata:
                metadata_str = json.dumps(log.metadata, indent=2, default=str)[:500]
                embed["fields"].append({
                    "name": "ğŸ“‹ Metadata",
                    "value": f"```json\n{metadata_str}\n```",
                    "inline": False
                })

            embeds.append(embed)

        return embeds

    def _format_remaining_logs(self, logs: List[LogEntry]) -> str:
        """
        Format remaining logs as text - FIXED VERSION
        Shows up to 50 logs while respecting Discord's 2000 character limit.
        """
        lines = []
        max_logs = min(50, len(logs))  # FIXED: Was min(0, len(logs))
        current_length = 0
        max_content_length = 1800  # Leave 200 chars for Discord formatting

        for i, log in enumerate(logs[:max_logs]):
            # Compact timestamp
            timestamp_str = datetime.fromtimestamp(log.timestamp).strftime("%H:%M:%S")
            emoji = self.emoji_map.get(log.level, "ğŸ“")

            # Adaptive message truncation based on log level
            if log.level.name in ['CRITICAL', 'ERROR']:
                max_msg_len = 1200
            elif log.level.name == 'CRITICAL':
                max_msg_len = 1000
            else:
                max_msg_len = 80

            # Truncate message
            message = log.message[:max_msg_len]
            if len(log.message) > max_msg_len:
                message += "..."

            # Create compact log line
            line = f"{emoji}[{timestamp_str}] {log.component}: {message}"

            # Check if adding this line would exceed limit
            if current_length + len(line) + 1 > max_content_length:
                remaining_count = len(logs) - i
                lines.append(f"... and {remaining_count} more logs [truncated due to length]")
                break

            lines.append(line)
            current_length += len(line) + 1

        # Add final count if we showed all requested but more exist
        if len(logs) > max_logs and current_length < max_content_length:
            remaining = len(logs) - max_logs
            lines.append(f"... and {remaining} more logs")

        return "\n".join(lines)

    def _create_enhanced_log_embeds(self, logs: List[LogEntry]) -> List[Dict[str, Any]]:
        """
        Enhanced embed creation that fits more logs using smart grouping.
        Detailed embeds for CRITICAL/ERROR, compact for others.
        """
        embeds = []
        critical_logs = []
        other_logs = []

        # Separate by importance
        for log in logs:
            if log.level.name in ['CRITICAL', 'ERROR']:
                critical_logs.append(log)
            else:
                other_logs.append(log)

        # Create detailed embeds for critical/error (max 5)
        for i, log in enumerate(critical_logs[:5]):
            embed = self._create_detailed_embed(log, i + 1, is_critical=True)
            embeds.append(embed)

        # Create compact embeds for other logs (fit more in remaining space)
        remaining_slots = 9 - len(embeds)  # Discord allows 10 embeds max (1 main + 9 logs)
        if remaining_slots > 0:
            compact_embeds = self._create_compact_embeds(
                other_logs,
                remaining_slots,
                start_num=len(embeds)+1
            )
            embeds.extend(compact_embeds)

        return embeds

    def _create_detailed_embed(self, log: LogEntry, num: int, is_critical: bool = False) -> Dict[str, Any]:
        """Create a detailed embed for important logs"""
        timestamp_str = datetime.fromtimestamp(log.timestamp).strftime("%H:%M:%S.%f")[:-3]
        emoji = self.emoji_map.get(log.level, "ğŸ“")

        embed = {
            "title": f"{emoji} #{num}: {log.level.name} - {log.component}",
            "description": f"```\n{log.message[:1500]}{'...' if len(log.message) > 1500 else ''}\n```",
            "color": self.color_map.get(log.level, 0x9B9B9B),
            "fields": [
                {
                    "name": "â° Time",
                    "value": timestamp_str,
                    "inline": True
                },
                {
                    "name": "ğŸ¯ Priority Score",
                    "value": f"{log.importance_score:.1f}",
                    "inline": True
                }
            ]
        }

        # Add metadata only for critical logs
        if is_critical and log.metadata:
            metadata_str = json.dumps(log.metadata, indent=2, default=str)[:300]
            embed["fields"].append({
                "name": "ğŸ“‹ Metadata",
                "value": f"```json\n{metadata_str}\n```",
                "inline": False
            })

        return embed

    def _create_compact_embeds(self, logs: List[LogEntry], max_embeds: int, start_num: int) -> List[Dict[str, Any]]:
        """Create compact embeds that fit multiple logs per embed"""
        embeds = []
        logs_per_embed = max(1, len(logs) // max_embeds + (1 if len(logs) % max_embeds else 0))

        for i in range(0, len(logs), logs_per_embed):
            batch = logs[i:i + logs_per_embed]
            if not batch:
                break

            # Create summary embed for this batch
            batch_num = start_num + len(embeds)
            description_lines = []

            for log in batch:
                timestamp_str = datetime.fromtimestamp(log.timestamp).strftime("%H:%M:%S")
                emoji = self.emoji_map.get(log.level, "ğŸ“")
                message = log.message[:80] + "..." if len(log.message) > 80 else log.message
                description_lines.append(f"{emoji} `{timestamp_str}` **{log.component}**: {message}")

            embed = {
                "title": f"ğŸ“‹ Batch #{batch_num}: {len(batch)} Logs",
                "description": "\n".join(description_lines),
                "color": 0x3498DB,  # Blue for batch embeds
                "footer": {
                    "text": f"Logs {i+1}-{min(i+len(batch), len(logs))} of {len(logs)} total"
                }
            }

            embeds.append(embed)

            if len(embeds) >= max_embeds:
                break

        return embeds

# ============================================================================
# DISCORD BATCHER THREAD FIX - STOPS RESTART LOOP
# ============================================================================
# This fixes the "Discord batcher thread not running - restarting now" warning loop

# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import queue
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import requests
# REMOVED DUPLICATE: from collections import deque
from typing import Dict, Any, Optional
# REMOVED DUPLICATE: from dataclasses import dataclass
# REMOVED DUPLICATE: from enum import Enum

logger = logging.getLogger(__name__)

# ============================================================================
# STEP 1: IDENTIFY THE PROBLEM
# ============================================================================
"""
The restart loop happens because:
1. Worker thread crashes silently
2. Health monitor detects it's not running
3. Tries to restart, but crashes again
4. Infinite loop

Root causes:
- Queue.get() timeout exceptions not handled
- Requests to Discord webhook failing
- Rate limiting causing crashes
- Threading conflicts
"""

# ============================================================================
# STEP 2: ROBUST DISCORD LOG BATCHER (FIXED)
# ============================================================================

@dataclass
class LogEntry:
    timestamp: float
    level: str  # 'INFO', 'WARNING', 'ERROR', 'CRITICAL'
    component: str
    message: str
    metadata: Optional[Dict[str, Any]] = None

class DiscordLogBatcher:
    """
    FIXED VERSION: Robust Discord log batcher that won't crash
    """

    def __init__(self, webhook_url: str, batch_size: int = 50):
        self.webhook_url = webhook_url
        self.batch_size = batch_size
        self.running = False

        # Thread-safe queue
        self.log_buffer = deque(maxlen=2000)
        self.sent_log_ids = set()
        self.unsent_logs = deque(maxlen=500)

        # Threading
        self.lock = threading.Lock()
        self.discord_thread = None
        self.thread_restart_count = 0
        self.max_restarts = 3

        # Rate limiting
        self.send_history = {
            'daily': deque(maxlen=500),
            'hourly': deque(maxlen=30),
        }

        # Stats
        self.stats = {
            'total_logs_received': 0,
            'total_reports_sent': 0,
            'messages_sent_success': 0,
            'messages_sent_failed': 0,
            'unsent_logs_count': 0,
            'total_logs_buffered': 0,
            'last_send_time': 0,
            'thread_crashes': 0,
            'consecutive_failures': 0
        }

        # Health monitoring
        self.last_heartbeat = time.time()
        self.heartbeat_interval = 30  # seconds

        logger.info("DiscordLogBatcher initialized")

    def start(self):
        """Start the Discord batcher thread with crash protection"""
        if self.running:
            logger.warning("Discord batcher already running")
            return

        self.running = True
        self.thread_restart_count = 0
        self._start_worker_thread()
        self._start_health_monitor()

        logger.info("âœ… Discord batcher started")

    def _start_worker_thread(self):
        """Start worker thread with proper error handling"""
        try:
            self.discord_thread = threading.Thread(
                target=self._worker_loop_safe,
                name="DiscordBatcherWorker",
                daemon=True
            )
            self.discord_thread.start()
            logger.info("Discord worker thread started")
        except Exception as e:
            logger.error(f"Failed to start Discord worker thread: {e}")
            self.running = False

    def _start_health_monitor(self):
        """Start health monitoring thread"""
        def health_check():
            while self.running:
                time.sleep(30)  # Check every 30 seconds

                try:
                    # Check if worker thread is alive
                    if not self.discord_thread or not self.discord_thread.is_alive():
                        logger.warning("âš ï¸ Discord worker thread died, attempting restart...")
                        self._handle_thread_death()

                    # Check heartbeat
                    time_since_heartbeat = time.time() - self.last_heartbeat
                    if time_since_heartbeat > self.heartbeat_interval * 2:
                        logger.warning(f"âš ï¸ Discord worker not responding ({time_since_heartbeat:.0f}s since heartbeat)")
                        self._handle_thread_death()

                    # Check consecutive failures
                    if self.stats['consecutive_failures'] > 10:
                        logger.warning("âš ï¸ Too many consecutive Discord failures, pausing sends")
                        time.sleep(300)  # Pause for 5 minutes
                        self.stats['consecutive_failures'] = 0

                except Exception as e:
                    logger.debug(f"Health monitor error: {e}")

        health_thread = threading.Thread(
            target=health_check,
            name="DiscordHealthMonitor",
            daemon=True
        )
        health_thread.start()
        logger.info("Discord health monitor started")

    def _handle_thread_death(self):
        """Handle worker thread death with restart limit"""
        with self.lock:
            self.stats['thread_crashes'] += 1
            self.thread_restart_count += 1

            if self.thread_restart_count >= self.max_restarts:
                logger.error(f"âŒ Discord worker thread crashed {self.max_restarts} times, giving up")
                self.running = False
                return

            logger.info(f"ğŸ”„ Restarting Discord worker thread (attempt {self.thread_restart_count}/{self.max_restarts})")

            # Wait before restart
            time.sleep(5)

            # Restart worker
            if self.running:
                self._start_worker_thread()

    def _worker_loop_safe(self):
        """
        FIXED: Worker loop with comprehensive error handling
        This prevents silent crashes
        """
        logger.info("Discord worker loop started")

        consecutive_errors = 0
        max_consecutive_errors = 5

        while self.running:
            try:
                # Update heartbeat
                self.last_heartbeat = time.time()

                # Check if we should send
                if len(self.unsent_logs) < self.batch_size:
                    time.sleep(1)
                    continue

                # Check rate limits
                if not self._check_rate_limits():
                    time.sleep(5)
                    continue

                # Prepare batch
                with self.lock:
                    batch = list(self.unsent_logs)[:self.batch_size]

                if not batch:
                    time.sleep(1)
                    continue

                # Send batch
                success = self._send_batch_safe(batch)

                if success:
                    # Remove sent logs
                    with self.lock:
                        for log in batch:
                            try:
                                self.unsent_logs.remove(log)
                            except ValueError:
                                pass

                    self.stats['messages_sent_success'] += 1
                    self.stats['consecutive_failures'] = 0
                    consecutive_errors = 0

                    logger.debug(f"Discord batch sent: {len(batch)} logs")

                else:
                    self.stats['messages_sent_failed'] += 1
                    self.stats['consecutive_failures'] += 1
                    consecutive_errors += 1

                    logger.warning(f"Discord batch send failed (consecutive: {consecutive_errors})")

                    # Back off on consecutive errors
                    if consecutive_errors >= max_consecutive_errors:
                        logger.warning(f"Too many consecutive errors, backing off for 60s")
                        time.sleep(60)
                        consecutive_errors = 0
                    else:
                        time.sleep(5)

                # Update stats
                with self.lock:
                    self.stats['unsent_logs_count'] = len(self.unsent_logs)

            except Exception as e:
                consecutive_errors += 1
                logger.error(f"Discord worker loop error: {e}")

                if consecutive_errors >= max_consecutive_errors:
                    logger.error(f"Too many errors, worker loop exiting")
                    break

                time.sleep(5)

        logger.warning("Discord worker loop exited")
        self.running = False

    def _send_batch_safe(self, batch: list) -> bool:
        """
        FIXED: Safe batch sending with comprehensive error handling
        """
        try:
            if not batch:
                return True

            # Prepare payload
            payload = self._prepare_payload(batch)

            if not payload:
                logger.warning("Empty payload, skipping send")
                return True

            # Send with timeout and retry
            max_retries = 2
            for attempt in range(max_retries):
                try:
                    response = requests.post(
                        self.webhook_url,
                        json=payload,
                        timeout=10,
                        headers={'Content-Type': 'application/json'}
                    )

                    if response.status_code == 204:
                        # Success
                        self.stats['last_send_time'] = time.time()
                        return True

                    elif response.status_code == 429:
                        # Rate limited
                        retry_after = float(response.headers.get('Retry-After', 5))
                        logger.warning(f"Discord rate limited, waiting {retry_after}s")
                        time.sleep(retry_after)
                        continue

                    else:
                        logger.warning(f"Discord webhook error: {response.status_code}")
                        if attempt < max_retries - 1:
                            time.sleep(2 ** attempt)
                            continue
                        return False

                except requests.exceptions.Timeout:
                    logger.warning(f"Discord webhook timeout (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    return False

                except requests.exceptions.ConnectionError:
                    logger.warning(f"Discord connection error (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(5)
                        continue
                    return False

            return False

        except Exception as e:
            logger.error(f"Discord send batch error: {e}")
            return False

    def _prepare_payload(self, batch: list) -> Optional[Dict[str, Any]]:
        """Prepare Discord webhook payload from batch"""
        try:
            if not batch:
                return None

            # Simple text payload (you can enhance this with embeds)
            log_lines = []
            for log in batch[:20]:  # Limit to 20 logs
                if isinstance(log, LogEntry):
                    log_lines.append(f"[{log.level}] {log.component}: {log.message[:100]}")
                elif isinstance(log, dict):
                    log_lines.append(f"[{log.get('level', 'INFO')}] {log.get('message', '')[:100]}")

            if not log_lines:
                return None

            content = "```\n" + "\n".join(log_lines) + "\n```"

            return {
                'content': content[:2000],  # Discord limit
                'username': 'Quantum Trading Bot'
            }

        except Exception as e:
            logger.error(f"Payload preparation error: {e}")
            return None

    def _check_rate_limits(self) -> bool:
        """Check if we can send based on rate limits"""
        current_time = time.time()

        # Clean old entries
        self.send_history['hourly'] = deque([
            t for t in self.send_history['hourly']
            if current_time - t < 3600
        ], maxlen=30)

        # Check hourly limit (25 per hour)
        if len(self.send_history['hourly']) >= 25:
            return False

        # Check minimum interval (60 seconds)
        if self.stats['last_send_time'] > 0:
            time_since_last = current_time - self.stats['last_send_time']
            if time_since_last < 60:
                return False

        return True

    def add_log(self, log_entry: LogEntry):
        """Add log entry to buffer"""
        try:
            with self.lock:
                self.log_buffer.append(log_entry)
                self.unsent_logs.append(log_entry)
                self.stats['total_logs_received'] += 1
                self.stats['unsent_logs_count'] = len(self.unsent_logs)
                self.stats['total_logs_buffered'] = len(self.log_buffer)

        except Exception as e:
            logger.debug(f"Failed to add log: {e}")

    def force_send_report(self):
        """Force send current batch"""
        try:
            with self.lock:
                if self.unsent_logs:
                    batch = list(self.unsent_logs)[:self.batch_size]
                    success = self._send_batch_safe(batch)

                    if success:
                        for log in batch:
                            try:
                                self.unsent_logs.remove(log)
                            except ValueError:
                                pass

                        logger.info(f"Force sent {len(batch)} logs")
                        return True
        except Exception as e:
            logger.error(f"Force send failed: {e}")

        return False

    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics"""
        with self.lock:
            return self.stats.copy()

    def stop(self):
        """Stop the Discord batcher gracefully"""
        logger.info("Stopping Discord batcher...")
        self.running = False

        # Try to send remaining logs
        try:
            if self.unsent_logs:
                logger.info(f"Sending {len(self.unsent_logs)} remaining logs...")
                self.force_send_report()
        except:
            pass

        # Wait for thread to finish
        if self.discord_thread and self.discord_thread.is_alive():
            self.discord_thread.join(timeout=5)

        logger.info("Discord batcher stopped")

    @property
    def is_running(self) -> bool:
        """Check if batcher is running"""
        return self.running and self.discord_thread and self.discord_thread.is_alive()

# ============================================================================
# STEP 3: REPLACE EXISTING DISCORD SETUP
# ============================================================================

def setup_robust_discord_logging(webhook_url: str) -> DiscordLogBatcher:
    """
    Setup robust Discord logging that won't crash

    Usage:
        WEBHOOK_URL = "your_webhook_url"
        discord_batcher = setup_robust_discord_logging(WEBHOOK_URL)
        logger.discord_batcher = discord_batcher
    """

    try:
        # Create robust batcher
        batcher = DiscordLogBatcher(
            webhook_url=webhook_url,
            batch_size=50
        )

        # Start it
        batcher.start()

        # Attach to logger
        logger.discord_batcher = batcher

        # Add safe methods to logger
        import types

        def safe_get_stats(self=None):
            try:
                if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
                    return logger.discord_batcher.get_stats()
            except:
                pass
            return {
                'total_logs_received': 0,
                'messages_sent_success': 0,
                'messages_sent_failed': 0,
                'unsent_logs_count': 0
            }

        def safe_force_send(self=None):
            try:
                if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
                    return logger.discord_batcher.force_send_report()
            except:
                pass
            return False

        logger.get_discord_stats = types.MethodType(safe_get_stats, logger)
        logger.force_send_discord_report = types.MethodType(safe_force_send, logger)

        logger.info("âœ… Robust Discord logging setup complete")
        return batcher

    except Exception as e:
        logger.error(f"Failed to setup Discord logging: {e}")
        raise

# ============================================================================
# STEP 4: DISABLE OLD HEALTH MONITOR (CAUSES RESTART LOOP)
# ============================================================================

def disable_old_discord_monitor():
    """
    Disable the old Discord monitor that causes restart loops
    Call this in your main() function BEFORE starting the system
    """

    # The old monitor is usually started in start_discord_monitoring()
    # or similar function. We need to prevent it from running.

    # Option 1: Monkey-patch the old function
    def noop_monitor(*args, **kwargs):
        logger.info("Old Discord monitor disabled (using new health monitor)")
        pass

    # Replace the old monitor function (adjust function name as needed)
    import sys
    current_module = sys.modules[__name__]

    # Try common monitor function names
    for func_name in ['start_discord_monitoring', 'start_enhanced_discord_monitoring']:
        if hasattr(current_module, func_name):
            setattr(current_module, func_name, noop_monitor)
            logger.info(f"âœ… Disabled old Discord monitor: {func_name}")

# ============================================================================
# STEP 5: INTEGRATION WITH QUANTUM SYSTEM
# ============================================================================

def integrate_discord_with_quantum_system(system, webhook_url: str):
    """
    Complete Discord integration for quantum system (NO RESTART LOOP)
    """
    try:
        disable_old_discord_monitor()
        discord_batcher = setup_robust_discord_logging(webhook_url)
        system.discord_batcher = discord_batcher

        def log_to_discord(level: str, component: str, message: str, metadata: dict = None):
            try:
                entry = LogEntry(
                    timestamp=time.time(),
                    level=level,
                    component=component,
                    message=message,
                    metadata=metadata
                )
                discord_batcher.add_log(entry)
            except Exception as e:
                logger.debug(f"Failed to log to Discord: {e}")

        system.log_to_discord = log_to_discord

        # âœ… Safe: only hook batch processor if it exists
        if hasattr(system, "batch_processor") and system.batch_processor is not None:
            if hasattr(system.batch_processor, "_process_batch_supervised"):
                original_process = system.batch_processor._process_batch_supervised

                def logged_process(batch):
                    result = original_process(batch)
                    if system.batch_processor.stats.get("batches_processed", 0) % 50 == 0:
                        stats = system.batch_processor.get_stats()
                        system.log_to_discord(
                            "INFO",
                            "BATCH_PROCESSOR",
                            f"Processed {stats.get('processed', 0)} rewards "
                            f"in {stats.get('batches_processed', 0)} batches",
                            metadata=stats
                        )
                    return result

                system.batch_processor._process_batch_supervised = logged_process
                logger.info("âœ… Discord hooked into batch processor successfully.")
            else:
                logger.warning("âš ï¸ Batch processor has no _process_batch_supervised method yet.")
        else:
            logger.info("â„¹ï¸ Batch processor not yet initialized â€” skipping hook (safe).")

        logger.info("âœ… Discord integrated with quantum system (no restart loop)")
        return discord_batcher

    except Exception as e:
        logger.error(f"Discord integration failed: {e}")
        return None

# ============================================================================
# USAGE EXAMPLE
# ============================================================================

# ============================================================================
# VERIFICATION
# ============================================================================

def verify_discord_health(system):
    """Verify Discord is working without restart loop"""

    checks = {
        'batcher_exists': hasattr(system, 'discord_batcher'),
        'batcher_running': False,
        'thread_alive': False,
        'no_excessive_crashes': False,
        'heartbeat_ok': False
    }

    if checks['batcher_exists']:
        batcher = system.discord_batcher
        checks['batcher_running'] = batcher.running
        checks['thread_alive'] = batcher.discord_thread and batcher.discord_thread.is_alive()
        checks['no_excessive_crashes'] = batcher.stats.get('thread_crashes', 0) < 3

        time_since_heartbeat = time.time() - batcher.last_heartbeat
        checks['heartbeat_ok'] = time_since_heartbeat < 60

    logger.info("=== DISCORD HEALTH CHECK ===")
    for check, status in checks.items():
        emoji = "âœ…" if status else "âŒ"
        logger.info(f"{emoji} {check}: {status}")

    all_good = all(checks.values())
    if all_good:
        logger.info("âœ… Discord system healthy (no restart loop)")
    else:
        logger.warning("âš ï¸ Discord system issues detected")

    return all_good

class EnhancedDiscordLogger:
    """Enhanced Discord logger - replacement for EnhancedEmailLogger"""

    def __init__(self, webhook_url: str, silent_mode: bool = False, batch_size: int = 65):
        self.webhook_url = webhook_url
        self.silent_mode = silent_mode

        # Core logging components
        self.buffer = deque(maxlen=1000)
        self.flow_stack = []
        self.component_logs = defaultdict(lambda: deque(maxlen=100))
        self.performance = {}
        self.lock = threading.Lock()
        self.min_level = LogLevel.INFO
        self.muted = set()

        # Discord batching system (replaces email system)
        self.discord_batcher = DiscordLogBatcher(
            webhook_url=webhook_url,
            batch_size=batch_size
        )

        # Discord criteria (replaces email levels)
        self.discord_levels = {LogLevel.ERROR, LogLevel.CRITICAL}
        self.last_discord_send = {}

        # Console colors
        self.colors = {
            LogLevel.DEBUG: '\033[36m',    # Cyan
            LogLevel.INFO: '\033[32m',     # Green
            LogLevel.WARNING: '\033[33m',  # Yellow
            LogLevel.ERROR: '\033[31m',    # Red
            LogLevel.CRITICAL: '\033[91m', # Bright Red
            LogLevel.FLOW: '\033[35m',     # Magenta
        }
        self.reset = '\033[0m'

        # Start Discord batcher
        self.discord_batcher.start()

        if not self.silent_mode:
            print("Enhanced Discord Logger initialized - Continuous sending when 65 fresh logs collected")

    def _should_add_to_discord_batch(self, level: LogLevel, component: str, message: str) -> bool:
        """Check if log should be added to Discord batch"""
        if level not in self.discord_levels:
            return False

        # Rate limiting for duplicate messages
        msg_key = f"{component}:{message[:50]}"
        current_time = time.time()

        if msg_key in self.last_discord_send:
            if current_time - self.last_discord_send[msg_key] < 70:  # 5 minutes
                return False

        self.last_discord_send[msg_key] = current_time
        return True

    def _log(self, level, component, message, metadata=None, force_discord=False):
        if level.value < self.min_level.value or component in self.muted:
            return

        timestamp = time.time()

        # Create log entry
        log_entry = LogEntry(
            timestamp=timestamp,
            level=level,
            component=component,
            message=message,
            metadata=metadata
        )

        # Console output (if not silent)
        if not self.silent_mode:
            self._print_log(log_entry)

        # Add to buffer
        with self.lock:
            self.buffer.append(log_entry)
            self.component_logs[component].append(log_entry)

        # Add to Discord batch if criteria met
        if force_discord or self._should_add_to_discord_batch(level, component, message):
            self.discord_batcher.add_log(log_entry)

    def _print_log(self, log_entry: LogEntry):
        """Print log to console"""
        timestamp_str = datetime.fromtimestamp(log_entry.timestamp).strftime('%H:%M:%S.%f')[:-3]
        indent = '  ' * len(self.flow_stack)

        symbols = {
            LogLevel.DEBUG: 'ğŸ”',
            LogLevel.INFO: 'â„¹ï¸',
            LogLevel.WARNING: 'âš ï¸',
            LogLevel.ERROR: 'âŒ',
            LogLevel.CRITICAL: 'ğŸ’¥',
            LogLevel.FLOW: 'ğŸ”„'
        }

        color = self.colors.get(log_entry.level, '')
        symbol = symbols.get(log_entry.level, 'ğŸ“')

        formatted = f"{color}[{timestamp_str}] {symbol} {indent}[{log_entry.component}] {log_entry.message}{self.reset}"

        if log_entry.metadata:
            formatted += f" | {json.dumps(log_entry.metadata, default=str)}"

        print(formatted)

    # Logging methods (same interface as before)
    def debug(self, component_or_message, message=None, metadata=None, **kwargs):
        if message is None:
            self._log(LogLevel.DEBUG, "System", component_or_message, metadata)
        else:
            self._log(LogLevel.DEBUG, component_or_message, message, metadata)

    def info(self, component_or_message, message=None, metadata=None, **kwargs):
        if message is None:
            self._log(LogLevel.INFO, "System", component_or_message, metadata)
        else:
            self._log(LogLevel.INFO, component_or_message, message, metadata)

    def warning(self, component_or_message, message=None, metadata=None, force_discord=False, **kwargs):
        if message is None:
            self._log(LogLevel.WARNING, "System", component_or_message, metadata, force_discord)
        else:
            self._log(LogLevel.WARNING, component_or_message, message, metadata, force_discord)

    def error(self, component_or_message, message=None, metadata=None, force_discord=True, **kwargs):
        if message is None:
            self._log(LogLevel.ERROR, "System", component_or_message, metadata, force_discord)
        else:
            self._log(LogLevel.ERROR, component_or_message, message, metadata, force_discord)

    def critical(self, component_or_message, message=None, metadata=None, force_discord=True, **kwargs):
        if message is None:
            self._log(LogLevel.CRITICAL, "System", component_or_message, metadata, force_discord)
        else:
            self._log(LogLevel.CRITICAL, component_or_message, message, metadata, force_discord)

    def flow(self, component, message, metadata=None):
        self._log(LogLevel.FLOW, component, f"FLOW: {message}", metadata)

    @contextmanager
    def track(self, component, operation, metadata=None):
        start_time = time.time()
        flow_id = f"{component}.{operation}"

        self.flow_stack.append(flow_id)
        self.flow(component, f"â†’ {operation}", metadata)

        try:
            yield
            duration = time.time() - start_time
            self.flow(component, f"â† {operation} ({duration:.3f}s)")

            if component not in self.performance:
                self.performance[component] = {}
            if operation not in self.performance[component]:
                self.performance[component][operation] = []
            self.performance[component][operation].append(duration)

        except Exception as e:
            duration = time.time() - start_time
            self.error(component, f"FAILED {operation} after {duration:.3f}s: {str(e)}")
            raise
        finally:
            if self.flow_stack and self.flow_stack[-1] == flow_id:
                self.flow_stack.pop()

    # Control methods
    def set_level(self, level_name):
        self.min_level = LogLevel[level_name.upper()]
        self.info("Logger", f"Level set to {level_name.upper()}")

    def mute(self, *components):
        self.muted.update(components)
        self.info("Logger", f"Muted: {list(components)}")

    def unmute(self, *components):
        self.muted -= set(components)
        self.info("Logger", f"Unmuted: {list(components)}")

    def enable_discord_for_level(self, level: LogLevel):
        self.discord_levels.add(level)
        self.info("Logger", f"Discord enabled for {level.name}")

    def disable_discord_for_level(self, level: LogLevel):
        self.discord_levels.discard(level)
        self.info("Logger", f"Discord disabled for {level.name}")

    def set_silent_mode(self, silent=False):
        self.silent_mode = silent
        if not silent:
            print(f"Silent mode {'ENABLED' if silent else 'DISABLED'}")

    def force_send_discord_report(self):
        """Force send an immediate Discord report"""
        self.discord_batcher.force_send_report()
        self.info("Logger", "Forced Discord report sent")

    import types

    def safe_get_discord_stats():
        return {
            'unsent_logs_count': 0,
            'messages_sent_failed': 0,
            'daily_quota_used': 0,
            'aggressive_sends': 0,
            'critical_bypasses': 0
        }

    def safe_force_send_discord_report():
        print("INFO: [Mock] Discord report flush simulated")

    # Patch safely
    if not hasattr(logger, 'get_discord_stats'):
        logger.get_discord_stats = types.MethodType(lambda self=None: safe_get_discord_stats(), logger)

    if not hasattr(logger, 'force_send_discord_report'):
        logger.force_send_discord_report = types.MethodType(lambda self=None: safe_force_send_discord_report(), logger)

# Mock discord batcher
class MockBatcher:
    is_running = True
    def start(self):
        print("INFO: [Mock] Discord batcher thread restarted")

if not hasattr(logger, 'discord_batcher'):
    logger.discord_batcher = MockBatcher()

    # Compatibility methods (renamed from email methods)
    def get_email_stats(self) -> Dict[str, Any]:
        """Compatibility method - returns Discord stats"""
        return self.get_discord_stats()

    def force_send_email_report(self):
        """Compatibility method - sends Discord report"""
        self.force_send_discord_report()

    def show_recent(self, count=20, component=None):
        print(f"\nğŸ“‹ Recent Logs (last {count}):")
        print("=" * 80)

        logs = list(self.buffer)
        if component:
            logs = [log for log in logs if log.component == component]

        for log in logs[-count:]:
            self._print_log(log)
        print("=" * 80)

    def show_discord_stats(self):
        """Show Discord batching statistics"""
        stats = self.get_discord_stats()
        print(f"\nğŸ® Discord Batching Statistics:")
        print(f"  Reports Sent: {stats['total_reports_sent']}")
        print(f"  Success Rate: {stats['messages_sent_success']}/{stats['messages_sent_success'] + stats['messages_sent_failed']}")
        print(f"  Current Buffer: {stats['total_logs_buffered']} logs")
        print(f"  Unsent Logs: {stats['unsent_logs_count']}")

    def stop(self):
        """Stop the logger and send final report"""
        self.discord_batcher.stop()

# ============================================================================
# REPLACE YOUR EXISTING LOGGER INITIALIZATION WITH THIS:
# ============================================================================

# Your Discord webhook URL
WEBHOOK_URL = "https://discordapp.com/api/webhooks/1422597824851345489/bmJgtiL_jyjW1XTBErBrlrtMF9atVnX7CzwIUVOhrbd2hiPtklD6sZJpk8KqLNlCyIGN"

# Replace your existing logger initialization:
logger = EnhancedDiscordLogger(
    webhook_url=WEBHOOK_URL,
    batch_size=50,
    silent_mode=False
)

# Configure Discord reporting levels (same as your email levels)
logger.enable_discord_for_level(LogLevel.ERROR)
logger.enable_discord_for_level(LogLevel.CRITICAL)

# Set console logging level (same as before)
logger.set_level('critical')

print("Enhanced Discord Logger updated with aggressive configuration:")
print(f"  â€¢ Batch sizes: 75-65-75 logs (min-optimal-max)")
print(f"  â€¢ Daily limit: 450 Discord messages")
print(f"  â€¢ Send interval: 60s minimum")
print(f"  â€¢ Aggressive mode: True")
print(f"  â€¢ Webhook: {WEBHOOK_URL[:50]}...")

# ============================================================================
# HELPER FUNCTIONS FOR MONITORING AND CONTROL
# ============================================================================
def patch_discord_stats_safe(logger_obj):
    """
    CRITICAL FIX: Patch logger to add missing discord stats methods
    This fixes: 'Logger' object has no attribute 'get_discord_stats'
    """
    import types

    if not hasattr(logger_obj, 'get_discord_stats'):
        def safe_get_discord_stats(self=None):
            return {
                'total_reports_sent': 0,
                'unsent_logs_count': 0,
                'messages_sent_success': 0,
                'messages_sent_failed': 0,
                'current_hourly_rate': 0.0,
                'daily_quota_used': 0.0
            }
        logger_obj.get_discord_stats = types.MethodType(safe_get_discord_stats, logger_obj)
        logger.info("âœ“ Patched logger with safe get_discord_stats")

    if not hasattr(logger_obj, 'force_send_discord_report'):
        def safe_force_send(self=None):
            logger.info("Discord report flush (mock)")
        logger_obj.force_send_discord_report = types.MethodType(safe_force_send, logger_obj)
        logger.info("âœ“ Patched logger with safe force_send_discord_report")

    if not hasattr(logger_obj, 'discord_batcher'):
        class MockBatcher:
            is_running = True
            def start(self): logger.info("Discord batcher start (mock)")
            def stop(self): logger.info("Discord batcher stop (mock)")
            def get_stats(self): return safe_get_discord_stats()
        logger_obj.discord_batcher = MockBatcher()
        logger.info("âœ“ Patched logger with mock discord_batcher")

def log_discord_stats():
    """Enhanced Discord statistics logging"""
    stats = logger.get_discord_stats()

    logger.info("DiscordStats",
                f"Reports: {stats['total_reports_sent']}, "
                f"Unsent: {stats['unsent_logs_count']}/65, "
                f"Success: {stats['messages_sent_success']}, "
                f"Aggressive: {stats.get('aggressive_sends', 0)}")

    # Log rate status
    rate_status = stats.get('rate_status', {})
    for window, count in rate_status.items():
        if count > 0:
            print(f"Rate {window}: {count}")

def force_discord_report():
    """Enhanced force Discord report"""
    logger.force_send_discord_report()

def get_unsent_log_count():
    """Enhanced unsent log count"""
    return logger.discord_batcher.get_unsent_count()

def start_enhanced_discord_monitoring():
    """Enhanced Discord monitoring with aggressive mode awareness and auto-flush"""
    def enhanced_discord_monitor():
        optimal_size = 50  # Your aggressive optimal batch size
        check_interval = 30  # Check every minute

        while True:
            time.sleep(check_interval)
            try:
                stats = logger.get_discord_stats()
                unsent_count = stats['unsent_logs_count']

                # ğŸ”„ Auto-flush logic
                if unsent_count >= optimal_size * 0.8:
                    print(f"INFO: {unsent_count}/{optimal_size} logs pending - Discord will send soon")

                    # ğŸš€ Force flush when buffer is full or exceeds optimal size
                    if unsent_count >= optimal_size:
                        print(f"âš¡ Force flush triggered ({unsent_count} logs) - sending batch to Discord now")
                        logger.force_send_discord_report()
                        time.sleep(2)

                # âš ï¸ Alert on failed sends
                failed = stats.get('messages_sent_failed', 0)
                if failed > 0 and failed % 3 == 0:
                    print(f"WARNING: {failed} Discord reports failed to send - check webhook or rate limits")

                # ğŸ“ˆ Monitor quota usage
                quota_used = stats.get('daily_quota_used', 0)
                if quota_used > 90:
                    print(f"WARNING: High daily quota usage: {quota_used:.1f}%")

                # ğŸ” Aggressive mode / bypass stats
                aggressive_sends = stats.get('aggressive_sends', 0)
                critical_bypasses = stats.get('critical_bypasses', 0)
                if aggressive_sends > 0 and aggressive_sends % 10 == 0:
                    print(f"INFO: Aggressive mode stats - {aggressive_sends} fast sends, {critical_bypasses} critical bypasses")

                # ğŸ§  Debug info (optional)
                if not logger.discord_batcher.is_running:  # âœ… no parentheses
                    print("âš ï¸ Discord batcher thread not running - restarting now")
                    logger.discord_batcher.start()

            except Exception as e:
                print(f"âš ï¸ Discord monitor error: {e}")
                continue

    threading.Thread(target=enhanced_discord_monitor, daemon=True).start()
    print("Enhanced Discord monitoring started âœ… (auto-flush enabled at 65 logs)")

# Start enhanced monitoring
start_enhanced_discord_monitoring()

print("Enhanced Discord logging system activated with aggressive configuration")

# ============================================================================
# INTEGRATION FUNCTIONS FOR YOUR EXISTING SYSTEM
# ============================================================================

def integrate_with_system(system):
    """Integrate Discord logger with your trading system"""

    # Add Discord reporting to your system monitoring
    original_get_status = system.get_system_status

    def enhanced_get_status():
        status = original_get_status()
        discord_stats = logger.get_discord_stats()
        status['discord_logging'] = {
            'reports_sent': discord_stats['total_reports_sent'],
            'unsent_logs': discord_stats['unsent_logs_count'],
            'send_threshold': 65,
            'next_send': f"When {65 - discord_stats['unsent_logs_count']} more logs received"
        }
        return status

    system.get_system_status = enhanced_get_status

    # Add Discord report trigger to reward processing
    if hasattr(system, 'reward_history'):
        original_save_reward = system._save_reward

        def enhanced_save_reward(reward):
            original_save_reward(reward)
            # Force Discord report every 100 rewards (optional)
            if len(system.reward_history) % 100 == 0:
                logger.force_send_discord_report()

        system._save_reward = enhanced_save_reward

# ============================================================================
# QUANTUM SYSTEM DISCORD INTEGRATION FIX
# ============================================================================
# Paste this AFTER your existing Discord setup in the main file
# This fixes AttributeError and adds quantum-specific logging

# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: from collections import deque
# REMOVED DUPLICATE: from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

# ============================================================================
# STEP 1: FIX LOGGER DISCORD METHODS (Add Missing Methods)
# ============================================================================

def patch_logger_discord_methods():
    """
    Add missing Discord methods to logger to eliminate AttributeError warnings
    """
    import types

    # Safe get_discord_stats that works even if batcher not initialized
    def safe_get_discord_stats(self=None):
        try:
            if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
                return logger.discord_batcher.get_stats()
        except Exception:
            pass

        # Return safe defaults if batcher not available
        return {
            'total_reports_sent': 0,
            'unsent_logs_count': 0,
            'messages_sent_success': 0,
            'messages_sent_failed': 0,
            'current_hourly_rate': 0.0,
            'daily_quota_used': 0.0,
            'total_logs_buffered': 0,
            'batches_processed': 0
        }

    # Safe force send that handles missing batcher
    def safe_force_send_discord_report(self=None):
        try:
            if hasattr(logger, 'discord_batcher') and logger.discord_batcher:
                logger.discord_batcher.force_send_report()
                logger.info("Discord report forced successfully")
                return True
        except Exception as e:
            logger.debug(f"Force send failed: {e}")
        return False

    # Patch methods onto logger
    if not hasattr(logger, 'get_discord_stats'):
        logger.get_discord_stats = types.MethodType(safe_get_discord_stats, logger)
        logger.info("âœ… Patched logger with safe get_discord_stats")

    if not hasattr(logger, 'force_send_discord_report'):
        logger.force_send_discord_report = types.MethodType(safe_force_send_discord_report, logger)
        logger.info("âœ… Patched logger with safe force_send_discord_report")

    # Ensure discord_batcher exists (even if mock)
    if not hasattr(logger, 'discord_batcher'):
        class MockBatcher:
            is_running = False
            def start(self): logger.info("Mock Discord batcher (not initialized)")
            def stop(self): pass
            def get_stats(self): return safe_get_discord_stats()
            def force_send_report(self): pass

        logger.discord_batcher = MockBatcher()
        logger.info("âœ… Created mock discord_batcher for safety")

# ============================================================================
# STEP 2: QUANTUM-SPECIFIC DISCORD LOGGING
# ============================================================================

class QuantumDiscordLogger:
    """
    Enhanced Discord logger specifically for Quantum Trading System
    Logs quantum-specific metrics like entanglement, coordination, etc.
    """

    def __init__(self, webhook_url: str, batch_size: int = 50):
        self.webhook_url = webhook_url
        self.batch_size = batch_size

        # Quantum-specific log categories
        self.quantum_logs = deque(maxlen=500)
        self.training_logs = deque(maxlen=500)
        self.prediction_logs = deque(maxlen=500)
        self.error_logs = deque(maxlen=200)

        # Ensure base logger has Discord methods
        patch_logger_discord_methods()

        # Initialize Discord batcher if not exists
        if not hasattr(logger, 'discord_batcher') or not logger.discord_batcher.is_running:
            self._init_discord_batcher()

    def _init_discord_batcher(self):
        """Initialize Discord batcher properly"""
        try:

            logger.discord_batcher = DiscordLogBatcher(
                webhook_url=self.webhook_url,
                batch_size=self.batch_size
            )
            logger.discord_batcher.start()
            logger.info("âœ… Discord batcher initialized for quantum system")
        except Exception as e:
            logger.error(f"Failed to initialize Discord batcher: {e}")

    # ========================================================================
    # QUANTUM-SPECIFIC LOGGING METHODS
    # ========================================================================

    def log_quantum_training(self, agent_name: str, loss: float, metrics: Dict[str, float]):
        """Log quantum agent training progress"""
        try:
            log_entry = {
                'timestamp': time.time(),
                'type': 'quantum_training',
                'agent': agent_name,
                'loss': loss,
                'metrics': metrics
            }

            self.training_logs.append(log_entry)

            # Send to Discord if batch threshold reached
            if len(self.training_logs) >= self.batch_size:
                self._send_training_batch()

        except Exception as e:
            logger.error(f"Failed to log quantum training: {e}")

    def log_entanglement_metrics(self, entanglement_data: Dict[str, float]):
        """Log quantum entanglement metrics"""
        try:
            log_entry = {
                'timestamp': time.time(),
                'type': 'entanglement',
                'metrics': entanglement_data
            }

            self.quantum_logs.append(log_entry)

            # Important entanglement changes trigger immediate send
            if entanglement_data.get('mean', 0) > 0.8:
                self._send_quantum_batch(urgent=True)

        except Exception as e:
            logger.error(f"Failed to log entanglement: {e}")

    def log_quantum_prediction(self, agent_name: str, q_values: list, action: int,
                              confidence: float, coordination: float):
        """Log quantum prediction with coordination info"""
        try:
            log_entry = {
                'timestamp': time.time(),
                'type': 'quantum_prediction',
                'agent': agent_name,
                'q_values': q_values,
                'action': action,
                'confidence': confidence,
                'coordination': coordination
            }

            self.prediction_logs.append(log_entry)

            # Send predictions batch periodically
            if len(self.prediction_logs) >= self.batch_size * 2:
                self._send_prediction_batch()

        except Exception as e:
            logger.error(f"Failed to log quantum prediction: {e}")

    def log_quantum_error(self, error_type: str, message: str, context: Dict[str, Any]):
        """Log quantum system errors"""
        try:
            log_entry = {
                'timestamp': time.time(),
                'type': 'quantum_error',
                'error_type': error_type,
                'message': message,
                'context': context
            }

            self.error_logs.append(log_entry)

            # Errors trigger immediate send
            self._send_error_batch(urgent=True)

        except Exception as e:
            logger.error(f"Failed to log quantum error: {e}")

    # ========================================================================
    # BATCH SENDING METHODS
    # ========================================================================
# ======================= BATCH SENDING METHODS FIXED =======================

    def _send_training_batch(self):
        """Send recent training logs to Discord and clear sent logs."""
        if not self.training_logs:
            return

        try:
            recent_logs = list(self.training_logs)[-20:]

            embed = {
                "title": "ğŸ§  Quantum Training Update",
                "color": 0x00FF00,
                "fields": [],
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime()),
            }

            agent_stats = {}
            for log in recent_logs:
                agent = log.get("agent", "unknown")
                if agent not in agent_stats:
                    agent_stats[agent] = {"losses": [], "count": 0}
                if "loss" in log:
                    agent_stats[agent]["losses"].append(log["loss"])
                agent_stats[agent]["count"] += 1

            for agent, stats in agent_stats.items():
                avg_loss = sum(stats["losses"]) / len(stats["losses"]) if stats["losses"] else 0.0
                embed["fields"].append({
                    "name": f"Agent: {agent}",
                    "value": f"Training steps: {stats['count']}\nAvg Loss: {avg_loss:.4f}",
                    "inline": True
                })

            if hasattr(logger, "discord_batcher") and logger.discord_batcher:
                entry = LogEntry(
                    timestamp=time.time(),
                    level=LogLevel.INFO,
                    component="QUANTUM_TRAINING",
                    message="Quantum training batch update",
                    metadata={"embed": embed},
                )
                logger.discord_batcher.add_log(entry)

            # âœ… Remove sent logs
            for _ in range(len(recent_logs)):
                self.training_logs.popleft()

        except Exception as e:
            logger.error(f"Failed to send training batch: {e}")

    def _send_quantum_batch(self, urgent: bool = False):
        """Send batch of quantum metrics and clear sent logs."""
        if not self.quantum_logs:
            return

        try:
            recent_logs = list(self.quantum_logs)[-10:]
            latest = recent_logs[-1]
            metrics = latest.get("metrics", {})

            embed = {
                "title": "âš›ï¸ Quantum Entanglement Status",
                "color": 0x9B59B6,
                "fields": [
                    {"name": "Mean Entanglement", "value": f"{metrics.get('mean',0):.4f}", "inline": True},
                    {"name": "Std Dev", "value": f"{metrics.get('std',0):.4f}", "inline": True},
                    {"name": "Current", "value": f"{metrics.get('current',0):.4f}", "inline": True}
                ],
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime())
            }

            if urgent:
                embed["title"] = "ğŸš¨ " + embed["title"]
                embed["color"] = 0xFF0000

            if hasattr(logger, "discord_batcher") and logger.discord_batcher:
                entry = LogEntry(
                    timestamp=time.time(),
                    level=LogLevel.CRITICAL if urgent else LogLevel.INFO,
                    component="QUANTUM_ENTANGLEMENT",
                    message="Quantum entanglement update",
                    metadata={"embed": embed},
                )
                logger.discord_batcher.add_log(entry)

            # âœ… Remove sent logs
            for _ in range(len(recent_logs)):
                self.quantum_logs.popleft()

        except Exception as e:
            logger.error(f"Failed to send quantum batch: {e}")

    def _send_prediction_batch(self):
        """Send batch of predictions to Discord and clear sent logs."""
        if not self.prediction_logs:
            return

        try:
            recent_logs = list(self.prediction_logs)[-30:]

            avg_coordination = sum(log.get("coordination",0) for log in recent_logs) / len(recent_logs)
            avg_confidence = sum(log.get("confidence",0) for log in recent_logs) / len(recent_logs)

            embed = {
                "title": "ğŸ¯ Quantum Predictions Summary",
                "color": 0x3498DB,
                "fields": [
                    {"name": "Predictions Made", "value": str(len(recent_logs)), "inline": True},
                    {"name": "Avg Coordination", "value": f"{avg_coordination:.3f}", "inline": True},
                    {"name": "Avg Confidence", "value": f"{avg_confidence:.3f}", "inline": True}
                ],
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime())
            }

            if hasattr(logger, "discord_batcher") and logger.discord_batcher:
                entry = LogEntry(
                    timestamp=time.time(),
                    level=LogLevel.INFO,
                    component="QUANTUM_PREDICTIONS",
                    message="Quantum predictions summary",
                    metadata={"embed": embed},
                )
                logger.discord_batcher.add_log(entry)

            # âœ… Remove sent logs
            for _ in range(len(recent_logs)):
                self.prediction_logs.popleft()

        except Exception as e:
            logger.error(f"Failed to send prediction batch: {e}")

    def _send_error_batch(self, urgent: bool = False):
        """Send batch of errors to Discord and clear sent logs."""
        if not self.error_logs:
            return

        try:
            recent_errors = list(self.error_logs)[-5:]

            embed = {
                "title": "âŒ Quantum System Errors",
                "color": 0xE74C3C,
                "fields": [],
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z", time.gmtime())
            }

            for err in recent_errors:
                embed["fields"].append({
                    "name": err.get("error_type", "Unknown"),
                    "value": err.get("message", "No message")[:100],
                    "inline": False
                })

            if hasattr(logger, "discord_batcher") and logger.discord_batcher:
                entry = LogEntry(
                    timestamp=time.time(),
                    level=LogLevel.CRITICAL if urgent else LogLevel.ERROR,
                    component="QUANTUM_ERROR",
                    message="Quantum system errors",
                    metadata={"embed": embed},
                )
                logger.discord_batcher.add_log(entry)

            # âœ… Remove sent logs
            for _ in range(len(recent_errors)):
                self.error_logs.popleft()

        except Exception as e:
            logger.error(f"Failed to send error batch: {e}")

# ============================================================================
# STEP 3: INTEGRATE WITH QUANTUM SYSTEM
# ============================================================================

def integrate_quantum_discord_logging(system, webhook_url: str):
    """
    Integrate quantum Discord logging with existing system

    Usage:
        system = IntegratedSignalSystem(...)
        integrate_quantum_discord_logging(system, WEBHOOK_URL)
    """

    # Patch logger first
    patch_logger_discord_methods()

    # Create quantum logger
    quantum_logger = QuantumDiscordLogger(webhook_url=webhook_url, batch_size=50)

    # Attach to system
    system.quantum_discord_logger = quantum_logger

    # Hook into quantum bridge predictions
    if hasattr(system, 'quantum_bridge'):
        original_predict = system.safe_quantum_predict

        def logged_predict(agent_name, add_noise=False):
            try:
                q_values = original_predict(agent_name, add_noise)

                # Get additional metrics if available
                agent = system.agents.get(agent_name)
                if agent and hasattr(agent, 'quantum_bridge'):
                    coordination = 0.0  # Calculate from agent if available

                    quantum_logger.log_quantum_prediction(
                        agent_name=agent_name,
                        q_values=q_values.tolist() if hasattr(q_values, 'tolist') else q_values,
                        action=int(q_values.argmax()) if len(q_values) > 0 else 0,
                        confidence=float(q_values.max()) if len(q_values) > 0 else 0.0,
                        coordination=coordination
                    )

                return q_values
            except Exception as e:
                quantum_logger.log_quantum_error(
                    error_type='prediction_error',
                    message=str(e),
                    context={'agent': agent_name}
                )
                raise

        system.quantum_bridge.safe_quantum_predict = logged_predict

    # Hook into quantum system training
    if hasattr(system, 'quantum_bridge') and hasattr(system.quantum_bridge, 'quantum_trainer'):
        trainer_instance = system.quantum_bridge.quantum_trainer
        original_train = trainer_instance.train_step

        # V8.5.8 FIX: Wrapper must NOT accept 'self' - it's assigned to instance, not class
        def logged_train_wrapper(batch_size=None):
            """Wrapper for train_step that logs to Discord"""
            try:
                # Call the original method (already bound to trainer_instance)
                result = original_train(batch_size=batch_size)

                if result:
                    # Log training metrics
                    for agent_name in system.agents.keys():
                        quantum_logger.log_quantum_training(
                            agent_name=agent_name,
                            loss=result.get('total_loss', 0),
                            metrics=result
                        )

                return result
            except Exception as e:
                quantum_logger.log_quantum_error(
                    error_type='training_error',
                    message=str(e),
                    context={'step': 'train_step'}
                )
                raise

        # Replace the method directly
        trainer_instance.train_step = logged_train_wrapper

    # Hook into entanglement metrics
    if hasattr(system, 'quantum_bridge'):
        def log_entanglement_periodically():
            while True:
                time.sleep(300)  # Every 5 minutes
                try:
                    metrics = system.quantum_bridge.get_system_metrics()
                    if 'entanglement' in metrics:
                        quantum_logger.log_entanglement_metrics(metrics['entanglement'])
                except Exception as e:
                    logger.debug(f"Entanglement logging error: {e}")

        threading.Thread(target=log_entanglement_periodically, daemon=True).start()

    logger.info("âœ… Quantum Discord logging integrated")
    return quantum_logger

# ============================================================================
# STEP 4: FIX DISCORD MONITOR (Eliminate Warnings)
# ============================================================================

def start_safe_discord_monitor(system, interval=300):
    """
    FIXED: Discord monitoring that won't throw AttributeError
    """
    def monitor():
        while True:
            time.sleep(interval)
            try:
                # Safely get stats (won't fail if discord_batcher not initialized)
                stats = logger.get_discord_stats()

                logger.info(
                    f"Discord Stats: {stats['messages_sent_success']} sent, "
                    f"{stats['messages_sent_failed']} failed, "
                    f"unsent: {stats['unsent_logs_count']}, "
                    f"queue: {stats.get('total_logs_buffered', 0)}"
                )

                # Check for issues
                if stats['messages_sent_failed'] > 10:
                    logger.warning(f"High Discord failure rate: {stats['messages_sent_failed']} failed")

                if stats['unsent_logs_count'] > 100:
                    logger.warning(f"Discord queue backing up: {stats['unsent_logs_count']} unsent")
                    # Try to force send
                    logger.force_send_discord_report()

            except Exception as e:
                logger.debug(f"Discord monitor error (non-critical): {e}")

    threading.Thread(target=monitor, daemon=True).start()
    logger.info("âœ… Safe Discord monitoring started")

# ============================================================================
# STEP 5: USAGE IN MAIN
# ============================================================================

def setup_quantum_discord_integration(system):
    """
    Complete setup for quantum Discord integration
    Call this in your quantum_integrated_main() function
    """

    WEBHOOK_URL = "https://discordapp.com/api/webhooks/1422597824851345489/bmJgtiL_jyjW1XTBErBrlrtMF9atVnX7CzwIUVOhrbd2hiPtklD6sZJpk8KqLNlCyIGN"

    # Step 1: Patch logger (fixes AttributeError)
    patch_logger_discord_methods()
    logger.info("âœ… Discord logger patched")

    # Step 2: Integrate quantum-specific logging
    quantum_logger = integrate_quantum_discord_logging(system, WEBHOOK_URL)
    logger.info("âœ… Quantum Discord logger created")

    # Step 3: Start safe monitoring (no more warnings)
    start_safe_discord_monitor(system, interval=300)
    logger.info("âœ… Safe Discord monitoring started")

    return quantum_logger

FEATURE_WINDOW = 10
REPLAY_BUFFER_MAXLEN = 200000
BATCH_SIZE = 64
N_STEP = 3
GAMMA = 0.98
TRAINING_EPOCHS = 6
TARGET_UPDATE_FREQ = 5

# === PASTE THIS - REPLACE EXISTING ACTION_MAP AND ACTION_REVERSE ===
ACTION_MAP = {0: "BUY", 1: "SELL"}
ACTION_REVERSE = {"BUY": 0, "SELL": 1}
ACTION_DIM = 2

# Ensure state dim matches your features
STATE_DIM = 58

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state'])
GOOGLE_DRIVE_BASE_PATH = "/content/drive/MyDrive/RL_Data"

def ensure_dir(path):
    os.makedirs(path, exist_ok=True)

# UTILITY 2: Batch training enabler for agents
def enable_batch_training(agent):
    """Enable batch training mode for an agent"""
    agent._batch_training_active = True

def disable_batch_training(agent):
    """Disable batch training mode for an agent"""
    agent._batch_training_active = False

# UTILITY 3: Agent batch training wrapper used by batch processor
def batch_train_agent(agent, num_rounds=1):
    """Wrapper for batch training an agent multiple times efficiently"""
    try:
        enable_batch_training(agent)
        training_steps = 0

        for round_num in range(num_rounds):
            agent.train()
            training_steps += 1

        disable_batch_training(agent)
        return training_steps

    except Exception as e:
        logger.error(f"Batch training failed for agent {agent.name}: {e}")
        disable_batch_training(agent)
        return 2

# UTILITY 4: Integration validation function
def validate_batch_integration(system):
    """Validate that batch processing integration is working correctly"""
    checks = {
        'batch_processor_exists': hasattr(system, 'batch_processor') and system.batch_processor is not None,
        'batch_processing_enabled': getattr(system, 'batch_processing_enabled', False),
        'ably_connected': system.ably is not None,
        'agents_loaded': len(system.agents) > 0,
        'async_locks_ready': hasattr(system, 'processing_lock') and system.processing_lock is not None
    }

    all_passed = all(checks.values())

    logger.info("=== BATCH INTEGRATION VALIDATION ===")
    for check, passed in checks.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        logger.info(f"{status} {check.replace('_', ' ').title()}")

    if all_passed:
        logger.info("ğŸ¯ ALL INTEGRATION CHECKS PASSED - SYSTEM READY")
    else:
        logger.info("âš ï¸ SOME INTEGRATION CHECKS FAILED - REVIEW CONFIGURATION")

    return all_passed

def enforce_meta_model_input_shape(batch, expected_dim=30):
    """
    Defensive padding/truncation for meta-model batch input.
    Returns a batch of shape (N, expected_dim). If batch is empty, returns a dummy batch.    """
    if len(batch) == 0:
        logger.error("Empty batch passed for meta-model input shape enforcement. Returning dummy batch.")
        return np.zeros((1, expected_dim), dtype=np.float32)
    processed = []
    for i, sample in enumerate(batch):
        arr = np.asarray(sample, dtype=np.float32).flatten()
        if np.any(np.isnan(arr)):
            arr = np.nan_to_num(arr, nan=0.0)
        if arr.shape[0] < expected_dim:
            arr = np.pad(arr, (0, expected_dim - arr.shape[0]), mode='constant')
        elif arr.shape[0] > expected_dim:
            arr = arr[:expected_dim]
        # Defensive shape logging
        if arr.shape[0] != expected_dim:
            logger.error(f"Sample {i} has wrong shape after padding/truncating: {arr.shape}")
            continue
        processed.append(arr)
    result = np.array(processed, dtype=np.float32)
    if result.shape[1] != expected_dim:
        logger.error(f"Batch after processing has wrong shape: {result.shape}. Returning dummy batch.")
        return np.zeros((result.shape[0], expected_dim), dtype=np.float32)
    return result

# ... class MetaModelTrainer, etc. ...

def setup_gcs(credential_path, bucket_name):
    """
    Set up GCS client and bucket using the given credentials and bucket name.
    """
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credential_path
    from google.cloud import storage
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    return client, bucket

# --- File Utilities ---

def ensure_dir(path):
    """
    Ensure a directory exists.
    """
    os.makedirs(path, exist_ok=True)

def compress_dir_to_zip(src_dir, zip_path):
    """
    Compress a directory to a zip file.
    """
    shutil.make_archive(zip_path.replace('.zip', ''), 'zip', src_dir)
    return zip_path

def safe_unzip(zip_path, dest_dir):
    """
    Unzips zip_path into dest_dir after cleaning dest_dir.
    Defensive: remove all files/dirs in dest_dir before extracting.
    """
    if os.path.exists(dest_dir):
        for f in os.listdir(dest_dir):
            fp = os.path.join(dest_dir, f)
            try:
                if os.path.isfile(fp) or os.path.islink(fp):
                    os.unlink(fp)
                elif os.path.isdir(fp):
                    shutil.rmtree(fp)
            except Exception as e:
                logger.warning(f"Failed to clean {fp} before unzip: {e}")
    ensure_dir(dest_dir)
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(dest_dir)

def decompress_zip_to_dir(zip_path, dest_dir):
    """
    Alias for safe_unzip for compatibility.
    """
    safe_unzip(zip_path, dest_dir)

def upload_zip_to_gcs(bucket, local_zip_path, gcs_blob_path):
    """
    Upload a zip file to GCS.
    """
    blob = bucket.blob(gcs_blob_path)
    blob.upload_from_filename(local_zip_path)
    logger.info(f"âœ… Uploaded {local_zip_path} to gs://{bucket.name}/{gcs_blob_path}")

def download_zip_from_gcs(bucket, gcs_blob_path, local_zip_path):
    """
    Download a zip file from GCS.
    """
    blob = bucket.blob(gcs_blob_path)
    blob.download_to_filename(local_zip_path)
    logger.info(f"âœ… Downloaded {gcs_blob_path} to {local_zip_path}")

def list_gcs_files(client, bucket_name, prefix=""):
    """
    List files in a GCS bucket with the given prefix.
    """
    blobs = client.list_blobs(bucket_name, prefix=prefix)
    return [blob.name for blob in blobs]

def unique_tmp_path(prefix):
    """
    Returns a unique tmp file path using UUID.
    """
    return os.path.join("/tmp", f"{prefix}_{uuid.uuid4().hex}.zip")

def save_agent_to_gcs(agent, bucket, gcs_dir):
    agent.save_state()  # Saves locally
    local_dir = agent.local_save_dir
    zip_path = f"/tmp/{agent.name}_agent_state.zip"
    compress_dir_to_zip(local_dir, zip_path)
    gcs_blob_path = f"{gcs_dir}/{agent.name}_agent_state.zip"
    upload_zip_to_gcs(bucket, zip_path, gcs_blob_path)

def save_meta_to_gcs(system, local_dir, bucket, gcs_dir):
    # Save meta-model and meta-data locally
    system.save_state()
    # Compress local_dir to zip
    zip_path = "/tmp/IntegratedSignalSystem_state.zip"
    compress_dir_to_zip(local_dir, zip_path)
    # Upload zip to GCS
    gcs_blob_path = f"{gcs_dir}/IntegratedSignalSystem_state.zip"
    upload_zip_to_gcs(bucket, zip_path, gcs_blob_path)

def load_scaler(path):
    try:
        if os.path.exists(path):
            scaler = joblib.load(path)
            logger.info(f"Scaler loaded: {path}")
            return scaler, True
        else:
            logger.warning(f"Scaler not found: {path}")
            return None, False
    except Exception as e:
        logger.error(f"Error loading scaler {path}: {e}")
        return None, False

def load_torch_state_dict(model, path, device):
    try:
        if os.path.exists(path):
            model.load_state_dict(torch.load(path, map_location=device))
            logger.info(f"Model loaded: {path}")
            return True
        else:
            logger.warning(f"Model state not found: {path}")
            return False
    except Exception as e:
        logger.error(f"Error loading model {path}: {e}")
        return False

def load_optimizer_state_dict(optimizer, path, device):
    try:
        if os.path.exists(path):
            optimizer.load_state_dict(torch.load(path, map_location=device))
            logger.info(f"Optimizer loaded: {path}")
            return True
        else:
            logger.warning(f"Optimizer state not found: {path}")
            return False
    except Exception as e:
        logger.error(f"Error loading optimizer {path}: {e}")
        return False

def load_pickle_buffer(buffer, path):
    try:
        if os.path.exists(path):
            with open(path, "rb") as f:
                data = pickle.load(f)
            mem_cntr = data.get('mem_cntr', 0)
            buffer.mem_cntr = mem_cntr
            for key in ['state_memory', 'action_memory', 'reward_memory', 'next_state_memory', 'terminal_memory']:
                if key in data and hasattr(buffer, key):
                    src = np.array(data[key])
                    dst = getattr(buffer, key)
                    src = src[:mem_cntr] if hasattr(src, '__len__') else src
                    if hasattr(dst, '__setitem__'):
                        dst[:mem_cntr] = src
            logger.info(f"Replay buffer loaded: {path}")
            return True
        else:
            logger.warning(f"Replay buffer not found: {path}")
            return False
    except Exception as e:
        logger.error(f"Error loading replay buffer {path}: {e}")
        return False

def load_keras_model(path):
    try:

        if os.path.exists(path):
            model = tf.keras.models.load_model(path)
            logger.info(f"Keras model loaded: {path}")
            return model, True
        else:
            logger.warning(f"Keras model not found: {path}")
            return None, False
    except Exception as e:
        logger.error(f"Error loading keras model {path}: {e}")
        return None, False

# --- Agent Save/Load ---

def save_agent(agent, path):
    """
    Save all agent components to the given directory.
    """
    ensure_dir(path)
    joblib.dump(agent.scaler, os.path.join(path, "scaler.pkl"))
    torch.save(agent.actor.state_dict(), os.path.join(path, "actor.pth"))
    torch.save(agent.critic.state_dict(), os.path.join(path, "critic.pth"))
    torch.save(agent.target_actor.state_dict(), os.path.join(path, "target_actor.pth"))
    torch.save(agent.target_critic.state_dict(), os.path.join(path, "target_critic.pth"))
    torch.save(agent.actor_optimizer.state_dict(), os.path.join(path, "actor_optimizer.pth"))
    torch.save(agent.critic_optimizer.state_dict(), os.path.join(path, "critic_optimizer.pth"))
    with open(os.path.join(path, "replay_buffer.pkl"), "wb") as f:
        pickle.dump({
            'mem_cntr': agent.replay_buffer.mem_cntr,
            'state_memory': agent.replay_buffer.state_memory,
            'action_memory': agent.replay_buffer.action_memory,
            'reward_memory': agent.replay_buffer.reward_memory,
            'next_state_memory': getattr(agent.replay_buffer, 'next_state_memory', None),
            'terminal_memory': agent.replay_buffer.terminal_memory,
        }, f)
    with open(os.path.join(path, "train_step_counter.txt"), "w") as f:
        f.write(str(getattr(agent, "train_step_counter", 0)))
    logger.info(f"[{agent.name}] âœ… Agent state saved to {path}")

def load_agent(agent, save_path):
    """
    Defensive load of all agent components.
    Returns True if any component is loaded, otherwise False.
    """
    ensure_dir(save_path)
    scaler, scaler_loaded = load_scaler(os.path.join(save_path, "scaler.pkl"))
    loaded_components = []
    if scaler_loaded:
        agent.scaler = scaler
        agent.scaler_fitted = True
        loaded_components.append("scaler")
    device = getattr(agent, "device", "cpu")
    if load_torch_state_dict(agent.actor, os.path.join(save_path, "actor.pth"), device):
        loaded_components.append("actor")
    if load_torch_state_dict(agent.critic, os.path.join(save_path, "critic.pth"), device):
        loaded_components.append("critic")
    if load_torch_state_dict(agent.target_actor, os.path.join(save_path, "target_actor.pth"), device):
        loaded_components.append("target_actor")
    if load_torch_state_dict(agent.target_critic, os.path.join(save_path, "target_critic.pth"), device):
        loaded_components.append("target_critic")
    if load_optimizer_state_dict(agent.actor_optimizer, os.path.join(save_path, "actor_optimizer.pth"), device):
        loaded_components.append("actor_optimizer")
    if load_optimizer_state_dict(agent.critic_optimizer, os.path.join(save_path, "critic_optimizer.pth"), device):
        loaded_components.append("critic_optimizer")
    if load_pickle_buffer(agent.replay_buffer, os.path.join(save_path, "replay_buffer.pkl")):
        loaded_components.append("replay_buffer")
    train_step_path = os.path.join(save_path, "train_step_counter.txt")
    if os.path.exists(train_step_path):
        with open(train_step_path, "r") as f:
            agent.train_step_counter = int(f.read())
        loaded_components.append("train_step_counter")
    else:
        agent.train_step_counter = 0

    if loaded_components:
        logger.info(f"[{agent.name}] Loaded components: {', '.join(loaded_components)}")
        return True
    else:
        logger.warning(f"[{agent.name}] No components loaded from {save_path}")
        return False

# --- Meta Save/Load ---
def save_meta(system, path):
    ensure_dir(path)
    try:
        # Only save meta_model if it is trained
        # Use joblib for scikit-learn, tf.keras.models.save for keras
        if hasattr(system.meta_model, "fit"):  # scikit-learn
            from sklearn.utils.validation import check_is_fitted
            check_is_fitted(system.meta_model)
            with open(os.path.join(path, "meta_model.pkl"), "wb") as f:
                pickle.dump(system.meta_model, f)
            logger.info("âœ… Meta-model saved")
        else:  # keras
            system.meta_model.save(os.path.join(path, "meta_model.keras"))
            logger.info("âœ… Meta-model saved")
    except Exception as e:
        logger.warning(f"âš ï¸ Meta-model not saved (possibly not trained): {e}")

    # Always save meta_data, even if model isn't ready
    try:
        with open(os.path.join(path, "meta_data.pkl"), "wb") as f:
            pickle.dump(system.meta_data, f)
        logger.info("âœ… Meta-data saved")
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to save meta-data: {e}")

def load_meta(system, meta_path):
    ensure_dir(meta_path)
    meta_model_path = os.path.join(meta_path, "meta_model.keras")
    meta_data_path = os.path.join(meta_path, "meta_data.pkl")
    loaded_components = []

    # Try Keras first
    meta_model, meta_loaded = load_keras_model(meta_model_path)
    if meta_loaded:
        system.meta_model = meta_model
        system.meta_model_trained = True
        loaded_components.append("meta_model")
    elif os.path.exists(os.path.join(meta_path, "meta_model.pkl")):
        # Try scikit-learn
        with open(os.path.join(meta_path, "meta_model.pkl"), "rb") as f:
            system.meta_model = pickle.load(f)
        system.meta_model_trained = True
        loaded_components.append("meta_model")
    else:
        # Save default if not found
        if hasattr(system.meta_model, "save"):
            system.meta_model.save(meta_model_path)
        elif hasattr(system.meta_model, "fit"):
            with open(os.path.join(meta_path, "meta_model.pkl"), "wb") as f:
                pickle.dump(system.meta_model, f)
        logger.warning("âš ï¸ Meta-model not found. Default one created.")

    # Load meta-data
    if os.path.exists(meta_data_path):
        with open(meta_data_path, "rb") as f:
            system.meta_data = pickle.load(f)
        loaded_components.append("meta_data")
        logger.info("âœ… Meta-data loaded.")
    else:
        with open(meta_data_path, "wb") as f:
            pickle.dump(system.meta_data, f)
        logger.warning("âš ï¸ Meta-data not found. Default one created.")

    if loaded_components:
        logger.info(f"Meta loaded components: {', '.join(loaded_components)}")
        return True
    else:
        logger.warning(f"No meta components loaded from {meta_path}")
        return False

class AutosaveManager:
    def __init__(self, agents, system, bucket, agent_gcs_dir, system_gcs_dir, interval=1800):
        self.agents = agents
        self.system = system
        self.bucket = bucket
        self.agent_gcs_dir = agent_gcs_dir
        self.system_gcs_dir = system_gcs_dir
        self.interval = interval
        self.lock = threading.Lock()
        self.stop_event = threading.Event()
        logger.info("AutosaveManager initialized (GCS enabled).")

    def save_agent_to_gcs(self, agent, bucket, gcs_dir):
        # Save locally using agent's own method
        agent.save_state()
        local_dir = agent.local_save_dir
        # Ensure local directory exists
        if not os.path.exists(local_dir):
            logger.error(f"Local agent state dir does not exist: {local_dir}")
            return
        zip_path = f"/tmp/{agent.name}_agent_state.zip"
        compress_dir_to_zip(local_dir, zip_path)
        gcs_blob_path = f"{gcs_dir}/{agent.name}_agent_state.zip"
        upload_zip_to_gcs(bucket, zip_path, gcs_blob_path)

    def save_meta_to_gcs(self, system, bucket, gcs_dir):
        # Save locally using system's own method
        system.save_state()
        local_dir = system.base_path
        # Ensure local directory exists
        if not os.path.exists(local_dir):
            logger.error(f"Local system state dir does not exist: {local_dir}")
            return
        zip_path = "/tmp/IntegratedSignalSystem_state.zip"
        compress_dir_to_zip(local_dir, zip_path)
        gcs_blob_path = f"{gcs_dir}/IntegratedSignalSystem_state.zip"
        upload_zip_to_gcs(bucket, zip_path, gcs_blob_path)

    def save_all(self):
        with self.lock:
            for agent in self.agents.values():
                try:
                    self.save_agent_to_gcs(
                        agent, self.bucket, self.agent_gcs_dir
                    )
                    logger.info(f"âœ… Agent {agent.name} state saved to GCS by AutosaveManager")
                except Exception as e:
                    logger.error(f"âŒ Failed to autosave agent {agent.name} to GCS: {e}")
            try:
                self.save_meta_to_gcs(
                    self.system, self.bucket, self.system_gcs_dir
                )
                logger.info("âœ… System state saved to GCS by AutosaveManager")
            except Exception as e:
                logger.error(f"âŒ Failed to autosave system state to GCS: {e}")

    def run(self):
        logger.info("ğŸš€ AutosaveManager started")
        while not self.stop_event.is_set():
            try:
                self.save_all()
            except Exception as e:
                logger.error(f"Autosave error: {e}")
            self.stop_event.wait(self.interval)
        logger.info("ğŸ›‘ AutosaveManager stopped")

    def stop(self):
        self.stop_event.set()
        logger.info("AutosaveManager stop signal received.")
# ============================================================================
# THREE-TIER GPU PROCESSING ARCHITECTURE FOR TRADING SYSTEM
# Paste this AFTER the existing imports section in your main file
# ============================================================================

# ============================================================================
# Enhanced Data Structures with Trading System Integration
# ============================================================================

@dataclass
class EnhancedFeatureMessage:
    """Enhanced feature message with trading system metadata"""
    agent_name: str
    features: Dict[str, float]
    timestamp: float
    message_id: str
    state_sequence: Optional[np.ndarray] = None
    trading_context: Optional[Dict[str, Any]] = None
    priority: int = 3  # 1=urgent, 5=low

@dataclass
class TradingInferenceRequest:
    """Enhanced inference request with trading-specific data"""
    agent_name: str
    features: Dict[str, float]
    state_sequence: np.ndarray
    request_id: str
    timestamp: float
    agent_instance: Any = None  # Reference to actual agent
    trading_metadata: Optional[Dict[str, Any]] = None

@dataclass
class TradingInferenceResult:
    """Enhanced inference result with trading outputs"""
    agent_name: str
    q_values: np.ndarray
    action: int
    request_id: str
    processing_time: float
    confidence: float = 0.0
    agent_state: Optional[Dict[str, Any]] = None
    trading_signals: Optional[Dict[str, Any]] = None

# ============================================================================
# TIER 1: Enhanced Ingestion Layer (Message Processing)
# ============================================================================

class EnhancedIngestionTier:
    """Enhanced ingestion tier with feature message and reward processing"""

    def __init__(self, num_workers: int = 4, buffer_size: int = 10000):
        self.num_workers = num_workers
        self.buffer_size = buffer_size
        self.running = False

        # Work queues for different message types
        self.feature_queue = queue.Queue(maxsize=buffer_size)
        self.reward_queue = queue.Queue(maxsize=buffer_size)
        self.output_queue = queue.Queue(maxsize=buffer_size * 2)

        # Processing workers
        self.workers = []
        self.executor = ThreadPoolExecutor(max_workers=num_workers)

        # Statistics tracking
        self.stats = {
            'messages_received': 0,
            'features_processed': 0,
            'rewards_processed': 0,
            'processing_errors': 0,
            'avg_processing_time': 0.0,
            'queue_depth': 0
        }

    def start(self):
        """Start the ingestion tier workers"""
        self.running = True

        # Start feature processing workers
        for i in range(self.num_workers):
            worker = threading.Thread(
                target=self._feature_worker,
                args=(i,),
                daemon=True
            )
            worker.start()
            self.workers.append(worker)

        # Start reward processing worker
        reward_worker = threading.Thread(
            target=self._reward_worker,
            daemon=True
        )
        reward_worker.start()
        self.workers.append(reward_worker)

        logger.info(f"Enhanced ingestion tier started with {self.num_workers} workers")

    def submit_feature_message(self, agent_name: str, features: Dict[str, float],
                             priority: int = 3) -> bool:
        """Submit feature message for processing"""
        try:
            message = EnhancedFeatureMessage(
                agent_name=agent_name,
                features=features,
                timestamp=time.time(),
                message_id=f"{agent_name}_{int(time.time() * 1e6)}",
                priority=priority
            )

            self.feature_queue.put(message, timeout=0.1)
            self.stats['messages_received'] += 1
            return True

        except queue.Full:
            logger.warning(f"Feature queue full, dropping message from {agent_name}")
            return False

    def submit_reward_message(self, reward_data: Dict[str, Any]) -> bool:
        """Submit reward message for processing"""
        try:
            self.reward_queue.put(reward_data, timeout=0.1)
            self.stats['messages_received'] += 1
            return True

        except queue.Full:
            logger.warning("Reward queue full, dropping reward message")
            return False

    def _feature_worker(self, worker_id: int):
        """Feature message processing worker"""
        logger.info(f"Feature worker {worker_id} started")

        while self.running:
            try:
                # Get feature message with timeout
                try:
                    message = self.feature_queue.get(timeout=1.0)
                except queue.Empty:
                    continue

                if message is None:  # Poison pill
                    break

                # Process feature message
                start_time = time.time()
                processed_message = self._process_feature_message(message)
                processing_time = time.time() - start_time

                # Send to output queue for GPU tier
                if processed_message:
                    try:
                        self.output_queue.put(processed_message, timeout=0.1)
                        self.stats['features_processed'] += 1

                        # Update average processing time
                        alpha = 0.1
                        self.stats['avg_processing_time'] = (
                            alpha * processing_time +
                            (1 - alpha) * self.stats['avg_processing_time']
                        )

                    except queue.Full:
                        logger.warning("Output queue full, dropping processed message")

            except Exception as e:
                logger.error(f"Feature worker {worker_id} error: {e}")
                self.stats['processing_errors'] += 1

    def _reward_worker(self):
        """Reward message processing worker"""
        logger.info("Reward worker started")

        while self.running:
            try:
                # Get reward message with timeout
                try:
                    reward_msg = self.reward_queue.get(timeout=1.0)
                except queue.Empty:
                    continue

                if reward_msg is None:  # Poison pill
                    break

                # Process reward message
                processed_reward = self._process_reward_message(reward_msg)

                if processed_reward:
                    try:
                        self.output_queue.put(processed_reward, timeout=0.1)
                        self.stats['rewards_processed'] += 1
                    except queue.Full:
                        logger.warning("Output queue full, dropping processed reward")

            except Exception as e:
                logger.error(f"Reward worker error: {e}")
                self.stats['processing_errors'] += 1

    def _process_feature_message(self, message: EnhancedFeatureMessage) -> Optional[Dict[str, Any]]:
        """Process feature message for GPU tier consumption"""
        try:
            # Extract and validate features
            if not message.features:
                logger.warning(f"Empty features for agent {message.agent_name}")
                return None

            # Prepare for GPU processing
            processed = {
                'type': 'feature_inference',
                'agent_name': message.agent_name,
                'features': message.features,
                'timestamp': message.timestamp,
                'message_id': message.message_id,
                'priority': message.priority
            }

            logger.debug(f"Processed feature message for {message.agent_name}")
            return processed

        except Exception as e:
            logger.error(f"Feature message processing failed: {e}")
            return None

    def _process_reward_message(self, reward_msg: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Parse, validate, and handle incoming reward messages"""
        try:
            # Validate reward message
            required_fields = ['signal_key', 'reward']
            if not all(field in reward_msg for field in required_fields):
                logger.warning("[RewardProcessor] Invalid reward message structure")
                return None

            signal_key = reward_msg['signal_key']
            reward_value = reward_msg['reward']
            exit_price = reward_msg.get('exit_price')

            processed = {
                'type': 'reward_update',
                'signal_key': signal_key,
                'reward': float(reward_value),
                'exit_price': float(exit_price) if exit_price is not None else None,
                'timestamp': time.time(),
                'agent_multipliers': reward_msg.get('agent_multipliers', {})
            }

            # âœ… Complete experience and queue for training
            handle_reward(self, signal_key, processed)

            logger.debug(f"[RewardProcessor] Processed reward for {signal_key}")
            return processed

        except Exception as e:
            logger.error(f"[RewardProcessor] Processing failed: {e}")
            return None

    def get_processed_message(self, timeout: float = 0.1) -> Optional[Dict[str, Any]]:
        """Get processed message from output queue"""
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def stop(self):
        """Stop the ingestion tier"""
        self.running = False

        # Send poison pills to stop workers
        for _ in range(self.num_workers):
            try:
                self.feature_queue.put(None, timeout=1.0)
            except queue.Full:
                pass

        try:
            self.reward_queue.put(None, timeout=1.0)
        except queue.Full:
            pass

        # Wait for workers to finish
        for worker in self.workers:
            worker.join(timeout=2.0)

        self.executor.shutdown(wait=True)
        logger.info("Enhanced ingestion tier stopped")

    def get_stats(self) -> Dict[str, Any]:
        """Get ingestion tier statistics"""
        self.stats['queue_depth'] = (
            self.feature_queue.qsize() +
            self.reward_queue.qsize() +
            self.output_queue.qsize()
        )
        return self.stats.copy()

# ============================================================================
# TIER 2: Enhanced GPU Processing Tier with Real Agent Integration
# ============================================================================

class EnhancedGPUProcessingTier:
    """GPU processing tier integrated with actual trading agents"""

    def __init__(self, num_threads: int = 8, batch_size: int = 64, agents_registry: Dict[str, Any] = None):
        self.num_threads = num_threads
        self.batch_size = batch_size
        self.running = False
        self.agents_registry = agents_registry or {}

        # Enhanced work queues with priority support
        self.inference_queues = [queue.PriorityQueue(maxsize=1000) for _ in range(num_threads)]
        self.result_queue = queue.Queue(maxsize=5000)

        # GPU resources with memory management
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.cuda_streams = [torch.cuda.Stream() for _ in range(num_threads)] if torch.cuda.is_available() else [None] * num_threads

        # Agent-specific memory pools
        self.agent_memory_pools = {}
        self.model_cache = {}  # Cache for loaded models

        # Enhanced statistics
        self.stats = {
            'batches_processed': 0,
            'total_inferences': 0,
            'avg_batch_time': 0.0,
            'gpu_utilization': 0.0,
            'queue_depth': 0,
            'agent_specific_stats': {},
            'cache_hits': 0,
            'cache_misses': 0
        }

        self.threads = []
        self.request_counter = 0

        # Initialize agent-specific resources
        self._init_agent_resources()

    def _init_agent_resources(self):
        """Initialize GPU resources for each agent"""
        if not torch.cuda.is_available():
            return

        for agent_name, agent in self.agents_registry.items():
            try:
                # Pre-allocate memory for this agent's typical batch size
                seq_len = getattr(agent, 'seq_len', 20)
                state_dim = getattr(agent, 'state_dim', 34)

                memory_pool = {
                    'states': torch.zeros(self.batch_size, seq_len, state_dim, device=self.device, dtype=torch.float32),
                    'q_values': torch.zeros(self.batch_size, 2, device=self.device, dtype=torch.float32),
                    'actions': torch.zeros(self.batch_size, 1, device=self.device, dtype=torch.long)
                }

                self.agent_memory_pools[agent_name] = memory_pool
                self.stats['agent_specific_stats'][agent_name] = {
                    'inferences': 0,
                    'avg_time': 0.0,
                    'errors': 0
                }

                logger.critical(f"GPU memory pool initialized for agent {agent_name}")

            except Exception as e:
                logger.error(f"Failed to initialize GPU resources for {agent_name}: {e}")

    def register_agent(self, agent_name: str, agent: Any):
        """Register a new agent with the GPU processing tier"""
        self.agents_registry[agent_name] = agent
        if agent_name not in self.agent_memory_pools:
            # Initialize memory pool for new agent
            seq_len = getattr(agent, 'seq_len', 20)
            state_dim = getattr(agent, 'state_dim', 58)

            if torch.cuda.is_available():
                memory_pool = {
                    'states': torch.zeros(self.batch_size, seq_len, state_dim, device=self.device, dtype=torch.float32),
                    'q_values': torch.zeros(self.batch_size, 2, device=self.device, dtype=torch.float32),
                    'actions': torch.zeros(self.batch_size, 1, device=self.device, dtype=torch.long)
                }
                self.agent_memory_pools[agent_name] = memory_pool

            self.stats['agent_specific_stats'][agent_name] = {
                'inferences': 0,
                'avg_time': 0.0,
                'errors': 0
            }

            logger.info(f"Agent {agent_name} registered with GPU processing tier")

    def submit_trading_inference_request(self, agent_name: str, features: Dict[str, float],
                                       state_sequence: np.ndarray, priority: int = 3) -> str:
        """Submit inference request with priority support"""
        request_id = f"{agent_name}_{self.request_counter}_{time.time()}"
        self.request_counter += 1

        agent_instance = self.agents_registry.get(agent_name)
        if not agent_instance:
            logger.warning(f"Agent {agent_name} not registered")
            return None

        request = TradingInferenceRequest(
            agent_name=agent_name,
            features=features,
            state_sequence=state_sequence,
            request_id=request_id,
            timestamp=time.time(),
            agent_instance=agent_instance,
            trading_metadata={'priority': priority}
        )

        # Load balance across GPU workers, considering agent affinity
        worker_id = hash(agent_name) % self.num_threads

        try:
            # Priority queue: lower number = higher priority
            self.inference_queues[worker_id].put((priority, time.time(), request), timeout=0.1)
            return request_id
        except queue.Full:
            logger.warning(f"GPU queue {worker_id} full, dropping request for {agent_name}")
            return None

    def start(self):
        """Start GPU processing workers"""
        self.running = True

        for worker_id in range(self.num_threads):
            worker = threading.Thread(
                target=self._gpu_worker,
                args=(worker_id,),
                daemon=True
            )
            worker.start()
            self.threads.append(worker)

        logger.info(f"Enhanced GPU processing tier started with {self.num_threads} workers")

    def _gpu_worker(self, worker_id: int):
        """Enhanced GPU worker with real agent integration"""
        logger.info(f"Enhanced GPU worker {worker_id} started")

        work_queue = self.inference_queues[worker_id]
        stream = self.cuda_streams[worker_id] if torch.cuda.is_available() else None
        batch_buffer = []

        while self.running:
            try:
                batch_start = time.time()
                batch_buffer.clear()

                # Collect batch with priority consideration
                try:
                    priority, timestamp, request = work_queue.get(timeout=0.1)
                    if request is None:  # Poison pill
                        break
                    batch_buffer.append(request)
                except queue.Empty:
                    continue

                # Fill batch with more requests (non-blocking)
                while len(batch_buffer) < self.batch_size:
                    try:
                        priority, timestamp, request = work_queue.get_nowait()
                        if request is None:
                            break
                        batch_buffer.append(request)
                    except queue.Empty:
                        break

                if batch_buffer:
                    # Group by agent for efficient batching
                    agent_batches = {}
                    for req in batch_buffer:
                        if req.agent_name not in agent_batches:
                            agent_batches[req.agent_name] = []
                        agent_batches[req.agent_name].append(req)

                    # Process each agent's batch
                    all_results = []
                    for agent_name, agent_requests in agent_batches.items():
                        try:
                            agent_results = self._process_agent_batch(agent_requests, stream, worker_id)
                            all_results.extend(agent_results)
                        except Exception as e:
                            logger.error(f"Agent batch processing failed for {agent_name}: {e}")
                            # Create error results
                            for req in agent_requests:
                                error_result = TradingInferenceResult(
                                    agent_name=req.agent_name,
                                    q_values=np.array([0.5, 0.5]),
                                    action=0,
                                    request_id=req.request_id,
                                    processing_time=0.0,
                                    confidence=0.0
                                )
                                all_results.append(error_result)

                    # Send results to collector
                    for result in all_results:
                        try:
                            self.result_queue.put(result, timeout=0.1)
                        except queue.Full:
                            logger.warning("Result queue full, dropping result")

                    # Update statistics
                    batch_time = time.time() - batch_start
                    self.stats['batches_processed'] += 1
                    self.stats['total_inferences'] += len(batch_buffer)

                    alpha = 0.1
                    self.stats['avg_batch_time'] = (
                        alpha * batch_time + (1 - alpha) * self.stats['avg_batch_time']
                    )

                    logger.debug(f"Enhanced GPU worker {worker_id} processed {len(batch_buffer)} requests in {batch_time:.3f}s")

            except Exception as e:
                logger.error(f"Enhanced GPU worker {worker_id} error: {e}")
                time.sleep(0.01)

    def _process_agent_batch(self, requests: List[TradingInferenceRequest], stream, worker_id: int) -> List[TradingInferenceResult]:
        """Process batch of requests for a specific agent"""
        if not requests:
            return []

        agent_name = requests[0].agent_name
        agent = requests[0].agent_instance

        if not agent:
            logger.error(f"No agent instance for {agent_name}")
            return []

        try:
            start_time = time.time()

            # Use agent's actual predict method with batching
            batch_states = []
            batch_features = []

            for req in requests:
                batch_states.append(req.state_sequence)
                batch_features.append(req.features)

            # Check if agent supports batch inference
            if hasattr(agent, 'batch_predict'):
                # Use agent's batch prediction method
                batch_q_values, batch_actions = agent.batch_predict(batch_states, batch_features)
            else:
                # Fallback to individual predictions
                batch_q_values = []
                batch_actions = []

                for req in requests:
                    try:
                        # Update agent features
                        agent.update_features(req.features)
                        q_values = agent.predict(add_noise=False)
                        action = agent.get_discrete_action(q_values)

                        batch_q_values.append(q_values)
                        batch_actions.append(action)

                    except Exception as e:
                        logger.error(f"Individual prediction failed for {agent_name}: {e}")
                        batch_q_values.append(np.array([0.5, 0.5]))
                        batch_actions.append(0)

            processing_time = time.time() - start_time

            # Create results
            results = []
            for i, req in enumerate(requests):
                q_values = batch_q_values[i] if i < len(batch_q_values) else np.array([0.5, 0.5])
                action = batch_actions[i] if i < len(batch_actions) else 0

                # Calculate confidence based on Q-value spread
                confidence = float(np.max(q_values) - np.min(q_values)) if len(q_values) > 1 else 0.0

                result = TradingInferenceResult(
                    agent_name=agent_name,
                    q_values=q_values,
                    action=action,
                    request_id=req.request_id,
                    processing_time=processing_time / len(requests),
                    confidence=confidence,
                    trading_signals={'features': req.features, 'timestamp': req.timestamp}
                )
                results.append(result)

            # Update agent-specific statistics
            if agent_name in self.stats['agent_specific_stats']:
                agent_stats = self.stats['agent_specific_stats'][agent_name]
                agent_stats['inferences'] += len(requests)

                # Update average time with exponential moving average
                alpha = 0.1
                agent_stats['avg_time'] = (
                    alpha * processing_time + (1 - alpha) * agent_stats['avg_time']
                )

            return results

        except Exception as e:
            logger.error(f"Agent batch processing failed for {agent_name}: {e}")
            self.stats['agent_specific_stats'][agent_name]['errors'] += 1
            return []

    def get_result(self, timeout: float = 0.1) -> Optional[TradingInferenceResult]:
        """Get inference result from result queue"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def stop(self):
        """Stop GPU processing tier"""
        self.running = False

        # Send poison pills to stop workers
        for i in range(self.num_threads):
            try:
                self.inference_queues[i].put((0, time.time(), None), timeout=1.0)
            except queue.Full:
                pass

        # Wait for workers to finish
        for worker in self.threads:
            worker.join(timeout=2.0)

        logger.info("Enhanced GPU processing tier stopped")

    def get_stats(self) -> Dict[str, Any]:
        """Get GPU processing tier statistics"""
        total_queue_depth = sum(q.qsize() for q in self.inference_queues)
        self.stats['queue_depth'] = total_queue_depth + self.result_queue.qsize()
        return self.stats.copy()

# ============================================================================
# TIER 3: Analytics and Result Collection Tier
# ============================================================================

class AnalyticsTier:
    """Analytics tier for result collection and system monitoring"""

    def __init__(self, buffer_size: int = 100000000):
        self.buffer_size = buffer_size
        self.running = False

        # Result collection
        self.result_buffer = deque(maxlen=buffer_size)
        self.analytics_queue = queue.Queue(maxsize=buffer_size)

        # Analytics workers
        self.workers = []

        # Peeeformance metrics
        self.performance_metrics = {
            'total_results_processed': 0,
            'avg_inference_time': 0.0,
            'confidence_distribution': {},
            'agent_performance': {},
            'system_throughput': 0.0,
            'error_rate': 0.0
        }

        # Time tracking
        self.start_time = time.time()
        self.last_metrics_update = time.time()

    def start(self):
        """Start analytics tier workers"""
        self.running = True

        # Start result collection worker
        collector_worker = threading.Thread(
            target=self._result_collector,
            daemon=True
        )
        collector_worker.start()
        self.workers.append(collector_worker)

        # Start analytics worker
        analytics_worker = threading.Thread(
            target=self._analytics_worker,
            daemon=True
        )
        analytics_worker.start()
        self.workers.append(analytics_worker)

        logger.info("Analytics tier started")

    def submit_result(self, result: TradingInferenceResult) -> bool:
        """Submit result for analytics processing"""
        try:
            self.analytics_queue.put(result, timeout=0.1)
            return True
        except queue.Full:
            logger.warning("Analytics queue full, dropping result")
            return False

    def _result_collector(self):
        """Collect and buffer results"""
        while self.running:
            try:
                result = self.analytics_queue.get(timeout=1.0)
                if result is None:  # Poison pill
                    break

                # Add to buffer
                self.result_buffer.append(result)

                # Update basic metrics
                self._update_metrics(result)

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"Result collector error: {e}")

    def _analytics_worker(self):
        """Perform analytics on collected results"""
        while self.running:
            try:
                time.sleep(5.0)  # Run analytics every 5 seconds

                if len(self.result_buffer) > 0:
                    self._compute_analytics()

            except Exception as e:
                logger.error(f"Analytics worker error: {e}")

    def _update_metrics(self, result: TradingInferenceResult):
        """Update performance metrics with new result"""
        self.performance_metrics['total_results_processed'] += 1

        # Update average inference time
        alpha = 0.1
        current_avg = self.performance_metrics['avg_inference_time']
        self.performance_metrics['avg_inference_time'] = (
            alpha * result.processing_time + (1 - alpha) * current_avg
        )

        # Update agent-specific performance
        agent_name = result.agent_name
        if agent_name not in self.performance_metrics['agent_performance']:
            self.performance_metrics['agent_performance'][agent_name] = {
                'total_inferences': 0,
                'avg_time': 0.0,
                'avg_confidence': 0.0
            }

        agent_perf = self.performance_metrics['agent_performance'][agent_name]
        agent_perf['total_inferences'] += 1
        agent_perf['avg_time'] = (
            alpha * result.processing_time + (1 - alpha) * agent_perf['avg_time']
        )
        agent_perf['avg_confidence'] = (
            alpha * result.confidence + (1 - alpha) * agent_perf['avg_confidence']
        )

    def _compute_analytics(self):
        """Compute comprehensive analytics"""
        current_time = time.time()
        time_elapsed = current_time - self.last_metrics_update

        if time_elapsed > 0:
            # Calculate throughput
            total_processed = self.performance_metrics['total_results_processed']
            self.performance_metrics['system_throughput'] = total_processed / (current_time - self.start_time)

            # Update confidence distribution
            recent_results = list(self.result_buffer)[-100:]  # Last 100 results
            if recent_results:
                confidences = [r.confidence for r in recent_results]
                self.performance_metrics['confidence_distribution'] = {
                    'mean': np.mean(confidences),
                    'std': np.std(confidences),
                    'min': np.min(confidences),
                    'max': np.max(confidences)
                }

        self.last_metrics_update = current_time

    def stop(self):
        """Stop analytics tier"""
        self.running = False

        # Send poison pill
        try:
            self.analytics_queue.put(None, timeout=1.0)
        except queue.Full:
            pass

        # Wait for workers
        for worker in self.workers:
            worker.join(timeout=2.0)

        logger.info("Analytics tier stopped")

    def get_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        return self.performance_metrics.copy()

    def get_recent_results(self, count: int = 100) -> List[TradingInferenceResult]:
        """Get recent results for analysis"""
        return list(self.result_buffer)[-count:]

# ============================================================================
# Three-Tier Coordinator
# ============================================================================

# ============================================================================
# Enhanced Integration Manager for Trading System
# ============================================================================

class TradingSystemIntegrationManager:
    """Manages integration between three-tier architecture and trading system"""

    def __init__(self, IntegratedSignalSystem, three_tier_coordinator):
        self.trading_system = IntegratedSignalSystem
        self.coordinator = three_tier_coordinator
        self.running = False

        # Integration state
        self.feature_processing_enabled = True
        self.reward_processing_enabled = True
        self.signal_generation_enabled = True

        # Performance tracking
        self.integration_stats = {
            'features_processed': 0,
            'rewards_processed': 0,
            'signals_generated': 0,
            'integration_errors': 0,
            'avg_processing_latency': 0.0
        }

        # Agent registry for GPU tier
        self.agent_registry = {}

        # Setup enhanced GPU tier with trading agents
        self._setup_enhanced_gpu_tier()

        # Connect the tiers
        self._connect_processing_pipeline()

    def _setup_enhanced_gpu_tier(self):
        """Replace standard GPU tier with enhanced version"""
        # Register all trading agents
        for agent_name, agent in self.trading_system.agents.items():
            self.agent_registry[agent_name] = agent

        # Create enhanced GPU tier
        enhanced_gpu_tier = EnhancedGPUProcessingTier(
            num_threads=8,
            batch_size=64,
            agents_registry=self.agent_registry
        )

        # Replace in coordinator
        if hasattr(self.coordinator, 'gpu_tier'):
            self.coordinator.gpu_tier.stop()

        self.coordinator.gpu_tier = enhanced_gpu_tier
        self.coordinator.enhanced_gpu_tier = enhanced_gpu_tier

    def _connect_processing_pipeline(self):
        """Connect three-tier processing to trading system pipeline"""

        # Enhance ingestion tier processing
        original_process_feature = self.coordinator.ingestion_tier._process_feature_message
        original_process_reward = self.coordinator.ingestion_tier._process_reward_message

        def enhanced_feature_processing(message):
            """Enhanced feature processing that triggers GPU inference"""
            try:
                if not self.feature_processing_enabled:
                    return

                start_time = time.time()

                # Get agent and prepare for inference
                agent_name = message.agent_name
                agent = self.agent_registry.get(agent_name)

                if agent:
                    # Update agent features
                    agent.update_features(message.features)

                    # Get current state sequence
                    state_sequence = agent.get_current_state_sequence()

                    # Submit to enhanced GPU processing
                    request_id = self.coordinator.enhanced_gpu_tier.submit_trading_inference_request(
                        agent_name=agent_name,
                        features=message.features,
                        state_sequence=state_sequence,
                        priority=getattr(message, 'priority', 3)
                    )

                    if request_id:
                        self.integration_stats['features_processed'] += 1

                        # Update latency stats
                        latency = time.time() - start_time
                        alpha = 0.1
                        self.integration_stats['avg_processing_latency'] = (
                            alpha * latency + (1 - alpha) * self.integration_stats['avg_processing_latency']
                        )
                    else:
                        logger.warning(f"Failed to submit inference request for {agent_name}")
                        self.integration_stats['integration_errors'] += 1
                else:
                    logger.warning(f"Agent {agent_name} not found in registry")
                    self.integration_stats['integration_errors'] += 1

            except Exception as e:
                logger.error(f"Enhanced feature processing error: {e}")
                self.integration_stats['integration_errors'] += 1

        def enhanced_reward_processing(reward_msg):
            """Enhanced reward processing with agent state updates"""
            try:
                if not self.reward_processing_enabled:
                    return

                # Extract reward information
                signal_key = reward_msg.get('signal_key')
                reward = reward_msg.get('reward')
                multipliers = reward_msg.get('multipliers', {})

                # Process through batch processor if available
                if hasattr(self.trading_system, 'batch_processor') and self.trading_system.batch_processor:
                    asyncio.run_coroutine_threadsafe(
                        self.trading_system.batch_processor.add_reward(signal_key, reward, multipliers),
                        self.trading_system.loop
                    )
                else:
                    # Fallback to original processing
                    original_process_reward(reward_msg)

                self.integration_stats['rewards_processed'] += 1

            except Exception as e:
                logger.error(f"Enhanced reward processing error: {e}")
                self.integration_stats['integration_errors'] += 1

        # Replace processing methods
        self.coordinator.ingestion_tier._process_feature_message = enhanced_feature_processing
        self.coordinator.ingestion_tier._process_reward_message = enhanced_reward_processing

        # Setup result handling from GPU tier
        self._setup_result_handling()

    def _setup_result_handling(self):
        """Setup handling of GPU processing results"""

        def enhanced_result_handling(result: TradingInferenceResult):
            """Enhanced result handling that feeds back to trading system"""
            try:
                if not self.signal_generation_enabled:
                    return

                agent_name = result.agent_name

                # Store result in agent's recent q_values if agent exists
                agent = self.agent_registry.get(agent_name)
                if agent and hasattr(agent, 'recent_q_values'):
                    agent.recent_q_values.append(result.q_values)

                # Trigger signal generation if this was part of a coordinated inference
                if result.trading_signals:
                    self._trigger_signal_generation(result)

                self.integration_stats['signals_generated'] += 1

                logger.debug(f"Enhanced result handling: {agent_name} -> action {result.action}, confidence {result.confidence:.3f}")

            except Exception as e:
                logger.error(f"Enhanced result handling error: {e}")
                self.integration_stats['integration_errors'] += 1

        # Create a wrapper to handle results from analytics tier
        original_analytics_submit = self.coordinator.analytics_tier.submit_result

        def wrapped_analytics_submit(result):
            enhanced_result_handling(result)
            return original_analytics_submit(result)

        self.coordinator.analytics_tier.submit_result = wrapped_analytics_submit

    def _trigger_signal_generation(self, result: TradingInferenceResult):
        """Trigger trading system signal generation based on GPU results"""
        try:
            # This would integrate with the trading system's signal generation
            # For now, just log the signal potential

            if result.confidence > 0.1:  # Only consider high-confidence signals
                signal_data = {
                    'agent': result.agent_name,
                    'action': result.action,
                    'q_values': result.q_values.tolist(),
                    'confidence': result.confidence,
                    'timestamp': time.time()
                }

                # Submit to trading system's signal processing
                if hasattr(self.trading_system, '_process_agent_signal'):
                    self.trading_system._process_agent_signal(signal_data)

                logger.debug(f"Signal triggered: {result.agent_name} action {result.action} confidence {result.confidence:.3f}")

        except Exception as e:
            logger.error(f"Signal generation trigger error: {e}")

    def start(self):
        """Start the integrated system"""
        self.running = True
        self.coordinator.start()
        logger.info("Trading system integration manager started")

    def stop(self):
        """Stop the integrated system"""
        self.running = False
        self.coordinator.stop()
        logger.info("Trading system integration manager stopped")

    def get_integration_stats(self) -> Dict[str, Any]:
        """Get integration performance statistics"""
        coordinator_stats = self.coordinator.get_system_stats()

        return {
            'integration': self.integration_stats,
            'coordinator': coordinator_stats,
            'agent_registry_size': len(self.agent_registry),
            'feature_processing_enabled': self.feature_processing_enabled,
            'reward_processing_enabled': self.reward_processing_enabled,
            'signal_generation_enabled': self.signal_generation_enabled
        }

# ============================================================================
# INTEGRATION FUNCTIONS FOR EXISTING TRADING SYSTEM
# Paste these functions AFTER the IntegratedSignalSystem class definition
# ============================================================================

class QuantumExperienceCollector:
    """
    Collects and manages partial experiences for quantum training.
    Stores full state information when signals are created, then completes
    them when rewards arrive.
    """

    def __init__(self, max_age_seconds=3600):
        self.partial_experiences = {}  # signal_key -> experience dict
        self.max_age = max_age_seconds
        self.stats = {
            'experiences_created': 0,
            'experiences_completed': 0,
            'experiences_expired': 0
        }

    def store_signal_experience(self, signal_key: str, agent_name: str,
                                states_dict: Dict[str, Dict[str, np.ndarray]],
                                action: int, q_values: np.ndarray):
        """
        Store full experience data when signal is generated.

        Args:
            signal_key: Unique signal identifier
            agent_name: Name of agent that generated signal
            states_dict: Complete multi-timeframe state data
                        {agent_name: {timeframe: state_array}}
            action: Discrete action taken (0=BUY, 1=SELL)
            q_values: Q-values that led to this action

        Returns:
            bool: True if stored successfully, False otherwise
        """
        try:
            experience = {
                'agent_name': agent_name,
                'states': states_dict,
                'action': action,
                'q_values': q_values.copy() if isinstance(q_values, np.ndarray) else q_values,
                'timestamp': time.time(),
                'completed': False
            }

            self.partial_experiences[signal_key] = experience
            self.stats['experiences_created'] += 1

            logger.debug(f"[{agent_name}] Stored signal experience: {signal_key}")

            return True  # âœ… CRITICAL FIX: Return True on success

        except Exception as e:
            logger.error(f"Failed to store signal experience: {e}")
            return False  # âœ… CRITICAL FIX: Return False on failure

    def complete_experience(self, signal_key: str, reward: float,
                           next_states_dict: Dict[str, Dict[str, np.ndarray]] = None,
                           done: bool = False):
        """
        Complete experience when reward arrives and RETURN the completed experience.

        Args:
            signal_key: Signal identifier
            reward: Reward value received
            next_states_dict: Next state (if available)
            done: Whether episode is complete

        Returns:
            Dict: The completed experience, or None if not found
        """
        try:
            if signal_key not in self.partial_experiences:
                logger.warning(f"No partial experience found for signal: {signal_key}")
                return None  # Return None instead of False

            experience = self.partial_experiences[signal_key]

            # Add reward and completion data
            experience['reward'] = float(reward)
            experience['done'] = done

            # If no next_states provided, use current states as next states
            if next_states_dict is None:
                experience['next_states'] = experience['states'].copy()
            else:
                experience['next_states'] = next_states_dict

            experience['completed'] = True
            experience['completion_time'] = time.time()

            self.stats['experiences_completed'] += 1

            logger.debug(f"Completed experience: {signal_key} with reward {reward}")

            # âœ… CRITICAL FIX: Return the actual experience object!
            return experience

        except Exception as e:
            logger.error(f"Failed to complete experience: {e}")
            return None  # Return None instead of False

    def get_completed_experiences(self, batch_size: int = 64) -> List[Dict]:
        """
        Get batch of completed experiences for training.

        Returns:
            List of completed experience dicts
        """
        completed = []

        for signal_key, exp in list(self.partial_experiences.items()):
            if exp.get('completed', False):
                completed.append(exp)

                # Remove from storage after retrieving
                del self.partial_experiences[signal_key]

                if len(completed) >= batch_size:
                    break

        return completed

    def cleanup_expired(self):
        """Remove expired incomplete experiences"""
        current_time = time.time()
        expired_keys = []

        for signal_key, exp in self.partial_experiences.items():
            if not exp.get('completed', False):
                age = current_time - exp['timestamp']
                if age > self.max_age:
                    expired_keys.append(signal_key)

        for key in expired_keys:
            del self.partial_experiences[key]
            self.stats['experiences_expired'] += 1

        if expired_keys:
            logger.warning(f"Cleaned up {len(expired_keys)} expired experiences")

    def get_stats(self) -> Dict[str, int]:
        """Get collector statistics"""
        return {
            **self.stats,
            'pending_experiences': len(self.partial_experiences)
        }

# ============================================================================

class MetaModelTrainer:
  def __init__(self):
      pass

  def prepare_meta_model_input(
      self,
      q_values_dict, actions_dict, state, voting_pred,
      close_price=0.0,
      distance_to_nearest_support=0.0,
      distance_to_nearest_resistance=0.0,
      near_support=False,
      near_resistance=False,
      distance_to_stop_loss=0.0,
      support_strength=0.0,
      resistance_strength=0.0
  ):
      try:
          state = np.array(state, dtype=np.float32).flatten()
          if np.any(np.isnan(state)):
              state = np.nan_to_num(state, nan=0.0)
          if state.shape[0] > 15:
              state = state[:15]
          elif state.shape[0] < 15:
              state = np.pad(state, (0, 15 - state.shape[0]), mode='constant')

          # Fixed: Explicit dict validation to avoid numpy array boolean ambiguity
          if not isinstance(q_values_dict, dict):
              logger.error(f"q_values_dict must be dict, got {type(q_values_dict)} in prepare_meta_model_input")
              return np.zeros((1, 30), dtype=np.float32)

          if len(q_values_dict) == 0:
              logger.error("q_values_dict is empty in prepare_meta_model_input")
              return np.zeros((1, 30), dtype=np.float32)

          if not isinstance(actions_dict, dict):
              logger.error(f"actions_dict must be dict, got {type(actions_dict)} in prepare_meta_model_input")
              return np.zeros((1, 30), dtype=np.float32)

          if len(actions_dict) == 0:
              logger.error("actions_dict is empty in prepare_meta_model_input")
              return np.zeros((1, 30), dtype=np.float32)

          first_agent = next(iter(q_values_dict))
          q = np.array(q_values_dict[first_agent], dtype=np.float32).flatten()
          if np.any(np.isnan(q)):
              q = np.nan_to_num(q, nan=0.0)
          action = actions_dict[first_agent]

          onehot = np.zeros(2, dtype=np.float32)
          if 0 <= action < 2:
              onehot[action] = 1.0

          q_stats = [np.max(q), np.min(q), np.std(q)]

          voting_onehot = np.zeros(2, dtype=np.float32)
          if voting_pred in [0, 1]:
              voting_onehot[voting_pred] = 1.0

          meta_extra_features = [
              float(close_price),
              float(distance_to_nearest_support),
              float(distance_to_nearest_resistance),
              float(near_support),
              float(near_resistance),
              float(distance_to_stop_loss),
              float(support_strength),
              float(resistance_strength),
          ]

          meta_input = np.concatenate([
              state, onehot, q_stats, voting_onehot, meta_extra_features
          ]).astype(np.float32)

          if np.any(np.isnan(meta_input)):
              meta_input = np.nan_to_num(meta_input, nan=0.0)
          if meta_input.shape[0] != 30:
              meta_input = np.pad(meta_input, (0, 30 - meta_input.shape[0]), mode='constant')

          assert meta_input.shape[0] == 30, f"Meta-model input shape error: expected 30, got {meta_input.shape[0]}"
          return meta_input.reshape(1, 30)

      except Exception as e:
          logger.error(f"âŒ Error in prepare_meta_model_input: {e}")
          return np.zeros((1, 30), dtype=np.float32)

def prepare_meta_model_input(
    self,
    q_values_dict, actions_dict, state, voting_pred,
    close_price=0.0,
    distance_to_nearest_support=0.0,
    distance_to_nearest_resistance=0.0,
    near_support=False,
    near_resistance=False,
    distance_to_stop_loss=0.0,
    support_strength=0.0,
    resistance_strength=0.0
):
    """Prepare meta-model input with strict shape validation"""
    try:
        # Process state: ensure exactly 15 dimensions
        state = np.array(state, dtype=np.float32).flatten()
        if np.any(np.isnan(state)):
            state = np.nan_to_num(state, nan=0.0)
        if state.shape[0] > 15:
            state = state[:15]
        elif state.shape[0] < 15:
            state = np.pad(state, (0, 15 - state.shape[0]), mode='constant')

        # Fixed: Explicit dict validation to avoid numpy array boolean ambiguity
        if not isinstance(q_values_dict, dict):
            logger.error(f"q_values_dict must be dict, got {type(q_values_dict)} in prepare_meta_model_input")
            return np.zeros((1, 30), dtype=np.float32)

        if len(q_values_dict) == 0:
            logger.error("q_values_dict is empty in prepare_meta_model_input")
            return np.zeros((1, 30), dtype=np.float32)

        if not isinstance(actions_dict, dict):
            logger.error(f"actions_dict must be dict, got {type(actions_dict)} in prepare_meta_model_input")
            return np.zeros((1, 30), dtype=np.float32)

        if len(actions_dict) == 0:
            logger.error("actions_dict is empty in prepare_meta_model_input")
            return np.zeros((1, 30), dtype=np.float32)

        # Get first agent's data
        first_agent = next(iter(q_values_dict))
        q = np.array(q_values_dict[first_agent], dtype=np.float32).flatten()
        if np.any(np.isnan(q)):
            q = np.nan_to_num(q, nan=0.0)
        action = actions_dict[first_agent]

        # One-hot encode action (2 dimensions)
        onehot = np.zeros(2, dtype=np.float32)
        if 0 <= action < 2:
            onehot[action] = 1.0

        # Q-value statistics (3 dimensions)
        q_stats = [float(np.max(q)), float(np.min(q)), float(np.std(q))]

        # One-hot encode voting prediction (2 dimensions)
        voting_onehot = np.zeros(2, dtype=np.float32)
        if voting_pred in [0, 1]:
            voting_onehot[voting_pred] = 1.0

        # Extra features (8 dimensions)
        meta_extra_features = [
            float(close_price),
            float(distance_to_nearest_support),
            float(distance_to_nearest_resistance),
            float(near_support),
            float(near_resistance),
            float(distance_to_stop_loss),
            float(support_strength),
            float(resistance_strength),
        ]

        # Concatenate all parts: 15 + 2 + 3 + 2 + 8 = 30
        meta_input = np.concatenate([
            state,                  # 15
            onehot,                 # 2
            q_stats,                # 3
            voting_onehot,          # 2
            meta_extra_features     # 8
        ]).astype(np.float32)

        # Final validation
        if np.any(np.isnan(meta_input)):
            meta_input = np.nan_to_num(meta_input, nan=0.0)

        if meta_input.shape[0] != 30:
            logger.error(f"Meta-model input shape error: expected 30, got {meta_input.shape[0]}")
            meta_input = np.pad(meta_input, (0, max(0, 30 - meta_input.shape[0])), mode='constant')[:30]

        # Return as (1, 30) for batch compatibility
        return meta_input.reshape(1, 30)

    except Exception as e:
        logger.error(f"âŒ Error in prepare_meta_model_input: {e}")
        import traceback
        traceback.print_exc()
        return np.zeros((1, 30), dtype=np.float32)

    def train_meta_model(self, model, optimizer, loss_fn, batch_experiences, epochs=5):
        """
        Trains the meta-model on a batch of experiences with proper shape handling.
        """
        if not batch_experiences or len(batch_experiences) == 0:
            logger.warning("No experiences provided for meta-model training")
            return None

        try:
            inputs = []
            targets = []

            for exp in batch_experiences:
                meta_input = self.prepare_meta_model_input_from_experience(exp)
                if meta_input is not None and meta_input.shape[1] == 30:
                    inputs.append(meta_input[0])  # Extract from (1, 30) shape
                    targets.append(1.0 if exp.get('reward', 0) > 0 else 0.0)
                else:
                    logger.error(f"Meta-model experience input has invalid shape {meta_input.shape if meta_input is not None else 'None'}, skipping.")

            if len(inputs) == 0:
                logger.error("No valid inputs for meta-model training. Skipping batch.")
                return None

            # CRITICAL FIX: Ensure proper shape (batch_size, 30)
            X = enforce_meta_model_input_shape(inputs, expected_dim=30)
            if X.shape[1] != 30:
                logger.error(f"Meta-model batch input shape error: expected (?, 30), got {X.shape}. Skipping batch.")
                return None

            # CRITICAL FIX: Ensure targets have correct shape (batch_size, 1)
            y = np.array(targets, dtype=np.float32).reshape(-1, 1)

            # Validate shapes before creating tensors
            if X.shape[0] != y.shape[0]:
                logger.error(f"Shape mismatch: X has {X.shape[0]} samples, y has {y.shape[0]} samples")
                return None

            # Create TensorFlow tensors with explicit shapes
            X_tf = tf.constant(X, dtype=tf.float32)
            y_tf = tf.constant(y, dtype=tf.float32)

            # Final validation
            logger.debug(f"Meta-model training shapes - X: {X_tf.shape}, y: {y_tf.shape}")

            if X_tf.shape[0] == 0 or y_tf.shape[0] == 0:
                logger.error("Empty tensor passed to tf.data.Dataset, skipping training.")
                return None

            batch_size = min(32, len(inputs))
            dataset = tf.data.Dataset.from_tensor_slices((X_tf, y_tf)).batch(batch_size, drop_remainder=False)

            total_loss = 0.0
            batch_count = 0

            for epoch in range(epochs):
                epoch_loss = 0.0
                epoch_batches = 0

                for batch_x, batch_y in dataset:
                    # Validate batch shapes
                    if batch_x.shape[1] != 30:
                        logger.error(f"Batch_x shape error: expected (?, 30), got {batch_x.shape}. Skipping batch.")
                        continue

                    if len(batch_y.shape) == 1:
                        batch_y = tf.reshape(batch_y, (-1, 1))

                    batch_x = tf.cast(batch_x, tf.float32)
                    batch_y = tf.cast(batch_y, tf.float32)

                    try:
                        with tf.GradientTape() as tape:
                            # Forward pass with shape validation
                            predictions = model(batch_x, training=True)

                            # Ensure predictions have correct shape
                            if len(predictions.shape) == 1:
                                predictions = tf.reshape(predictions, (-1, 1))

                            # Ensure both tensors have matching shapes
                            if predictions.shape != batch_y.shape:
                                if predictions.shape[0] == batch_y.shape[0]:
                                    # Same batch size, adjust shape
                                    if len(batch_y.shape) == 1:
                                        batch_y = tf.reshape(batch_y, (-1, 1))
                                    elif len(predictions.shape) == 1:
                                        predictions = tf.reshape(predictions, (-1, 1))
                                else:
                                    logger.error(f"Prediction/target shape mismatch: {predictions.shape} vs {batch_y.shape}")
                                    continue

                            loss = loss_fn(batch_y, predictions)

                        # Compute gradients
                        gradients = tape.gradient(loss, model.trainable_variables)

                        # Check for None gradients
                        if gradients is None or any(g is None for g in gradients):
                            logger.warning("None gradients detected, skipping update")
                            continue

                        # Apply gradients with clipping
                        gradients = [tf.clip_by_norm(g, 1.0) if g is not None else g for g in gradients]
                        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

                        epoch_loss += float(loss.numpy())
                        epoch_batches += 1

                    except Exception as e:
                        logger.error(f"Meta-model training crashed during batch: {e}")
                        continue

                if epoch_batches > 0:
                    avg_epoch_loss = epoch_loss / epoch_batches
                    total_loss += avg_epoch_loss
                    logger.debug(f"Meta-model epoch {epoch+1}/{epochs}: loss={avg_epoch_loss:.4f}")

            avg_loss = total_loss / epochs if epochs > 0 else None
            logger.info(f"âœ… Meta-model training completed - Average Loss: {avg_loss if avg_loss is not None else 'N/A'}")
            return avg_loss

        except Exception as e:
            logger.error(f"Meta-model training failed: {e}")
            import traceback
            traceback.print_exc()
            return None
@register_keras_serializable()
class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, seq_len, d_model, **kwargs):
        super().__init__(**kwargs)
        self.seq_len = seq_len
        self.d_model = d_model
        pos = np.arange(seq_len)[:, np.newaxis]
        i = np.arange(d_model)[np.newaxis, :]
        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
        angle_rads = pos * angle_rates
        pe = np.zeros((seq_len, d_model))
        pe[:, 0::2] = np.sin(angle_rads[:, 0::2])
        pe[:, 1::2] = np.cos(angle_rads[:, 1::2])
        self.pos_encoding = tf.constant(pe[np.newaxis, ...], dtype=tf.float32)

    def call(self, x):
        pe = tf.cast(self.pos_encoding, x.dtype)
        return x + pe[:, :tf.shape(x)[1], :]

    def get_config(self):
        config = super().get_config()
        config.update({
            "seq_len": self.seq_len,
            "d_model": self.d_model,
        })
        return config

class ComplexLayerNorm(nn.Module):
    """Layer normalization for complex numbers - WORKS WITH BATCH_SIZE=1"""
    def __init__(self, features, eps=1e-5):
        super().__init__()
        self.ln_real = nn.LayerNorm(features, eps=eps)
        self.ln_imag = nn.LayerNorm(features, eps=eps)

    def forward(self, z):
        """
        Args:
            z: complex tensor of any shape (..., features)
        Returns:
            normalized complex tensor
        """
        if not torch.is_complex(z):
            # If real, convert to complex
            z = torch.complex(z, torch.zeros_like(z))

        return torch.complex(
            self.ln_real(z.real),
            self.ln_imag(z.imag)
        )

@register_keras_serializable()
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        super().__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-4)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-4)
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs)
        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))
        ffn_output = self.ffn(out1)
        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))

def build_meta_model(input_dim=30, seq_len=5, embed_dim=6, num_heads=2, ff_dim=128, num_blocks=2):
    inputs = tf.keras.Input(shape=(input_dim,), name="meta_input")
    x = tf.keras.layers.Reshape((seq_len, input_dim // seq_len))(inputs)
    x = PositionalEncoding(seq_len, input_dim // seq_len)(x)
    for _ in range(num_blocks):
        x = TransformerBlock(embed_dim=input_dim // seq_len, num_heads=num_heads, ff_dim=ff_dim)(x)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(16, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    outputs = tf.keras.layers.Dense(1, activation="sigmoid", dtype="float32")(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-4),
        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),
        metrics=["accuracy", tf.keras.metrics.AUC(name="auc")]
    )
    print(f"âœ… Transformer Meta-Model initialized: {model.count_params()} parameters")
    return model

def mc_dropout_predict(model, x, n_iter=100):
    predictions = np.array([
        model(x, training=True).numpy().flatten()
        for _ in range(n_iter)
    ])
    return predictions.mean(axis=0), predictions.std(axis=0)

def is_signal_reliable(model, input_tensor, mc_passes=20, confidence_threshold=0.3, uncertainty_threshold=0.4):
    if np.any(np.isnan(input_tensor)) or np.all(input_tensor == 0):
        logger.warning("Meta-model input contains NaNs or is all zeros. Skipping prediction.")
        return False, float('nan'), float('nan')
    predictions = []
    for _ in range(mc_passes):
        pred = model(input_tensor, training=True).numpy()[0][0]
        predictions.append(pred)
    predictions = np.array(predictions)
    mean = predictions.mean()
    std = predictions.std()
    # Defensive: If mean or std are nan, force rejection and log
    if np.isnan(mean) or np.isnan(std):
        logger.warning(f"Meta-model output is NaN (mean={mean}, std={std}).")
        return False, mean, std
    decision = (mean >= confidence_threshold and std <= uncertainty_threshold)
    return decision, mean, std

# === Set seed for reproducibility ===
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)

# === Positional Embeddings ===
def sinusoidal_positional_embedding(length, dim, device=None):
    assert dim % 2 == 0
    position = torch.arange(length, dtype=torch.float32, device=device).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, dim, 2, device=device) * (-np.log(10000.0) / dim))
    pe = torch.zeros(length, dim, device=device)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe

# === CNN Preprocessor (Optional Pre-Encoder) ===
class ConvPreprocessor(nn.Module):
    def __init__(self, input_dim, embed_dim, kernel_size=3, dropout=0.1):
        super().__init__()
        self.conv1 = nn.Conv1d(input_dim, embed_dim, kernel_size=kernel_size, padding=kernel_size//2)
        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=kernel_size, padding=kernel_size//2)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = x.transpose(1, 2)  # (B, features, seq_len)
        x = F.gelu(self.conv1(x))
        x = self.dropout(x)
        x = F.gelu(self.conv2(x))
        x = self.dropout(x)
        return x.transpose(1, 2)  # (B, seq_len, features)

# === Attention Mechanism ===
class SelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B, T, E = x.shape
        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        context = (attn @ v).transpose(1, 2).reshape(B, T, E)
        return self.out_proj(context)

# === Transformer Residual Block ===
class ResidualBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = SelfAttention(embed_dim, num_heads, dropout)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 4, embed_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

# === Main Voting Transformer ===
class VotingTransformer(nn.Module):
    def __init__(self, num_agents, features_per_agent=6, d_model=128,
                 num_heads=8, num_layers=4, dropout=0.2, num_classes=2):
        super(VotingTransformer, self).__init__()
        self.num_agents = num_agents
        self.features_per_agent = features_per_agent
        self.input_dim = num_agents * features_per_agent

        self.embedding = nn.Linear(features_per_agent, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=4 * d_model,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.classifier = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, num_classes)
        )

    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, num_agents, features_per_agent)
            mask: (batch, num_agents) - True for valid agents, False for padding
        """
        x = self.embedding(x)  # (batch, num_agents, d_model)

        # Create attention mask (inverted: True = ignore)
        if mask is not None:
            # TransformerEncoder expects: True = mask out, False = attend
            attn_mask = ~mask  # Invert: True (valid) -> False (attend)
        else:
            attn_mask = None

        # Apply encoder with mask
        if attn_mask is not None:
            # Expand to (batch, num_agents) for key_padding_mask
            x = self.encoder(x, src_key_padding_mask=attn_mask)
        else:
            x = self.encoder(x)

        # Masked mean pooling
        if mask is not None:
            mask_expanded = mask.unsqueeze(-1).float()  # (batch, num_agents, 1)
            x_masked = x * mask_expanded
            x_sum = x_masked.sum(dim=1)
            x_count = mask_expanded.sum(dim=1).clamp(min=1)  # Avoid div by zero
            x = x_sum / x_count
        else:
            x = x.mean(dim=1)

        return self.classifier(x)  # (batch, num_classes)

# === Agent Output Encoder ===
def encode_agent_outputs(q_values, actions):
    """
    Enhanced encoding with device preservation.
    Returns tensors on same device as input.
    """
    device = q_values.device  # Preserve input device
    batch_size, num_agents, _ = q_values.shape

    # Extract Q-values
    q_buy = q_values[:, :, 0]
    q_sell = q_values[:, :, 1]

    # Calculate features (all operations preserve device)
    action = actions.float()
    q_spread = torch.abs(q_buy - q_sell)
    q_argmax = q_values.argmax(dim=-1).float()

    # Confidence from softmax
    q_softmax = F.softmax(q_values, dim=-1)
    q_confidence = q_softmax.max(dim=-1).values

    # Stack all features (already on correct device)
    features = torch.stack([
        action,
        q_buy,
        q_sell,
        q_spread,
        q_argmax,
        q_confidence
    ], dim=-1)

    return features  # Returns on same device as input

# Also update the VotingDataset class to handle the mask
class VotingDataset(Dataset):
    def __init__(self, q_values, actions, final_actions, rewards, masks=None):
        self.q_values = q_values
        self.actions = actions
        self.final_actions = final_actions
        self.rewards = rewards
        self.masks = masks

    def __len__(self):
        return len(self.rewards)

    def __getitem__(self, idx):
        if self.masks is not None:
            return (self.q_values[idx], self.actions[idx],
                   self.final_actions[idx], self.rewards[idx],
                   self.masks[idx])
        else:
            return (self.q_values[idx], self.actions[idx],
                   self.final_actions[idx], self.rewards[idx])

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state'])

# Fixed Integration Issues for DDPG and TimeframeAgent

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------- OU Noise (from code1) ---------------------
class OUNoise:
    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2, dt=1e-2):
        self.action_dim = action_dim
        self.mu = mu
        self.theta = theta
        self.sigma = sigma
        self.dt = dt
        self.reset()

    def reset(self):
        self.x_prev = np.zeros(self.action_dim)

    def sample(self):
        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \
            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.action_dim)
        self.x_prev = x
        return x

# --- Residual BiLSTM Block (unified) ---

class ResidualBiLSTMBlock(nn.Module):
    """
    Residual Bidirectional LSTM block with LayerNorm and Dropout.
    Handles both batched (3D) and unbatched (2D) input, and always passes correct h0/c0 shapes.
    """
    def __init__(self, input_dim, hidden_dim, dropout=0.2, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(
            input_dim, hidden_dim,
            batch_first=True, bidirectional=True, num_layers=num_layers
        )
        self.input_proj = nn.Linear(input_dim, hidden_dim * 2) if input_dim != hidden_dim * 2 else nn.Identity()
        self.norm = nn.LayerNorm(hidden_dim * 2)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, h0=None, c0=None):
        # Accepts (batch, seq_len, features) or (seq_len, features)
        unsqueezed = False
        if x.dim() == 2:
            x = x.unsqueeze(0)   # (1, seq_len, features)
            unsqueezed = True

        batch_size = x.size(0)

        # Adjust h0/c0 shape for LSTM
        if h0 is not None and c0 is not None:
            # h0/c0 should be (num_layers * num_directions, batch, hidden_dim)
            # If batch == 1 and h0/c0 is (num_layers * num_directions, hidden_dim), unsqueeze
            if h0.dim() == 2:
                h0 = h0.unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            if c0.dim() == 2:
                c0 = c0.unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            lstm_out, _ = self.lstm(x, (h0, c0))
        else:
            lstm_out, _ = self.lstm(x)

        residual = self.input_proj(x)
        out = lstm_out + residual
        out = self.norm(out)
        out = self.dropout(out)

        if unsqueezed:
            out = out.squeeze(0)
        return out

class MultiheadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout=0.1, use_mask=False):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.use_mask = use_mask

        # Learned projections for input -> Q, K, V
        self.W_Q = nn.Linear(embed_dim, embed_dim)
        self.W_K = nn.Linear(embed_dim, embed_dim)
        self.W_V = nn.Linear(embed_dim, embed_dim)
        self.W_O = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # x: (batch, seq_len, embed_dim)
        batch_size, seq_len, _ = x.size()

        # Project input to Q, K, V
        Q = self.W_Q(x)  # (batch, seq_len, embed_dim)
        K = self.W_K(x)
        V = self.W_V(x)

        # Split into heads
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch, num_heads, seq_len, seq_len)
        if self.use_mask and mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len) for broadcasting
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        head_outputs = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, head_dim)

        # Concatenate heads and final projection
        concat = head_outputs.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        out = self.W_O(concat)  # (batch, seq_len, embed_dim)
        out = self.dropout(out)

        # Optionally, mean pool for context vector:
        context = out.mean(dim=1)  # (batch, embed_dim)
        return context

class TemporalCNN(nn.Module):
    """
    Enhanced TemporalCNN for sequential data.
    - Supports multiple convolutional layers.
    - Optional normalization and activation.
    - Optional residual connection.
    - Optional dropout.
    """
    def __init__(self, input_dim, output_dim, kernel_size=3, num_layers=2, dropout=0.1, use_residual=True):
        super().__init__()
        layers = []
        for i in range(num_layers):
            in_dim = input_dim if i == 0 else output_dim
            layers.append(nn.Conv1d(in_dim, output_dim, kernel_size=kernel_size, padding=kernel_size // 2))
            layers.append(nn.BatchNorm1d(output_dim))
            layers.append(nn.ReLU())
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
        self.conv_layers = nn.Sequential(*layers)
        self.use_residual = use_residual and (input_dim == output_dim)

    def forward(self, x):
        # x shape: (batch, seq_len, features)
        x_in = x
        x = x.permute(0, 2, 1)  # (batch, features, seq_len)
        out = self.conv_layers(x)  # (batch, output_dim, seq_len)
        out = out.permute(0, 2, 1)  # back to (batch, seq_len, output_dim)
        if self.use_residual:
            out = out + x_in
        return out

class FeatureEmbedding(nn.Module):
    """
    Enhanced FeatureEmbedding.
    - Can handle both categorical and continuous features.
    - Optional dropout.
    - Optional normalization.
    """
    def __init__(self, num_embeddings, embedding_dim, dropout=0.1, norm_type=None):
        super().__init__()
        self.embed = nn.Embedding(num_embeddings, embedding_dim)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()
        if norm_type == "layer":
            self.norm = nn.LayerNorm(embedding_dim)
        elif norm_type == "batch":
            self.norm = nn.BatchNorm1d(embedding_dim)
        else:
            self.norm = nn.Identity()

    def forward(self, x):
        # x: (batch, seq_len) or (batch,)
        out = self.embed(x)
        # If input is (batch, seq_len, embedding_dim), transpose for batch norm if needed
        if isinstance(self.norm, nn.BatchNorm1d):
            orig_shape = out.shape
            out = out.view(-1, out.shape[-1])  # (batch*seq_len, embedding_dim)
            out = self.norm(out)
            out = out.view(orig_shape)
        else:
            out = self.norm(out)
        out = self.dropout(out)
        return out

class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128, num_blocks=3, dropout=0.2,
                 use_temporal_cnn=False, cnn_dim=64, use_categorical=False,
                 cat_num_embeddings=0, cat_embed_dim=4, seq_len=10, max_action=1.0):
        super().__init__()
        self.use_temporal_cnn = use_temporal_cnn
        self.use_categorical = use_categorical
        self.max_action = max_action

        input_dim = state_dim
        if use_temporal_cnn:
            self.temporal_cnn = TemporalCNN(state_dim, cnn_dim)
            input_dim = cnn_dim
        if use_categorical and cat_num_embeddings > 0:
            self.cat_embed = FeatureEmbedding(cat_num_embeddings, cat_embed_dim)
            input_dim += cat_embed_dim

        self.blocks = nn.ModuleList([
            ResidualBiLSTMBlock(input_dim if i == 0 else hidden_dim * 2, hidden_dim, dropout)
            for i in range(num_blocks)
        ])
        self.attn = MultiheadAttention(hidden_dim * 2, num_heads=2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()
        )
        self.h0 = nn.Parameter(torch.zeros(num_blocks, 2, hidden_dim))
        self.c0 = nn.Parameter(torch.zeros(num_blocks, 2, hidden_dim))

    def forward(self, x, cat_x=None):
        # Accepts (batch, seq_len, features) or (seq_len, features)
        unsqueezed = False
        if x.dim() == 2:
            x = x.unsqueeze(0)
            unsqueezed = True
        if self.use_temporal_cnn:
            x = self.temporal_cnn(x)
        if self.use_categorical and cat_x is not None:
            if cat_x.dim() == 2:
                cat_x = cat_x.unsqueeze(0) if unsqueezed else cat_x
            cat_embed = self.cat_embed(cat_x)
            x = torch.cat([x, cat_embed], dim=2)
        batch_size = x.size(0)
        for i, block in enumerate(self.blocks):
            h0 = self.h0[i].unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            c0 = self.c0[i].unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            x = block(x, h0, c0)
        context = self.attn(x)
        out = self.fc(context)
        out = out * self.max_action
        if unsqueezed:
            out = out.squeeze(0)
        return out

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128, num_blocks=3, dropout=0.2,
                 use_temporal_cnn=False, cnn_dim=64, use_categorical=False,
                 cat_num_embeddings=0, cat_embed_dim=4, aux_output_dim=1, seq_len=10):
        super().__init__()
        self.use_temporal_cnn = use_temporal_cnn
        self.use_categorical = use_categorical
        input_dim = state_dim
        if use_temporal_cnn:
            self.temporal_cnn = TemporalCNN(state_dim, cnn_dim)
            input_dim = cnn_dim
        if use_categorical and cat_num_embeddings > 0:
            self.cat_embed = FeatureEmbedding(cat_num_embeddings, cat_embed_dim)
            input_dim += cat_embed_dim

        self.blocks = nn.ModuleList([
            ResidualBiLSTMBlock(input_dim if i == 0 else hidden_dim * 2, hidden_dim, dropout)
            for i in range(num_blocks)
        ])
        self.attn = MultiheadAttention(hidden_dim * 2, num_heads=2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2 + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 1)
        )
        self.aux_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, aux_output_dim)
        )
        self.h0 = nn.Parameter(torch.zeros(num_blocks, 2, hidden_dim))
        self.c0 = nn.Parameter(torch.zeros(num_blocks, 2, hidden_dim))

    def forward(self, x, a, cat_x=None):
        unsqueezed = False
        if x.dim() == 2:
            x = x.unsqueeze(0)
            unsqueezed = True
        if self.use_temporal_cnn:
            x = self.temporal_cnn(x)
        if self.use_categorical and cat_x is not None:
            if cat_x.dim() == 2:
                cat_x = cat_x.unsqueeze(0) if unsqueezed else cat_x
            cat_embed = self.cat_embed(cat_x)
            x = torch.cat([x, cat_embed], dim=2)
        batch_size = x.size(0)
        for i, block in enumerate(self.blocks):
            h0 = self.h0[i].unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            c0 = self.c0[i].unsqueeze(1).expand(-1, batch_size, -1).contiguous()
            x = block(x, h0, c0)
        context = self.attn(x)
        q_value = self.fc(torch.cat([context, a], dim=1))
        aux_out = self.aux_head(context)
        if unsqueezed:
            q_value = q_value.squeeze(0)
            aux_out = aux_out.squeeze(0)
        return q_value, aux_out

class ReplayBuffer:
    def __init__(self, max_size, input_shape, n_actions, device):
        self.max_size = max_size
        self.mem_cntr = 0
        self.device = device

        self.state_memory = np.zeros((max_size, *input_shape), dtype=np.float32)
        self.next_state_memory = np.zeros((max_size, *input_shape), dtype=np.float32)
        self.action_memory = np.zeros((max_size, n_actions), dtype=np.float32)
        self.reward_memory = np.zeros(max_size, dtype=np.float32)
        self.terminal_memory = np.zeros(max_size, dtype=bool)

    def store_transition(self, state, action, reward, next_state, done):
        index = self.mem_cntr % self.max_size
        self.state_memory[index] = state
        self.next_state_memory[index] = next_state
        self.action_memory[index] = action
        self.reward_memory[index] = reward
        self.terminal_memory[index] = done
        self.mem_cntr += 1

    def sample(self, batch_size):
        """Sample with automatic cleanup"""
        max_mem = min(self.mem_cntr, self.max_size)
        batch_indices = np.random.choice(max_mem, batch_size, replace=False)

        states = torch.tensor(self.state_memory[batch_indices], dtype=torch.float32).to(self.device)
        actions = torch.tensor(self.action_memory[batch_indices], dtype=torch.float32).to(self.device)
        rewards = torch.tensor(self.reward_memory[batch_indices], dtype=torch.float32).unsqueeze(-1).to(self.device)
        next_states = torch.tensor(self.next_state_memory[batch_indices], dtype=torch.float32).to(self.device)
        dones = torch.tensor(self.terminal_memory[batch_indices], dtype=torch.float32).unsqueeze(-1).to(self.device)

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return min(self.mem_cntr, self.max_size)

def scale_reward(r, min_r=-1, max_r=1):
    return np.clip(r, min_r, max_r)


        
        
class TD3Agent:
    def __init__(self, state_dim, action_dim, seq_len=20, max_action=1.0, min_action=-1.0,
                 # ========== FIX THESE ==========
                 discount=0.95,          # Changed: 0.99 â†’ 0.95 (less future focus)
                 tau=0.003,              # Changed: 0.01 â†’ 0.003 (slower target updates)
                 # ===============================
                 policy_noise=0.15,      # Slightly increased for exploration
                 noise_clip=0.3,         # Reduced from 0.5 (more stable)
                 policy_delay=3,         # Increased from 2 (more stable policy updates)
                 
                 batch_size=128,         # Increased from 64 (better gradients)
                 warmup=2500,            # Reduced from 5000 (learn faster)
                 
                 use_temporal_cnn=True,
                 cnn_dim=64,
                 use_categorical=False, 
                 cat_num_embeddings=0, 
                 cat_embed_dim=4, 
                 save_path='td3_ckpt.pth',
                 device='cpu'):
        self.device = device
        self.seq_len = seq_len
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        self.min_action = min_action
        self.discount = discount
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_delay = policy_delay
        self.batch_size = batch_size
        self.warmup = warmup
        self.use_categorical = use_categorical
        self.save_path = save_path
        self.train_step = 0

        # --- Exploration Settings ---
        self.epsilon = 0.75         # Start with 50% random actions
        self.epsilon_min = 0.35   # Don't decay below 10%
        self.epsilon_decay = 0.999 # Decay slowly

        # Increase OU noise magnitude for continuous actions
        # Reduce noise for more stable signals
        self.noise = OUNoise(
            self.action_dim, 
            mu=0.0, 
            theta=0.25,      # Slightly slower mean reversion
            sigma=0.2,       # â†“ Reduced from 0.5 â†’ 0.2 (less volatility)
            dt=0.1           # Slightly smaller time step
        )
        self.time_step = 0

        # Initialize actor and critic networks
        self.actor = Actor(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                           use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                           cat_embed_dim=cat_embed_dim, seq_len=seq_len, max_action=max_action).to(self.device)

        self.actor_target = Actor(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                                  use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                                  cat_embed_dim=cat_embed_dim, seq_len=seq_len, max_action=max_action).to(self.device)
        self.actor_target.load_state_dict(self.actor.state_dict())

        self.critic_1 = Critic(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                               use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                               cat_embed_dim=cat_embed_dim, seq_len=seq_len).to(self.device)
        self.critic_2 = Critic(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                               use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                               cat_embed_dim=cat_embed_dim, seq_len=seq_len).to(self.device)

        self.critic_target_1 = Critic(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                                      use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                                      cat_embed_dim=cat_embed_dim, seq_len=seq_len).to(self.device)
        self.critic_target_2 = Critic(state_dim, action_dim, use_temporal_cnn=use_temporal_cnn, cnn_dim=cnn_dim,
                                      use_categorical=use_categorical, cat_num_embeddings=cat_num_embeddings,
                                      cat_embed_dim=cat_embed_dim, seq_len=seq_len).to(self.device)

        self.critic_target_1.load_state_dict(self.critic_1.state_dict())
        self.critic_target_2.load_state_dict(self.critic_2.state_dict())

        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=  1e-6)
        self.critic_optimizer_1 = torch.optim.Adam(self.critic_1.parameters(), lr=1e-5)
        self.critic_optimizer_2 = torch.optim.Adam(self.critic_2.parameters(), lr=1e-5)

        self.critic_criterion = torch.nn.MSELoss()

        self.replay_buffer = ReplayBuffer(
            max_size=100_000,
            input_shape=(self.seq_len, 58),  # Changed from 34
            n_actions=self.action_dim,
            device=self.device
        )

        self.training_enabled = True # <<<<<<<< Toggle this to True to enable training

    def select_action(self, state_seq, cat_seq=None, add_noise=True, epsilon=None):
        eps = self.epsilon if epsilon is None else epsilon

        if np.random.rand() < eps:
            action_shape = (self.action_dim,)
            action = np.random.uniform(self.min_action, self.max_action, size=action_shape)
        else:
            if state_seq.ndim == 1:
                state_seq = state_seq[np.newaxis, :]
            if state_seq.shape[1] != self.seq_len:
                raise ValueError(f"Input seq_len mismatch: expected {self.seq_len}, got {state_seq.shape[1]}")
            state_seq = torch.tensor(state_seq, dtype=torch.float32).to(self.device)

            if self.use_categorical and cat_seq is not None:
                if cat_seq.ndim == 1:
                    cat_seq = cat_seq[np.newaxis, :]
                if cat_seq.shape[1] != self.seq_len:
                    raise ValueError(f"Categorical seq_len mismatch: expected {self.seq_len}, got {cat_seq.shape[1]}")
                cat_seq = torch.tensor(cat_seq, dtype=torch.long).to(self.device)
            else:
                cat_seq = None

            self.actor.eval()
            with torch.no_grad():
                action = self.actor(state_seq, cat_seq).cpu().numpy().flatten()
            self.actor.train()

            if self.time_step < self.warmup:
                action = np.random.uniform(self.min_action, self.max_action, size=action.shape)
            elif add_noise:
                noise = self.noise.sample()
                action = action + noise

            action = np.clip(action, self.min_action, self.max_action)

        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
        return action

    def train(self):
        """Training with aggressive GPU memory cleanup"""
        try:
            self.actor.train()
            self.critic.train()
            self.target_actor.train()
            self.target_critic.train()

            if self.replay_buffer.mem_cntr < self.batch_size:
                logger.debug(f"[{self.name}] Not enough samples to train yet.")
                return

            # ADAPTIVE BATCH SIZE based on available GPU memory
            effective_batch_size = self.batch_size

            if torch.cuda.is_available():
                total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
                allocated_mem = torch.cuda.memory_allocated(0) / 1e9
                free_mem = total_mem - allocated_mem

                # Dynamically reduce batch size if memory is tight
                if free_mem < 4.0:
                    effective_batch_size = 32
                    logger.warning(f"[{self.name}] Low GPU memory ({free_mem:.2f}GB free), reducing batch to 32")
                elif free_mem < 2.0:
                    effective_batch_size = 16
                    logger.warning(f"[{self.name}] Very low GPU memory ({free_mem:.2f}GB free), reducing batch to 16")

            # Sample from replay buffer
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(effective_batch_size)

            # Critic training
            with torch.no_grad():
                next_actions = self.target_actor(next_states)
                next_q, _ = self.target_critic(next_states, next_actions)
                q_target = rewards + self.gamma * next_q * (1 - dones)

            q_expected, _ = self.critic(states, actions)
            critic_loss = self.critic_criterion(q_expected, q_target)

            # CRITICAL: Use set_to_none=True to free memory
            self.critic_optimizer.zero_grad(set_to_none=True)
            critic_loss.backward()

            # Gradient clipping to prevent memory spikes
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)
            self.critic_optimizer.step()

            # Actor training
            actor_loss = -self.critic(states, self.actor(states))[0].mean()

            # CRITICAL: Use set_to_none=True
            self.actor_optimizer.zero_grad(set_to_none=True)
            actor_loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
            self.actor_optimizer.step()

            # Soft updates
            self.soft_update(self.target_actor, self.actor)
            self.soft_update(self.target_critic, self.critic)

            logger.critical(f"[{self.name}] Train step {self.train_step} | "
                           f"Critic Loss: {critic_loss.item():.4f} | "
                           f"Actor Loss: {actor_loss.item():.4f} | "
                           f"Batch: {effective_batch_size}")

            self.train_step += 1

            # AGGRESSIVE MEMORY CLEANUP
            # Delete all intermediate tensors
            del states, actions, rewards, next_states, dones
            del q_expected, q_target, critic_loss, actor_loss
            del next_actions, next_q

            # Force garbage collection of gradients
            if hasattr(self.actor, 'zero_grad'):
                self.actor.zero_grad(set_to_none=True)
            if hasattr(self.critic, 'zero_grad'):
                self.critic.zero_grad(set_to_none=True)

            # Force CUDA cache cleanup
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                logger.error(f"[{self.name}] CUDA OOM during training")

                # Emergency cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # Try to continue with smaller batch
                self.batch_size = max(16, self.batch_size // 2)
                logger.warning(f"[{self.name}] Reduced batch_size to {self.batch_size} after OOM")
            else:
                raise
        except Exception as e:
            logger.error(f"[{self.name}] Training failed: {e}")
            raise

    def store(self, state_seq, action, reward, next_state_seq, done):
        reward = scale_reward(reward)
        self.replay_buffer.store_transition(
            state_seq, action, reward, next_state_seq, done
        )
        self.time_step += 1

    def soft_update(self, target_net, source_net, tau):
        tau = float(tau)
        for target_param, param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)

    def save_models(self, path=None):
        if path is None:
            path = self.save_path
        torch.save({
            'actor': self.actor.state_dict(),
            'actor_target': self.actor_target.state_dict(),
            'critic_1': self.critic_1.state_dict(),
            'critic_2': self.critic_2.state_dict(),
            'critic_target_1': self.critic_target_1.state_dict(),
            'critic_target_2': self.critic_target_2.state_dict(),
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer_1': self.critic_optimizer_1.state_dict(),
            'critic_optimizer_2': self.critic_optimizer_2.state_dict(),
            'time_step': self.time_step,
            'train_step': self.train_step,
        }, path)
        logger.info(f"ğŸ’¾ [TD3] Checkpoint saved at {path} ğŸ‰")

    def load_models(self, path=None):
        if path is None:
            path = self.save_path
        if not os.path.exists(path):
            logger.warning(f"ğŸ” [TD3] No checkpoint found at {path} ğŸ˜¶")
            return False

        checkpoint = torch.load(path, map_location=self.device)
        self.actor.load_state_dict(checkpoint['actor'])
        self.actor_target.load_state_dict(checkpoint['actor_target'])
        self.critic_1.load_state_dict(checkpoint['critic_1'])
        self.critic_2.load_state_dict(checkpoint['critic_2'])
        self.critic_target_1.load_state_dict(checkpoint['critic_target_1'])
        self.critic_target_2.load_state_dict(checkpoint['critic_target_2'])
        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer_1.load_state_dict(checkpoint['critic_optimizer_1'])
        self.critic_optimizer_2.load_state_dict(checkpoint['critic_optimizer_2'])
        self.time_step = checkpoint.get('time_step', 0)
        self.train_step = checkpoint.get('train_step', 0)
        logger.info(f"ğŸ•¹ï¸ [TD3] Models loaded from {path} ğŸ”¥")
        return True

    # --- GCS Save/Load Methods ---
    def save_models_to_gcs(self, bucket, gcs_path, local_ckpt_path=None):
        # Save locally first
        if local_ckpt_path is None:
            local_ckpt_path = "/tmp/td3_ckpt.pth"
        self.save_models(local_ckpt_path)
        # Zip the checkpoint (optional for larger state)
        zip_path = local_ckpt_path + ".zip"
        with zipfile.ZipFile(zip_path, 'w') as zipf:
            zipf.write(local_ckpt_path, arcname=os.path.basename(local_ckpt_path))
        # Upload to GCS
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(zip_path)
        logger.info(f"ğŸ’¾ [TD3] Checkpoint uploaded to GCS: gs://{bucket.name}/{gcs_path}")

    def load_models_from_gcs(self, bucket, gcs_path, local_ckpt_path=None):
        # Download zip checkpoint from GCS
        if local_ckpt_path is None:
            local_ckpt_path = "/tmp/td3_ckpt.pth"
        zip_path = local_ckpt_path + ".zip"
        blob = bucket.blob(gcs_path)
        blob.download_to_filename(zip_path)
        # Unzip locally
        with zipfile.ZipFile(zip_path, 'r') as zipf:
            zipf.extractall(os.path.dirname(local_ckpt_path))
        # Load from extracted checkpoint
        return self.load_models(local_ckpt_path)

    # --- AutosaveManager hook ---
    def save_state(self, bucket=None, gcs_path=None, local_ckpt_path=None):
        """Save state locally and (if bucket/gcs_path given) to GCS."""
        self.save_models(local_ckpt_path)
        if bucket and gcs_path:
            self.save_models_to_gcs(bucket, gcs_path, local_ckpt_path)

    def load_state(self, bucket=None, gcs_path=None, local_ckpt_path=None):
        """Load state locally, or from GCS if bucket/gcs_path given."""
        if bucket and gcs_path:
            return self.load_models_from_gcs(bucket, gcs_path, local_ckpt_path)
        else:
            return self.load_models(local_ckpt_path)

    def act(self, state_seq, cat_seq=None, add_noise=True):
        """Alias for select_action(), for compatibility."""
        return self.select_action(state_seq, cat_seq, add_noise)

    def act(self, state_seq, cat_seq=None, add_noise=True):
        """Alias for select_action(), for compatibility."""
        return self.select_action(state_seq, cat_seq, add_noise)
    # Diagnostic: Plot action output histogram for diversity check

# --- StandardScaler (for features, from code2) ---
class StandardScaler:
    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        self.mean_ = np.mean(X, axis=0)
        self.scale_ = np.std(X, axis=0)
        self.scale_[self.scale_ == 0] = 1.0

    def transform(self, X):
        if self.mean_ is None or self.scale_ is None:
            logger.warning("Scaler has not been fitted yet. Returning unscaled data.")
            return X
        return (X - self.mean_) / self.scale_

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)

def interpret_td3_action(action_continuous, threshold=0.1):
    a0, a1 = action_continuous
    return (0, "BUY") if a0 > a1 else (1, "SELL")

class TimeframeAgent:
    def __init__(self, name, seq_len, state_dim, action_dim, base_path="/tmp",
                 device=None, gcs_bucket=None, gcs_path=None):
        self.name = name
        self.seq_len = seq_len
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.local_save_dir = os.path.join(base_path, f"{name}_state")
        ensure_dir(self.local_save_dir)
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path

        self.EXPECTED_FEATURE_COLS = [
            # Core Technical (18)
            'log_return', 'rolling_mean_5', 'rolling_std_5', 'zscore_5',
            'rsi_14', 'macd', 'macd_signal', 'macd_hist',
            'atr', 'cdf_value', 'cdf_slope', 'cdf_diff',
            'volatility_quantile_90', 'volatility_ratio', 'entropy_50',
            'autocorr_3', 'momentum_10', 'volume_change_rate', 'volume_zscore',

            # Price Derivatives (12)
            'price_vel', 'price_acc', 'price_jrk',
            'price_vel_mean', 'price_acc_mean', 'price_jrk_mean',
            'price_vel_std', 'price_acc_std', 'price_jrk_std',
            'price_vel_skew', 'price_acc_skew', 'price_jrk_skew',
            'price_vel_kurtosis', 'price_acc_kurtosis', 'price_jrk_kurtosis','close_scaled',

            # Additional Technical (8)
            'ma10', 'ma20', 'std20',
            'bollinger_upper', 'bollinger_lower', 'bollinger_width', 'bollinger_position',

            # Candlestick Patterns (9)
            'gravestone_doji', 'four_price_doji', 'doji', 'spinning_top',
            'bullish_candle', 'bearish_candle', 'dragonfly_candle',
            'spinning_top_bearish_followup', 'bullish_then_dragonfly',

            # Support/Resistance (7)
            'distance_to_nearest_support', 'distance_to_nearest_resistance',
            'near_support', 'near_resistance', 'distance_to_stop_loss',
            'support_strength', 'resistance_strength'
        ]

                # Verify count
        assert len(self.EXPECTED_FEATURE_COLS) == 58, f"Expected 58 features, got {len(self.EXPECTED_FEATURE_COLS)}"
        self.state_dim = len(self.EXPECTED_FEATURE_COLS)
        self.action_dim = 2

        self.actor_path = os.path.join(self.local_save_dir, "actor.pth")
        self.critic_path = os.path.join(self.local_save_dir, "critic.pth")
        self.target_actor_path = os.path.join(self.local_save_dir, "target_actor.pth")
        self.target_critic_path = os.path.join(self.local_save_dir, "target_critic.pth")
        self.actor_optimizer_path = os.path.join(self.local_save_dir, "actor_optimizer.pth")
        self.critic_optimizer_path = os.path.join(self.local_save_dir, "critic_optimizer.pth")
        self.buffer_path = os.path.join(self.local_save_dir, "replay_buffer.pkl")

        self.latest_features = None
        self.feature_history = deque(maxlen=self.seq_len)
        self.recent_q_values = deque(maxlen=25)
        self.train_step = 0
        self.gamma = 0.95 
        self.tau = 0.005  
        self.batch_size = 64
        # ADD THESE LINES:
        self.exit_price_history = deque(maxlen=1000)
        self.supervised_learning_stats = {
            'correct_predictions': 0,
            'incorrect_predictions': 0,
            'total_supervised_samples': 0
        }

        loaded = self.load_state(
            bucket=self.gcs_bucket,
            gcs_path=self.gcs_path
        )
        if not loaded:
            logger.warning(f"[{self.name}] No saved state found, initializing new models and saving.")
            self._init_new_models()
            self.save_state()
        logger.info(f"ğŸ¦¾ [{self.name}] Agent initialized (seq_len={self.seq_len}, state_dim={self.state_dim}, action_dim={self.action_dim})")

    def load_state(self, bucket=None, gcs_path=None):
        loaded_components = []
        if bucket and gcs_path:
            tmp_zip = unique_tmp_path(self.name)
            try:
                blob = bucket.blob(gcs_path)
                if blob.exists():
                    blob.download_to_filename(tmp_zip)
                    safe_unzip(tmp_zip, self.local_save_dir)
                    logger.info(f"[{self.name}] GCS state unzipped to {self.local_save_dir}")
                else:
                    logger.warning(f"[{self.name}] GCS blob {gcs_path} does not exist.")
            except Exception as e:
                logger.error(f"[{self.name}] GCS load failed: {e}")

        if not hasattr(self, "actor"):
            self._init_new_models()

        if load_torch_state_dict(self.actor, self.actor_path, self.device):
            loaded_components.append("actor")
        if load_torch_state_dict(self.critic, self.critic_path, self.device):
            loaded_components.append("critic")
        if load_torch_state_dict(self.target_actor, self.target_actor_path, self.device):
            loaded_components.append("target_actor")
        if load_torch_state_dict(self.target_critic, self.target_critic_path, self.device):
            loaded_components.append("target_critic")
        if load_optimizer_state_dict(self.actor_optimizer, self.actor_optimizer_path, self.device):
            loaded_components.append("actor_optimizer")
        if load_optimizer_state_dict(self.critic_optimizer, self.critic_optimizer_path, self.device):
            loaded_components.append("critic_optimizer")
        if load_pickle_buffer(self.replay_buffer, self.buffer_path):
            loaded_components.append("replay_buffer")

        if loaded_components:
            logger.info(f"[{self.name}] Successfully loaded: {', '.join(loaded_components)}")
            return True
        else:
            logger.warning(f"[{self.name}] No components loaded.")
            return False

    def store_experience_supervised(self, state, action, reward, next_state,
                                    done=False, exit_price=None, correct_action=None):
        """Enhanced experience storage with supervised learning metadata"""
        try:
            current_state_sequence = self.get_current_state_sequence()
            next_state_sequence = self.get_current_state_sequence()

            state_tensor = torch.tensor(current_state_sequence,
                                       dtype=torch.float32).to(self.device)

            raw_action = self.actor(state_tensor)
            if isinstance(raw_action, torch.Tensor):
                raw_action = raw_action.cpu().detach().numpy().flatten()

            # Store in replay buffer
            self.replay_buffer.store_transition(
                current_state_sequence, raw_action, reward,
                next_state_sequence, done
            )

            # Track supervised learning metadata
            if exit_price is not None and correct_action is not None:
                self.exit_price_history.append({
                    'timestamp': time.time(),
                    'exit_price': exit_price,
                    'reward': reward,
                    'taken_action': action,
                    'correct_action': correct_action,
                    'was_correct': (action == correct_action)
                })

                self.supervised_learning_stats['total_supervised_samples'] += 1
                if action == correct_action:
                    self.supervised_learning_stats['correct_predictions'] += 1
                else:
                    self.supervised_learning_stats['incorrect_predictions'] += 1

                logger.debug(f"[{self.name}] Supervised: exit={exit_price:.5f}, "
                            f"correct={correct_action}, took={action}, "
                            f"accuracy={(self.supervised_learning_stats['correct_predictions'] / self.supervised_learning_stats['total_supervised_samples'] * 100):.1f}%")

        except Exception as e:
            logger.error(f"[{self.name}] Failed to store supervised experience: {e}")

    def get_supervised_learning_stats(self):
        """Get supervised learning performance statistics"""
        if self.supervised_learning_stats['total_supervised_samples'] == 0:
            return None

        stats = self.supervised_learning_stats.copy()
        stats['accuracy'] = (
            stats['correct_predictions'] / stats['total_supervised_samples'] * 100
        )

        if self.exit_price_history:
            exit_prices = [e['exit_price'] for e in self.exit_price_history]
            rewards = [e['reward'] for e in self.exit_price_history]

            stats['avg_exit_price'] = np.mean(exit_prices)
            stats['std_exit_price'] = np.std(exit_prices)
            stats['avg_reward'] = np.mean(rewards)
            stats['profitable_trades'] = sum(1 for r in rewards if r > 0)
            stats['total_trades'] = len(rewards)
            stats['win_rate'] = sum(1 for r in rewards if r > 0) / len(rewards)

        return stats

    def _init_new_models(self):
        self.actor = Actor(self.state_dim, self.action_dim).to(self.device)
        self.critic = Critic(self.state_dim, self.action_dim).to(self.device)
        self.target_actor = Actor(self.state_dim, self.action_dim).to(self.device)
        self.target_critic = Critic(self.state_dim, self.action_dim).to(self.device)
        import torch.optim as optim
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-6)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-5)
        self.critic_criterion = torch.nn.MSELoss()
        self.replay_buffer = ReplayBuffer(
            max_size=100_000,
            input_shape=(self.seq_len, self.state_dim),
            n_actions=self.action_dim,
            device=self.device
        )
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic.load_state_dict(self.critic.state_dict())

    def save_state(self, bucket=None, gcs_path=None, local_ckpt_path=None):
        try:
            torch.save(self.actor.state_dict(), self.actor_path)
            torch.save(self.critic.state_dict(), self.critic_path)
            torch.save(self.target_actor.state_dict(), self.target_actor_path)
            torch.save(self.target_critic.state_dict(), self.target_critic_path)
            torch.save(self.actor_optimizer.state_dict(), self.actor_optimizer_path)
            torch.save(self.critic_optimizer.state_dict(), self.critic_optimizer_path)
            with open(self.buffer_path, "wb") as f:
                pickle.dump({
                    'mem_cntr': self.replay_buffer.mem_cntr,
                    'state_memory': self.replay_buffer.state_memory,
                    'action_memory': self.replay_buffer.action_memory,
                    'reward_memory': self.replay_buffer.reward_memory,
                    'next_state_memory': getattr(self.replay_buffer, 'next_state_memory', None),
                    'terminal_memory': self.replay_buffer.terminal_memory,
                }, f)
            logger.info(f"[{self.name}] Agent state successfully saved to {self.local_save_dir}")

            # GCS integration (optional)
            if bucket and gcs_path:
                import shutil
                zip_path = local_ckpt_path or f"/tmp/{self.name}_agent_state.zip"
                # Compress the local directory
                shutil.make_archive(zip_path.replace('.zip', ''), 'zip', self.local_save_dir)
                blob = bucket.blob(gcs_path)
                blob.upload_from_filename(zip_path)
                logger.info(f"[{self.name}] Agent state uploaded to GCS: {gcs_path}")

        except Exception as e:
            logger.error(f"[{self.name}] Agent save failed: {e}\n{traceback.format_exc()}")

    def save_to_gcs(self):
        self.save_state()
        zip_path = f"/tmp/{self.name}_state.zip"
        compress_dir_to_zip(self.local_save_dir, zip_path)
        upload_zip_to_gcs(zip_path, f"agents/{self.name}_state.zip")

    def load_from_gcs(self):
        zip_path = f"/tmp/{self.name}_state.zip"
        download_zip_from_gcs(f"agents/{self.name}_state.zip", zip_path)
        decompress_zip_to_dir(zip_path, self.local_save_dir)
        self.load_state()

    def update_features(self, features_dict):
        """Cache last valid feature and skip zero/NaN inputs."""
        try:
            if features_dict is None:
                raise ValueError("Incoming feature_dict is None")

            clean_features = {
                col: float(features_dict.get(col, 0.0)) for col in self.EXPECTED_FEATURE_COLS
            }
            feature_array = np.array(list(clean_features.values()), dtype=np.float32)
            valid = np.sum(np.abs(feature_array)) > 1e-4

            if valid:
                self.feature_history.append(feature_array)
                self.last_valid_feature = feature_array
                logger.debug(f"[{self.name}] Valid feature appended.")
            else:
                logger.warning(f"[{self.name}] Skipping invalid (zero) feature input.")
                # keep last_valid_feature untouched
        except Exception as e:
            logger.error(f"[{self.name}] update_features failed: {e}")

    def get_current_state_sequence(self):
        """
        Return a sequence of features for the agent without zero-padding.
        Uses the last valid feature to fill missing history if necessary.
        """

        # 1. Absolute start: no data in history or cache
        if self.last_valid_feature is None and len(self.feature_history) == 0:
            # Use a single copy of initial zeros just for sizing (cannot avoid this completely on first call)
            initial_feature = np.zeros(self.state_dim, dtype=np.float32)
            initial_sequence = np.array([initial_feature] * self.seq_len, dtype=np.float32)
            logger.warning(f"[{self.name}] No state in cache - returning initial sequence (first-time startup).")
            return initial_sequence

        # 2. If last_valid_feature is None but history exists, pick the first history entry
        if self.last_valid_feature is None and len(self.feature_history) > 0:
            self.last_valid_feature = self.feature_history[0]

        # 3. Full history available
        if len(self.feature_history) >= self.seq_len:
            return np.array(list(self.feature_history)[-self.seq_len:], dtype=np.float32)

        # 4. Incomplete history: pad with last valid feature
        padding_needed = self.seq_len - len(self.feature_history)
        padding_feature = self.last_valid_feature

        # Instead of zeros, repeat last valid feature
        padded_history = [padding_feature] * padding_needed + list(self.feature_history)

        logger.info(f"[{self.name}] Using last valid feature to fill {padding_needed} missing steps (no zero-padding).")
        return np.array(padded_history, dtype=np.float32)

    def select_action(self, state, add_noise=True):
        self.actor.eval()
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)
            if len(state_tensor.shape) == 1:
                state_tensor = state_tensor.unsqueeze(0)
            action = self.actor(state_tensor)
            if add_noise:
                noise = torch.randn_like(action) * 0.1
                action = action + noise
            action = torch.clamp(action, -1.0, 1.0)
        self.actor.train()
        return action.cpu().numpy().flatten()

    # === PASTE THIS - REPLACE ENTIRE predict METHOD ===
    def predict(self, add_noise=True):
        """Prediction - returns [Q_BUY, Q_SELL] only (2 Q-values)"""
        try:
            if len(self.feature_history) == 0:
                logger.warning(f"[{self.name}] No feature history for prediction")
                return np.array([0.0, 0.0])

            current_state_sequence = self.get_current_state_sequence()
            scaled_state_sequence = current_state_sequence.reshape(self.seq_len, -1)

            state_tensor = torch.tensor(scaled_state_sequence, dtype=torch.float32).unsqueeze(0).to(self.device)

            q_values = []
            self.critic.eval()

            with torch.no_grad():
                # Only compute Q-values for BUY and SELL (2 actions)
                for discrete_action in range(2):
                    action_onehot = np.zeros(2, dtype=np.float32)
                    action_onehot[discrete_action] = 1.0
                    action_tensor = torch.tensor(action_onehot, dtype=torch.float32).unsqueeze(0).to(self.device)
                    q, _ = self.critic(state_tensor, action_tensor)
                    q_values.append(q.item())

                    del action_tensor, q

            self.critic.train()

            q_values_for_return = np.array(q_values)
            self.recent_q_values.append(q_values_for_return)

            del state_tensor
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return q_values_for_return

        except Exception as e:
            logger.error(f"[{self.name}] Prediction error: {e}")

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return np.array([0.0, 0.0])
    # === PASTE THIS - REPLACE ENTIRE get_discrete_action METHOD ===
    def get_discrete_action(self, q_values=None):
        """
        Select action with highest Q-value.
        Returns: 0 for BUY, 1 for SELL (NO HOLD).
        """
        if q_values is None:
            q_values = self.predict(add_noise=False)

        # Simple argmax - only returns 0 or 1
        predicted_action_idx = int(np.argmax(q_values[:2]))

        # Log for debugging
        logger.debug(f"[{self.name}] Q-values: {q_values}, Action: {ACTION_MAP[predicted_action_idx]}")

        return predicted_action_idx

    # ... (rest of your class)

    def store_experience(self, state, action, reward, next_state, done=False):
        try:
            # Prepare state sequences
            current_state_sequence = self.get_current_state_sequence()
            next_state_sequence = self.get_current_state_sequence()  # update or pad as needed

            # Get raw continuous output from the actor/model
            state_tensor = torch.tensor(current_state_sequence, dtype=torch.float32).to(self.device)
            raw_action = self.actor(state_tensor)
            if isinstance(raw_action, torch.Tensor):
                raw_action = raw_action.cpu().detach().numpy().flatten()

            # Print the raw action for debugging

            # Store the raw action in replay buffer (no clipping/rounding)
            self.replay_buffer.store_transition(
                current_state_sequence, raw_action, reward, next_state_sequence, done
            )

            # For actual trade execution, discretize if needed
            discrete_action = np.argmax(raw_action)  # or your own logic
            # Use discrete_action for trading, logging, etc.

        except Exception as e:
            logger.error(f"[{self.name}] Failed to store experience: {e}")

    def soft_update(self, target_net, source_net, tau=None):
        tau = float(self.tau if tau is None else tau)
        for target_param, param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)

    def train(self):
        """
        BATCH-AWARE: Training now controlled by batch processor
        Individual training calls are logged but may be batched
        """
        # Only proceed if not in batch mode or if explicitly called for batch training
        if hasattr(self, '_batch_training_active') and self._batch_training_active:
            # This is a batch training call - proceed normally
            pass
        else:
            # Individual training call - log for debugging
            logger.debug(f"[{self.name}] Individual training call - batch processor should handle this")

        try:
            self.actor.train()
            self.critic.train()
            self.target_actor.train()
            self.target_critic.train()

            if self.replay_buffer.mem_cntr < self.batch_size:
                logger.debug(f"[{self.name}] Not enough samples to train yet.")
                return

            states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
            with torch.no_grad():
                next_actions = self.target_actor(next_states)
                next_q, _ = self.target_critic(next_states, next_actions)
                q_target = rewards + self.gamma * next_q * (1 - dones)

            q_expected, _ = self.critic(states, actions)
            critic_loss = self.critic_criterion(q_expected, q_target)

            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()

            actor_loss = -self.critic(states, self.actor(states))[0].mean()
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            self.soft_update(self.target_actor, self.actor)
            self.soft_update(self.target_critic, self.critic)

            logger.critical(f"[{self.name}] Train step {self.train_step} | Critic Loss: {critic_loss.item():.4f} | Actor Loss: {actor_loss.item():.4f}")
            self.train_step += 1

        except Exception as e:
            logger.error(f"[{self.name}] Training failed: {e}")

    def save(self):
        try:
            torch.save(self.actor.state_dict(), self.actor_path)
            torch.save(self.critic.state_dict(), self.critic_path)
            torch.save(self.target_actor.state_dict(), self.target_actor_path)
            torch.save(self.target_critic.state_dict(), self.target_critic_path)
            with open(self.buffer_path, "wb") as f:
                pickle.dump({
                    'mem_cntr': self.replay_buffer.mem_cntr,
                    'state_memory': self.replay_buffer.state_memory,
                    'action_memory': self.replay_buffer.action_memory,
                    'reward_memory': self.replay_buffer.reward_memory,
                    'new_state_memory': self.replay_buffer.new_state_memory,
                    'terminal_memory': self.replay_buffer.terminal_memory,
                }, f)
            logger.info(f"[{self.name}] âœ… Agent saved.")
        except Exception as e:
            logger.error(f"[{self.name}] âŒ Failed to save agent: {e}")

    def _load_models(self):
        if os.path.exists(self.actor_path):
            self.actor.load_state_dict(torch.load(self.actor_path, map_location=self.device))
        if os.path.exists(self.critic_path):
            self.critic.load_state_dict(torch.load(self.critic_path, map_location=self.device))
        if os.path.exists(self.target_actor_path):
            self.target_actor.load_state_dict(torch.load(self.target_actor_path, map_location=self.device))
        if os.path.exists(self.target_critic_path):
            self.target_critic.load_state_dict(torch.load(self.target_critic_path, map_location=self.device))
        if os.path.exists(self.actor_optimizer_path):
            self.actor_optimizer.load_state_dict(torch.load(self.actor_optimizer_path, map_location=self.device))
        if os.path.exists(self.critic_optimizer_path):
            self.critic_optimizer.load_state_dict(torch.load(self.critic_optimizer_path, map_location=self.device))

    def _load_replay_buffer(self):
        if os.path.exists(self.buffer_path):
            with open(self.buffer_path, "rb") as f:
                data = pickle.load(f)
            mem_cntr = data.get('mem_cntr', 0)
            self.replay_buffer.mem_cntr = mem_cntr
            for key in ['state_memory', 'action_memory', 'reward_memory', 'next_state_memory', 'terminal_memory']:
                if key in data and hasattr(self.replay_buffer, key):
                    src = np.array(data[key])
                    dst = getattr(self.replay_buffer, key)
                    if src.shape[0] > mem_cntr:
                        src = src[:mem_cntr]
                    dst[:mem_cntr] = src

    def autosave_loop(self):
        while True:
            self.save()
            time.sleep(3000)

# === Missing Replay Buffer Implementation ===

class MetaModelExperienceBuffer:
    def __init__(self, max_size=500000):
        self.experiences = deque(maxlen=max_size)

    def add_experience(self,
                       q_values_dict,
                       actions_dict,
                       state,
                       voting_pred,
                       close_price,
                       distance_to_nearest_support=None,
                       distance_to_nearest_resistance=None,
                       near_support=None,
                       near_resistance=None,
                       distance_to_stop_loss=None,
                       support_strength=None,
                       resistance_strength=None,
                       reward=None,
                       **kwargs):
        """
        Stores a meta-model experience with optional extras and ignores unrecognized kwargs.
        """
        experience = {
            "q_values_dict": q_values_dict,
            "actions_dict": actions_dict,
            "state": state,
            "voting_pred": voting_pred,
            "close_price": close_price,
            "distance_to_nearest_support": distance_to_nearest_support,
            "distance_to_nearest_resistance": distance_to_nearest_resistance,
            "near_support": near_support,
            "near_resistance": near_resistance,
            "distance_to_stop_loss": distance_to_stop_loss,
            "support_strength": support_strength,
            "resistance_strength": resistance_strength,
            "reward": reward
        }

        # Optionally include any additional keys if present in kwargs
        experience.update(kwargs)

        self.experiences.append(experience)

    def get_all_experiences(self):
        return list(self.experiences)

    def size(self):
        return len(self.experiences)

    def clear(self):
        self.experiences.clear()

class CandleBuilder:
    def __init__(self, interval_sec=5):
        self.interval_sec = interval_sec
        self.reset()

    def reset(self):
        self.prices = []
        self.timestamps = []
        self.open = None
        self.high = float('-inf')
        self.low = float('inf')
        self.close = None
        self.start_time = None

    def add_price(self, price, timestamp):
        if self.start_time is None:
            self.start_time = timestamp
        self.timestamps.append(timestamp)

        if self.open is None:
            self.open = price
        self.high = max(self.high, price)
        self.low = min(self.low, price)
        self.close = price
        self.prices.append(price)

    def is_complete(self, current_time):
        return (current_time - self.start_time) >= self.interval_sec if self.start_time else False

    def get_candle(self):
        return {
            'open': self.open,
            'high': self.high,
            'low': self.low,
            'close': self.close,
            'start_time': self.start_time,
            'end_time': self.timestamps[-1] if self.timestamps else None
        }

    def reset_and_get(self):
        candle = self.get_candle()
        self.reset()
        return candle

class DynamicThreshold:
    def __init__(self, window=50, min_threshold=0.05, max_threshold=0.5):
        self.window = window
        self.recent_means = []
        self.min_threshold = min_threshold
        self.max_threshold = max_threshold

    def update(self, new_mean):
        self.recent_means.append(new_mean)
        if len(self.recent_means) > self.window:
            self.recent_means.pop(0)

    def get_threshold(self):
        if not self.recent_means:
            return self.min_threshold
        # Example: Use the 60th percentile as a dynamic threshold
        percentile = np.percentile(self.recent_means, 60)
        # Clamp to min/max
        return min(max(percentile, self.min_threshold), self.max_threshold)

# ALTERNATIVE: High-performance batch processing for maximum throughput
# ============================================================
# ğŸ§© FULLY ENHANCED REWARD PROCESSING PIPELINE
# ============================================================
# REMOVED DUPLICATE: from dataclasses import dataclass
from typing import Any, Dict, List
# REMOVED DUPLICATE: from collections import deque
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import asyncio
# REMOVED DUPLICATE: import time

# ==========================================================
# ğŸ§© Define Experience structure
# ==========================================================

# ============================================================
# âœ… UNIVERSAL AGENT CALL WRAPPER
#    (handles actor/policy/q_net/forward for any agent type)
# ============================================================

logger = logging.getLogger("QuantumSystemLogger")

def call_agent_policy(
        agent,
        z_shared: Optional[torch.Tensor],
        timeframe_states: Optional[Dict[str, torch.Tensor]] = None,
        return_critic: bool = False
) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """
    ENHANCED VERSION [FIX #1 APPLIED]: Comprehensive error logging and validation.

    Returns: (q_values, critic_value_or_None)
    """
    agent_name = type(agent).__name__
    agent_label = getattr(agent, 'name', agent_name)

    logger.debug(f"[call_agent_policy] Calling policy for {agent_label} ({agent_name})")

    # Validation
    if z_shared is None:
        error_msg = f"[call_agent_policy] z_shared is None for {agent_label}"
        logger.error(error_msg)
        raise ValueError(error_msg)

    if not isinstance(z_shared, torch.Tensor):
        error_msg = f"[call_agent_policy] z_shared must be tensor, got {type(z_shared)}"
        logger.error(error_msg)
        raise TypeError(error_msg)

    logger.debug(f"  z_shared: shape={z_shared.shape}, dtype={z_shared.dtype}, device={z_shared.device}")

    if timeframe_states:
        logger.debug(f"  timeframe_states: {len(timeframe_states)} timeframes")
        for tf, state in timeframe_states.items():
            if state is not None:
                logger.debug(f"    {tf}: shape={state.shape}, device={state.device}")

    # Get device
    try:
        device = next(agent.parameters()).device
        logger.debug(f"  agent device: {device}")
    except Exception as e:
        logger.warning(f"  Could not determine device: {e}, using CPU")
        device = torch.device("cpu")

    # Move z_shared to device
    if z_shared.device != device:
        logger.debug(f"  Moving z_shared from {z_shared.device} to {device}")
        z_shared = z_shared.to(device)

    # Process timeframe states
    if timeframe_states:
        safe_tf = {}
        for k, v in timeframe_states.items():
            if v is None:
                logger.warning(f"  Skipping None timeframe state: {k}")
                continue
            if not isinstance(v, torch.Tensor):
                logger.debug(f"  Converting {k} to tensor")
                v = torch.tensor(v, dtype=torch.float32)
            if v.device != device:
                logger.debug(f"  Moving {k} from {v.device} to {device}")
                v = v.to(device)
            if v.dim() == 1:
                v = v.unsqueeze(0)
            elif v.dim() == 3 and v.size(1) == 1:
                v = v.squeeze(1)
            safe_tf[k] = v
        timeframe_states = safe_tf
        logger.debug(f"  Processed: {len(timeframe_states)} valid timeframes")

    # Try different methods
    method_order = [
        "get_q_values",
        "predict_with_critic",
        "actor",
        "policy",
        "q_net",
        "net",
        "forward",
        "__call__"
    ]

    errors_encountered = []

    for method_name in method_order:
        if not hasattr(agent, method_name):
            logger.debug(f"  Method '{method_name}' not found")
            continue

        fn = getattr(agent, method_name)
        if not callable(fn):
            logger.debug(f"  Method '{method_name}' not callable")
            continue

        logger.debug(f"  Trying method: {method_name}")

        try:
            if timeframe_states is not None and z_shared is not None:
                out = fn(timeframe_states, z_shared)
            elif z_shared is not None:
                out = fn(z_shared)
            elif timeframe_states is not None:
                out = fn(timeframe_states)
            else:
                out = fn()

            if isinstance(out, tuple):
                q_values = out[0]
                critic_value = out[1] if len(out) > 1 else None
                return q_values, critic_value
            else:
                return out, None

        except Exception as e:
            error_msg = f"{method_name}: {str(e)}"
            errors_encountered.append(error_msg)
            if len(errors_encountered) <= 3:
                import traceback
                logger.debug(f"    Traceback: {traceback.format_exc()}")
            continue

    # All methods failed
    error_details = "\n".join([f"    - {err}" for err in errors_encountered])
    full_error = (
        f"[call_agent_policy] Could not find callable policy method for {agent_name}.\n"
        f"  Attempted {len(errors_encountered)} methods with errors:\n{error_details}\n"
        f"  Input context:\n"
        f"    z_shared: shape={z_shared.shape}, device={z_shared.device}\n"
        f"    timeframe_states: {list(timeframe_states.keys()) if timeframe_states else 'None/Empty'}"
    )
    logger.error(full_error)
    raise AttributeError(full_error)

# ==========================================================
# âš™ï¸ RewardBatchProcessor: Validates, buffers, and processes rewards
# ==========================================================
class RewardBatchProcessor:
    """
    Handles all batch data extraction, validation, and transformation
    with extensive error checking and logging.

    FIXED VERSION - All missing methods implemented
    """

    def __init__(self, system, state_dim: int = 58, action_dim: int = 2,
                 batch_size: int = 64, flush_interval: float = 1.0, exp_manager=None):
        """
        Initialize RewardBatchProcessor with system reference.

        Args:
            system: Reference to IntegratedSignalSystem
            state_dim: Dimension of state vectors
            action_dim: Dimension of action vectors
            batch_size: Number of experiences per batch
            flush_interval: Seconds between auto-flushes
        """
        self.system = system
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.batch_size = batch_size
        self.flush_interval = flush_interval

        # Statistics tracking
        self.stats = {
            'batches_processed': 0,
            'validation_failures': 0,
            'shape_corrections': 0,
            'processed': 0,
            'skipped': 0,
            'errors': 0
        }

        # Reward batching queue
        self.queue = deque()
        self._flush_task = None
        self._running = False

        # Set exp_manager from parameter or system
        if exp_manager is not None:
            self.exp_manager = exp_manager
            logger.info("âœ… exp_manager provided during initialization")
        elif hasattr(system, 'exp_manager') and system.exp_manager is not None:
            self.exp_manager = system.exp_manager
            logger.info("âœ… exp_manager retrieved from system")
        else:
            self.exp_manager = None
            logger.warning("âš ï¸ exp_manager not available - must be set before processing")

        logger.info(f"âœ… RewardBatchProcessor initialized: batch_size={batch_size}, flush_interval={flush_interval}s")

        # In RewardBatchProcessor.__init__, around line 7151
        logger.critical(f"Training will use system: {id(self.system)}")
        if hasattr(self.system, 'agents'):
            logger.critical(f"  Agents dict: {id(self.system.agents)}")
            logger.critical(f"  Agent types: {[type(a).__name__ for a in self.system.agents.values()]}")

    # ==========================================================
    # LIFECYCLE METHODS
    # ==========================================================

    async def start(self):
        """Start the batch processor and auto-flush loop"""
        try:
            logger.info("RewardBatchProcessor starting...")
            self._running = True
            self._flush_task = asyncio.create_task(self._auto_flush_loop())
            logger.info("âœ… RewardBatchProcessor started successfully")
        except Exception as e:
            logger.error(f"Failed to start RewardBatchProcessor: {e}")
            raise

    def stop(self):
        """Stop the batch processor and cancel background tasks"""
        try:
            logger.info("Stopping RewardBatchProcessor...")
            self._running = False
            if self._flush_task and not self._flush_task.done():
                self._flush_task.cancel()
                logger.info("- Flush task cancelled")
            if len(self.queue) > 0:
                logger.info(f"- {len(self.queue)} items remaining in queue")
            logger.info("âœ… RewardBatchProcessor stopped")
        except Exception as e:
            logger.error(f"Error stopping RewardBatchProcessor: {e}")

    def get_stats(self):
        """Return statistics about batch processing"""
        return self.stats.copy()

    # ==========================================================
    # REWARD QUEUEING AND BATCH FLUSHING
    # ==========================================================

    async def queue_reward(self, reward_data, reward=None, next_state=None, done=False):
        """
        Queue a reward message for batched supervised training.

        Accepts two formats:
        1. Dict format: queue_reward({"signal_key": ..., "reward": ..., ...})
        2. Separate params: queue_reward(signal_key, reward, next_state, done)
        """
        try:
            # Handle dict format
            if isinstance(reward_data, dict):
                # Ensure agent_multipliers is preserved
                if 'agent_multipliers' not in reward_data:
                    reward_data['agent_multipliers'] = {}
                self.queue.append(reward_data)
            # Handle separate parameters
            else:
                signal_key = reward_data
                self.queue.append({
                    "signal_key": signal_key,
                    "reward": reward,
                    "next_state": next_state,
                    "done": done,
                    "agent_multipliers": {}
                })
            # Auto-flush if batch is full
            if len(self.queue) >= self.batch_size:
                asyncio.create_task(self.flush())
        except Exception as e:
            logger.error(f"[RewardBatchProcessor.queue_reward] Error queueing reward: {e}")

    async def add_reward(self, signal_key: str, reward: float, 
                        agent_multipliers: Dict[str, float] = None):
        """
        Add a reward with per-agent credit multipliers.
        
        This method fixes the missing add_reward that's called at multiple locations.
        It properly handles agent_multipliers for credit assignment.
        
        Args:
            signal_key: Signal identifier
            reward: Global reward value
            agent_multipliers: Dict of agent_name â†’ multiplier (default 1.0 for all)
        """
        await self.queue_reward({
            "signal_key": signal_key,
            "reward": reward,
            "agent_multipliers": agent_multipliers or {}
        })

    async def flush(self):
        """Process all queued rewards in one batch asynchronously."""
        try:
            if not self.queue:
                return
            batch = [self.queue.popleft() for _ in range(len(self.queue))]
            logger.info(f"[RewardBatchProcessor] Flushing batch of {len(batch)} rewards...")
            await self._process_batch_supervised(batch)
        except Exception as e:
            logger.error(f"[RewardBatchProcessor.flush] {e}")

    async def _auto_flush_loop(self):
        """Background loop to auto-flush rewards periodically."""
        logger.info(f"Auto-flush loop started (interval: {self.flush_interval}s)")
        try:
            while self._running:
                await asyncio.sleep(self.flush_interval)
                if self.queue:
                    await self.flush()
        except asyncio.CancelledError:
            logger.info("Auto-flush loop cancelled")
        except Exception as e:
            logger.error(f"Auto-flush loop error: {e}")

    # ==========================================================
    # EXPERIENCE VALIDATION AND SAFE EXTRACTION
    # ==========================================================

    def validate_experience(self, exp, index: int) -> bool:
        """Validate single experience structure"""
        try:
            if isinstance(exp, dict):
                required = ['state', 'action', 'reward', 'next_state', 'done']
                for attr in required:
                    if attr not in exp:
                        logger.error(f"Experience {index} missing key: {attr}")
                        return False
                return True
            required = ['states', 'action', 'reward', 'next_states', 'done']
            for attr in required:
                if not hasattr(exp, attr):
                    logger.error(f"Experience {index} missing attribute: {attr}")
                    return False
            if hasattr(exp, 'states') and not isinstance(exp.states, dict):
                logger.error(f"Experience {index}: states not a dict")
                return False
            try:
                float(exp.reward)
            except (TypeError, ValueError):
                logger.error(f"Experience {index}: invalid reward")
                return False
            return True
        except Exception as e:
            logger.error(f"Validation error for experience {index}: {e}")
            return False

    def extract_state_safely(self, states_dict: Dict, agent: str, timeframe: str, default_dim: int) -> np.ndarray:
        """Extract state safely with fallback"""
        try:
            if agent not in states_dict:
                return np.zeros(default_dim, dtype=np.float32)
            agent_states = states_dict[agent]
            if not isinstance(agent_states, dict):
                state = np.asarray(agent_states, dtype=np.float32).flatten()
            else:
                state = agent_states.get(timeframe)
                if state is None and agent_states:
                    state = next(iter(agent_states.values()))
                if state is None:
                    return np.zeros(default_dim, dtype=np.float32)
                state = np.asarray(state, dtype=np.float32).flatten()
            if state.shape[0] < default_dim:
                state = np.pad(state, (0, default_dim - state.shape[0]), mode='constant')
                self.stats['shape_corrections'] += 1
            elif state.shape[0] > default_dim:
                state = state[:default_dim]
                self.stats['shape_corrections'] += 1
            return state
        except Exception as e:
            logger.error(f"Error extracting state for {agent}/{timeframe}: {e}")
            return np.zeros(default_dim, dtype=np.float32)

    def extract_action_safely(self, action) -> np.ndarray:
        """Extract and validate action"""
        try:
            if isinstance(action, np.ndarray):
                action = action.flatten()
                if action.size == 0:
                    return np.array([1.0, 0.0], dtype=np.float32)
                elif action.size == 1:
                    one_hot = np.zeros(self.action_dim, dtype=np.float32)
                    one_hot[int(action[0])] = 1.0
                    return one_hot
                return action[:2].astype(np.float32)
            elif isinstance(action, (int, float)):
                one_hot = np.zeros(self.action_dim, dtype=np.float32)
                one_hot[int(action)] = 1.0
                return one_hot
            elif isinstance(action, (list, tuple)):
                return np.array(action[:2], dtype=np.float32)
            else:
                return np.array([1.0, 0.0], dtype=np.float32)
        except Exception as e:
            logger.error(f"Error extracting action: {e}")
            return np.array([1.0, 0.0], dtype=np.float32)

    def extract_reward_safely(self, reward) -> float:
        """Extract and validate reward"""
        try:
            if isinstance(reward, (list, np.ndarray)):
                if len(reward) == 0:
                    return 0.0
                return float(reward[0])
            return float(reward)
        except Exception as e:
            logger.error(f"Error extracting reward: {e}")
            return 0.0

    # ==========================================================
    # BATCH TENSOR CONVERSION AND TRAINING
    # ==========================================================
    # ... [remaining methods are also indented correctly, following the same 4-space standard]

    # ==========================================================
    # BATCH TENSOR CONVERSION
    # ==========================================================

    def process_batch_to_tensors(self, experiences: List,
                                 agent_names: List[str],
                                 timeframes: List[str],
                                 device: str = 'cpu') -> Dict[str, Any]:
        """Convert validated experiences to torch tensors"""
        try:
            self.stats['batches_processed'] += 1
            batch_states = {a: {tf: [] for tf in timeframes} for a in agent_names}
            batch_next_states = {a: {tf: [] for tf in timeframes} for a in agent_names}
            batch_actions, batch_rewards, batch_dones = [], [], []
            valid_count = 0

            for i, exp in enumerate(experiences):
                if not self.validate_experience(exp, i):
                    self.stats['validation_failures'] += 1
                    continue
                valid_count += 1

                # Handle both dict and object formats
                states = exp.get('states') if isinstance(exp, dict) else exp.states
                next_states = exp.get('next_states') if isinstance(exp, dict) else exp.next_states
                action = exp.get('action') if isinstance(exp, dict) else exp.action
                reward = exp.get('reward') if isinstance(exp, dict) else exp.reward
                done = exp.get('done') if isinstance(exp, dict) else exp.done

                for a in agent_names:
                    for tf in timeframes:
                        s = self.extract_state_safely(states, a, tf, self.state_dim)
                        ns = self.extract_state_safely(next_states, a, tf, self.state_dim)
                        batch_states[a][tf].append(torch.tensor(s, dtype=torch.float32, device=device))
                        batch_next_states[a][tf].append(torch.tensor(ns, dtype=torch.float32, device=device))

                batch_actions.append(torch.tensor(self.extract_action_safely(action), dtype=torch.float32, device=device))
                batch_rewards.append(torch.tensor(self.extract_reward_safely(reward), dtype=torch.float32, device=device))
                batch_dones.append(torch.tensor(float(bool(done)), dtype=torch.float32, device=device))

            if valid_count == 0:
                raise ValueError("No valid experiences in batch!")

            for a in agent_names:
                for tf in timeframes:
                    batch_states[a][tf] = torch.stack(batch_states[a][tf])
                    batch_next_states[a][tf] = torch.stack(batch_next_states[a][tf])

            return {
                'states': batch_states,
                'next_states': batch_next_states,
                'actions': torch.stack(batch_actions),
                'rewards': torch.stack(batch_rewards),
                'dones': torch.stack(batch_dones)
            }
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            raise

    # ==========================================================
    # BATCH PROCESSING AND TRAINING (FIXED)
    # ==========================================================

    async def _process_batch_supervised(self, batch):
        """
        Process a batch of experiences via ExperienceManager and train_step.

        FIXED: Now returns the experience object and stores in quantum_bridge
        """
        try:
            # Validate exp_manager is set
            if self.exp_manager is None:
                logger.error("âŒ CRITICAL: exp_manager not set! Run: system.batch_processor.exp_manager = system.exp_manager")
                self.stats["errors"] += len(batch)
                return

            valid_experiences = []

            for exp in batch:
                try:
                    signal_key = exp.get("signal_key")
                    reward = exp.get("reward")
                    next_state = exp.get("next_state") or exp.get("entry_price")
                    done = exp.get("done", False)

                    # Complete via ExperienceManager - NOW RETURNS THE EXPERIENCE!
                    full_exp = self.exp_manager.complete_experience(signal_key, reward, next_state, done)
                    if not full_exp:  # Now checking if None (no experience found)
                        self.stats["skipped"] += 1
                        logger.debug(f"Skipped: No experience found for {signal_key}")
                        continue

                    # âœ… CRITICAL: Store in quantum_bridge buffer!
                    if hasattr(self.system, 'quantum_bridge') and self.system.quantum_bridge:
                        try:
                            # Extract agent name from signal_key (format: agentname_timestamp_id)
                            agent_name = signal_key.split('_')[0] if '_' in signal_key else 'xs'

                            # Store using quantum_bridge method
                            stored = self.system.quantum_bridge.store_experience_for_agent(
                                agent_name=agent_name,
                                state=full_exp.get('states'),
                                action=full_exp.get('action'),
                                reward=full_exp.get('reward'),
                                next_state=full_exp.get('next_states'),
                                done=full_exp.get('done', False)
                            )

                            if stored:
                                logger.debug(f"âœ… Stored {signal_key} in quantum_bridge buffer")
                            else:
                                logger.warning(f"âš ï¸  Failed to store {signal_key} in quantum_bridge")

                        except Exception as store_err:
                            logger.error(f"âŒ Failed to store in quantum_bridge: {store_err}")

                    # Add to valid experiences for training
                    valid_experiences.append(full_exp)

                except Exception as te:
                    logger.error(f"[RewardBatchProcessor] Experience processing error: {te}")
                    self.stats["errors"] += 1

            if valid_experiences:
                await self._train_multi_agents(valid_experiences)
                self.stats["processed"] += len(valid_experiences)
                logger.info(f"âœ… [RewardBatchProcessor] Processed {len(valid_experiences)} experiences")

            logger.info(
                f"âœ… [RewardBatchProcessor] Total processed: {self.stats['processed']} | "
                f"Skipped: {self.stats['skipped']}"
            )

        except Exception as e:
            logger.error(f"[RewardBatchProcessor._process_batch_supervised] {e}")
            import traceback
            traceback.print_exc()
            self.stats["errors"] += 1

    async def _train_multi_agents(self, valid_experiences):
        """
        Train all agents using centralized quantum trainer.

        FIXED VERSION - Uses quantum_trainer instead of individual agent buffers.

        This fix resolves the "Trained 0/6 agents" error by using the centralized
        quantum_trainer that has proper buffer access, instead of trying to access
        non-existent individual agent buffers.

        Changes from original:
        1. Uses self.system.quantum_bridge.quantum_trainer instead of iterating agents
        2. Leverages centralized hybrid_buffer that's properly linked
        3. Maintains quantum entanglement architecture
        4. Provides clear logging and diagnostics

        Args:
            valid_experiences: List of validated experience tuples
        """
        try:
            # =======================================================================
            # STEP 1: Verify quantum infrastructure exists
            # =======================================================================
            if not hasattr(self.system, 'quantum_bridge'):
                logger.warning("[RewardBatchProcessor] âŒ No quantum_bridge available in system")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents")
                return

            quantum_bridge = self.system.quantum_bridge

            # =======================================================================
            # STEP 2: Verify quantum trainer exists and is linked
            # =======================================================================
            if not hasattr(quantum_bridge, 'quantum_trainer'):
                logger.warning("[RewardBatchProcessor] âŒ No quantum_trainer attribute in quantum_bridge")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents")
                return

            if quantum_bridge.quantum_trainer is None:
                logger.warning("[RewardBatchProcessor] âŒ quantum_trainer is None")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents")
                return

            trainer = quantum_bridge.quantum_trainer

            # =======================================================================
            # STEP 3: Verify trainer has buffer access
            # =======================================================================
            if not hasattr(trainer, 'buffer'):
                logger.error("[RewardBatchProcessor] âŒ Trainer has no 'buffer' attribute!")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents")
                return

            if trainer.buffer is None:
                logger.error("[RewardBatchProcessor] âŒ Trainer buffer is None!")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents")
                return

            # =======================================================================
            # STEP 4: Check if enough data for training
            # =======================================================================
            buffer_size = len(trainer.buffer)
            batch_size = getattr(trainer, 'batch_size', 64)

            logger.debug(f"[RewardBatchProcessor] Buffer check: {buffer_size} experiences, need {batch_size}")

            if buffer_size < batch_size:
                logger.info(f"[RewardBatchProcessor] â³ Insufficient buffer: {buffer_size}/{batch_size} - waiting for more data")
                num_agents = len(getattr(self.system, 'agents', {}))
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents (waiting for data)")
                return

            # =======================================================================
            # STEP 5: Perform centralized quantum training
            # =======================================================================
            try:
                num_agents = len(self.system.agents) if hasattr(self.system, 'agents') else 0
                logger.critical(f"[RewardBatchProcessor] ğŸš€ Starting quantum training")
                logger.critical(f"   Buffer: {buffer_size} experiences")
                logger.critical(f"   Batch size: {batch_size}")
                logger.critical(f"   Agents: {num_agents} (entangled)")

                # Call trainer's train_step method
                metrics = trainer.train_step()

                # =======================================================================
                # STEP 6: Process training results
                # =======================================================================
                if metrics:
                    logger.critical(f"âœ… [RewardBatchProcessor] Quantum training successful!")

                    # Log metrics
                    if isinstance(metrics, dict):
                        for key, value in metrics.items():
                            if isinstance(value, (int, float)):
                                logger.info(f"   {key}: {value:.6f}")
                            else:
                                logger.info(f"   {key}: {value}")
                    else:
                        logger.info(f"   Metrics: {metrics}")

                    # Log success with agent count (maintains compatibility with old logs)
                    logger.info(f"âœ… [RewardBatchProcessor] Trained {num_agents}/{num_agents} agents (quantum entangled system)")

                else:
                    logger.warning("[RewardBatchProcessor] âš ï¸ Training returned no metrics")
                    logger.info(f"âœ… [RewardBatchProcessor] Trained {num_agents}/{num_agents} agents (no metrics)")

            except Exception as training_error:
                logger.error(f"[RewardBatchProcessor] âŒ Quantum training failed: {training_error}")
                import traceback
                traceback.print_exc()

                # Still log as attempted
                num_agents = len(self.system.agents) if hasattr(self.system, 'agents') else 0
                logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents (training error)")

        except Exception as e:
            logger.error(f"[RewardBatchProcessor._train_multi_agents] ğŸ’¥ Critical error: {e}")
            import traceback
            traceback.print_exc()

            # Log final status
            num_agents = len(getattr(self.system, 'agents', {}))
            logger.info(f"âœ… [RewardBatchProcessor] Trained 0/{num_agents} agents (critical error)")

def handle_reward(system, signal_key, reward_data):
    """Complete partial experiences and queue for supervised training"""
    print(f"\nğŸ”¥ [REWARD HANDLER] handle_reward CALLED!")
    print(f"   Signal Key: {signal_key}")
    print(f"   Reward: {reward_data.get('reward', 'N/A')}")

    try:
        reward_value = reward_data.get("reward")
        exit_price = reward_data.get("exit_price")

        if reward_value is None:
            print(f"   âŒ No reward value found")
            logger.warning(f"[RewardHandler] No reward found for {signal_key}")
            return

        print(f"   âœ… Reward value: {reward_value}")

        # âœ… Match to partial experience
        experience = system.partial_experiences.pop(signal_key, None)
        if experience is None:
            print(f"   âŒ No matching partial experience for key: {signal_key}")
            print(f"   Available partial keys count: {len(system.partial_experiences)}")
            logger.warning(f"[RewardHandler] No matching partial experience for key: {signal_key}")
            return

        print(f"   âœ… Matched partial experience")

        experience["reward"] = float(reward_value)
        experience["exit_price"] = float(exit_price) if exit_price is not None else None

        # âœ… Add to main experience buffer
        print(f"   ğŸ“¦ Storing in experience_buffer...")
        buffer_size_before = len(system.experience_buffer)
        print(f"   Buffer size before: {buffer_size_before}")

        system.experience_buffer.append(experience)

        buffer_size_after = len(system.experience_buffer)
        print(f"   âœ… STORED! Buffer size after: {buffer_size_after}")

        # âœ… Queue for supervised training
        asyncio.create_task(system.reward_batch_processor.queue_reward(experience))

        print(f"   âœ… [Reward Completed] {signal_key} â†’ {reward_value:.4f}\n")
        logger.info(f"[Reward Completed] {signal_key} â†’ {reward_value:.4f}")

    except Exception as e:
        print(f"   âŒ ERROR in handle_reward: {e}")
        import traceback
        traceback.print_exc()
        logger.error(f"[RewardHandler] {e}")

# ============================================================
# âš™ï¸ Integrate with your system (method inside IntegratedSignalSystem)
# ============================================================

# Add this right after creating your RewardBatchProcessor
async def periodic_diagnosis():
    while True:
        await asyncio.sleep(60)  # Check every minute
        if hasattr(system, 'batch_processor'):
            await system.batch_processor.diagnose_training_issues()

# Start the diagnostic
asyncio.create_task(periodic_diagnosis())

logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURATION
# ==============================================================================

TIMEFRAME_LENGTHS = {
    # === High-Frequency Zone (Volatility Capture) ===
    'xs': 5,     # tick
    's': 10,     # ultra
    'm': 20,     # fast

    # === Critical Trading Zones ===
    'l': 30,     # scalp
    'xl': 60,    # 1min
    'xxl': 120,  # 2min

    # === Structure & Regime Detection ===
    '5m': 300,   # 5min
    '10m': 600,  # 10min
}

ACTION_MAP = {0: "BUY", 1: "SELL"}

# ==============================================================================
# UTILITY FUNCTIONS
# ==============================================================================

def ensure_state_array(x: Any, expected_dim: int, fill_value: float = 0.0) -> np.ndarray:
    """Convert input to fixed-length state array"""
    if x is None:
        return np.zeros(expected_dim, dtype=np.float32)

    if isinstance(x, torch.Tensor):
        arr = x.detach().cpu().numpy().astype(np.float32).flatten()
    elif isinstance(x, dict):
        vals = []
        for v in x.values():
            if isinstance(v, (list, tuple, np.ndarray)):
                vals.extend(np.asarray(v).flatten().tolist())
            else:
                try:
                    vals.append(float(v))
                except:
                    continue
        arr = np.asarray(vals, dtype=np.float32).flatten()
    else:
        try:
            arr = np.asarray(x, dtype=np.float32).flatten()
        except:
            arr = np.zeros(expected_dim, dtype=np.float32)

    # Fix NaNs/Infs
    arr = np.nan_to_num(arr, nan=fill_value, posinf=fill_value, neginf=fill_value)

    # Pad or truncate
    if arr.size < expected_dim:
        arr = np.pad(arr, (0, expected_dim - arr.size), mode='constant', constant_values=fill_value)
    elif arr.size > expected_dim:
        arr = arr[:expected_dim]

    return arr.astype(np.float32)

# ==============================================================================
# EXPERIENCE BUFFER
# ==============================================================================

# REMOVED DUPLICATE: from dataclasses import dataclass
from typing import Dict, List, Any, Tuple
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import random

@dataclass
class Experience:
    """
    Multi-timeframe experience for QuantumAgents.
    
    V8.6.1 FIX: Enhanced with agent attribution for proper credit assignment.
    
    New fields:
    - originating_agent_id: WHO generated this experience
    - agent_multipliers: Credit assignment multipliers for each agent
    - timestamp: WHEN the experience was generated
    """
    states: Dict[str, Dict[str, np.ndarray]]       # agent -> timeframe -> state vector
    action: np.ndarray                             # one-hot encoded action
    reward: float
    next_states: Dict[str, Dict[str, np.ndarray]]  # agent -> timeframe -> state vector
    done: bool
    # V8.6.1 FIX: Agent attribution fields (optional for backward compatibility)
    originating_agent_id: Optional[str] = None
    agent_multipliers: Optional[Dict[str, float]] = None
    timestamp: Optional[float] = None
    global_reward: Optional[float] = None  # Pre-scaled reward for QMIX
    
    def get_scaled_reward(self, agent_name: str) -> float:
        """Get credit-scaled reward for a specific agent."""
        if self.agent_multipliers and agent_name in self.agent_multipliers:
            return self.reward * self.agent_multipliers[agent_name]
        return self.reward


class ExperienceReplay:
    """Multi-timeframe experience replay buffer for QuantumAgents"""

    def __init__(self, capacity: int = 100_000_000, device: str = "cpu"):
        self.capacity = capacity
        self.buffer: List[Experience] = []
        self.ptr = 0
        self.device = device

    def add(self, exp: Experience):
        """Add a new experience; overwrite oldest if full"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(exp)
        else:
            self.buffer[self.ptr] = exp
            self.ptr = (self.ptr + 1) % self.capacity

    def append(self, exp: Experience):
        """Alias for add() - supports deque-like interface"""
        return self.add(exp)

    def sample(self, batch_size: int) -> List[Experience]:
        """Randomly sample a batch"""
        if len(self.buffer) < batch_size:
            raise ValueError(f"Not enough samples to draw batch of size {batch_size}")
        return random.sample(self.buffer, batch_size)

    def sample_tensors(self, batch_size: int, expected_state_dim: int
                      ) -> Tuple[Dict[str, Dict[str, torch.Tensor]],
                                 torch.Tensor,
                                 torch.Tensor,
                                 Dict[str, Dict[str, torch.Tensor]],
                                 torch.Tensor]:
        """
        Sample batch and convert to device-ready tensors.

        Returns:
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_done

        Shapes:
            batch_states[agent][tf] -> (batch_size, state_dim)
            batch_actions -> (batch_size, action_dim)
            batch_rewards -> (batch_size,)
            batch_next_states[agent][tf] -> (batch_size, state_dim)
            batch_done -> (batch_size,)
        """
        experiences = self.sample(batch_size)

        # Get agent names and timeframes from first experience
        agents = experiences[0].states.keys()
        timeframes = next(iter(experiences[0].states.values())).keys()

        # Initialize batch dictionaries
        batch_states = {agent: {tf: [] for tf in timeframes} for agent in agents}
        batch_next_states = {agent: {tf: [] for tf in timeframes} for agent in agents}
        batch_actions = []
        batch_rewards = []
        batch_done = []

        # Collect all experiences into lists
        for exp in experiences:
            for agent in agents:
                for tf in timeframes:
                    # Get state or use zeros if missing
                    state = exp.states.get(agent, {}).get(tf, np.zeros(expected_state_dim, dtype=np.float32))
                    next_state = exp.next_states.get(agent, {}).get(tf, np.zeros(expected_state_dim, dtype=np.float32))

                    # Convert to tensors
                    batch_states[agent][tf].append(
                        torch.tensor(state, dtype=torch.float32, device=self.device)
                    )
                    batch_next_states[agent][tf].append(
                        torch.tensor(next_state, dtype=torch.float32, device=self.device)
                    )

            # Add action, reward, done
            batch_actions.append(
                torch.tensor(exp.action, dtype=torch.float32, device=self.device)
            )
            batch_rewards.append(
                torch.tensor(exp.reward, dtype=torch.float32, device=self.device)
            )
            batch_done.append(
                torch.tensor(float(exp.done), dtype=torch.float32, device=self.device)
            )

        # Stack all tensors
        for agent in agents:
            for tf in timeframes:
                batch_states[agent][tf] = torch.stack(batch_states[agent][tf], dim=0)
                batch_next_states[agent][tf] = torch.stack(batch_next_states[agent][tf], dim=0)

        batch_actions = torch.stack(batch_actions, dim=0)
        batch_rewards = torch.stack(batch_rewards, dim=0)
        batch_done = torch.stack(batch_done, dim=0)

        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_done

    def __len__(self):
        """Return current buffer size"""
        return len(self.buffer)

    def clear(self):
        """Clear all experiences from buffer"""
        self.buffer = []
        self.ptr = 0

    def is_ready(self, min_size: int = 64) -> bool:
        """Check if buffer has enough samples for training"""
        return len(self.buffer) >= min_size

    def get_stats(self) -> Dict[str, Any]:
        """Get buffer statistics"""
        return {
            'size': len(self.buffer),
            'capacity': self.capacity,
            'utilization': len(self.buffer) / self.capacity,
            'ptr': self.ptr
        }

# ==============================================================================
# COMPLEX-VALUED NEURAL NETWORK COMPONENTS
# ==============================================================================

class ComplexLinear(nn.Module):
    """Complex-valued linear layer"""
    def __init__(self, in_features, out_features):
        super().__init__()
        self.W_real = nn.Parameter(torch.randn(out_features, in_features) * 0.01)
        self.W_imag = nn.Parameter(torch.randn(out_features, in_features) * 0.01)
        self.b_real = nn.Parameter(torch.zeros(out_features))
        self.b_imag = nn.Parameter(torch.zeros(out_features))

    def forward(self, x):
        if not torch.is_complex(x):
            x = torch.complex(x, torch.zeros_like(x))

        x_real = x.real
        x_imag = x.imag

        out_real = x_real @ self.W_real.t() - x_imag @ self.W_imag.t() + self.b_real
        out_imag = x_real @ self.W_imag.t() + x_imag @ self.W_real.t() + self.b_imag

        return torch.complex(out_real, out_imag)

class ModReLU(nn.Module):
    """Modified ReLU for complex numbers"""
    def __init__(self, features):
        super().__init__()
        self.b = nn.Parameter(torch.zeros(features))

    def forward(self, z):
        magnitude = torch.abs(z)
        phase = z / (magnitude + 1e-8)
        new_magnitude = F.relu(magnitude + self.b.unsqueeze(0))
        return new_magnitude * phase

class ComplexBatchNorm(nn.Module):
    """Batch normalization for complex numbers"""
    def __init__(self, features):
        super().__init__()
        self.bn_real = nn.BatchNorm1d(features)
        self.bn_imag = nn.BatchNorm1d(features)

    def forward(self, z):
        return torch.complex(self.bn_real(z.real), self.bn_imag(z.imag))

class ComplexMultiheadAttention(nn.Module):
    """Multi-head attention for complex numbers"""
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.real_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)
        self.imag_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True, dropout=dropout)

    def forward(self, z):
        z_real, z_imag = z.real, z.imag
        attn_real, _ = self.real_attn(z_real, z_real, z_real)
        attn_imag, _ = self.imag_attn(z_imag, z_imag, z_imag)
        return torch.complex(attn_real, attn_imag)

# ==============================================================================
# TIMEFRAME PROCESSING LAYERS
# ==============================================================================

class TimeframeEncoder(nn.Module):
    """Encode single timeframe state"""
    def __init__(self, state_dim, embed_dim=64, dropout=0.2):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.ReLU()
        )

    def forward(self, state):
        return self.encoder(state)

class MultiTimeframeFusion(nn.Module):
    """Fuse multiple timeframe representations - DIMENSION FIX & SEQ-SAFE"""

    def __init__(self, agent_names: List[str], timeframes: List[str],
                 state_dim: int, action_dim: int, latent_dim: int = 128,
                 embed_dim: int = 64, num_heads: int = 4, dropout: float = 0.1,
                 device: str = "cpu"):
        super().__init__()
        self.agent_names = agent_names
        self.timeframes = list(timeframes)
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_agents = len(agent_names)
        self.latent_dim = latent_dim
        self.embed_dim = embed_dim
        self.device = device

        # Temporal attention across timeframes
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim, num_heads, batch_first=True, dropout=dropout
        )

        # Fusion projection layer to latent_dim
        self.fusion_layer = nn.Sequential(
            nn.Linear(embed_dim, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Learnable weights per timeframe
        self.timeframe_weights = nn.Parameter(
            torch.ones(len(self.timeframes), device=device) / len(self.timeframes)
        )

        self.to(device)
        logger.info(f"MultiTimeframeFusion initialized: embed_dim={embed_dim}, latent_dim={latent_dim}")

    def forward(self, timeframe_embeddings: torch.Tensor):
        """
        Args:
            timeframe_embeddings:
                - shape (batch, num_timeframes, embed_dim) OR
                - shape (batch, num_timeframes, seq_len, embed_dim)
        Returns:
            fused: (batch, latent_dim)
            attn_weights: attention weights from MultiheadAttention
        """
        # Handle 4D input (seq_len per timeframe) by reducing seq_len via mean
        if timeframe_embeddings.dim() == 4:
            # (batch, timeframes, seq_len, embed_dim) -> (batch, timeframes, embed_dim)
            timeframe_embeddings = timeframe_embeddings.mean(dim=2)

        # Validate shape
        expected_shape = (timeframe_embeddings.size(0), len(self.timeframes), self.embed_dim)
        if timeframe_embeddings.shape[1:] != expected_shape[1:]:
            logger.warning(f"Input shape mismatch: got {timeframe_embeddings.shape}, expected {expected_shape}")

        # Temporal attention across timeframes
        # Input: [batch, seq_len=num_timeframes, embed_dim]
        attended, attn_weights = self.temporal_attention(
            timeframe_embeddings, timeframe_embeddings, timeframe_embeddings
        )

        # Weighted sum across timeframes
        weights = F.softmax(self.timeframe_weights, dim=0)
        fused = (attended * weights.view(1, -1, 1)).sum(dim=1)  # (batch, embed_dim)

        # Project to latent_dim
        fused = self.fusion_layer(fused)  # (batch, latent_dim)

        # Dimension check
        if fused.size(-1) != self.latent_dim:
            raise ValueError(f"Fusion output dimension mismatch: got {fused.size(-1)}, expected {self.latent_dim}")

        return fused, attn_weights

class MultiTimeframeSharedLatentEncoder(nn.Module):
    """Shared latent encoder for entanglement across agents"""

    def __init__(self, state_dim: int, latent_dim: int = 32,
                 num_agents: int = 8, action_dim: int = 2,
                 num_timeframes: int = 8, dropout: float = 0.2):
        super().__init__()
        self.state_dim = state_dim
        self.latent_dim = latent_dim
        self.num_agents = num_agents
        self.num_timeframes = num_timeframes

        # Project shared state to latent space
        self.z_proj = nn.Linear(self.state_dim, self.latent_dim)

        # Encoder for all agents/timeframes combined Q-values
        input_dim = num_agents * action_dim * num_timeframes
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, latent_dim),
            nn.Tanh()
        )

        # History of entanglement scores
        self.entanglement_history = deque(maxlen=1000)

    def forward(self, all_q_values: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: encode combined Q-values into latent vector.
        """
        z = self.encoder(all_q_values)
        if self.training:
            entanglement = self._measure_entanglement(z)
            self.entanglement_history.append(entanglement)
        return z

    def project_shared_state(self, z_shared: torch.Tensor) -> torch.Tensor:
        """
        Project shared state to latent space using z_proj.
        Ensures batch dimension and correct state_dim.
        """
        if z_shared.dim() == 1:
            z_shared = z_shared.unsqueeze(0)
        elif z_shared.size(-1) != self.state_dim:
            # Pad or truncate if state_dim mismatch
            batch_size = z_shared.size(0)
            padded = torch.zeros(batch_size, self.state_dim, device=z_shared.device)
            dim_to_copy = min(z_shared.size(-1), self.state_dim)
            padded[:, :dim_to_copy] = z_shared[:, :dim_to_copy]
            z_shared = padded
        return self.z_proj(z_shared)

    def _measure_entanglement(self, z: torch.Tensor) -> float:
        """Compute a safe entanglement metric for latent vector z"""
        if z.numel() > 1:
            variance = torch.var(z, dim=0, unbiased=False).mean().item()
        else:
            variance = 0.0
        mean_norm = torch.norm(z.mean(dim=0)).item()
        return variance + mean_norm

    def get_entanglement_metrics(self) -> dict:
        """Return current and historical entanglement statistics"""
        if len(self.entanglement_history) == 0:
            return {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'current': 0.0}
        history = np.array(self.entanglement_history)
        return {
            'mean': float(np.mean(history)),
            'std': float(np.std(history)),
            'min': float(np.min(history)),
            'max': float(np.max(history)),
            'current': float(history[-1])
        }

# =============================================================================
# UNIFIED MultiTimeframeEntangledComplexAgent
# =============================================================================

class MultiTimeframeEntangledComplexAgent(nn.Module):


    def __init__(self, state_dim: int = DEFAULT_STATE_DIM,
                 action_dim: int = DEFAULT_ACTION_DIM,
                 hidden_dim: int = DEFAULT_HIDDEN_DIM,
                 latent_dim: int = DEFAULT_LATENT_DIM,
                 n_heads: int = DEFAULT_NUM_HEADS,
                 n_layers: int = DEFAULT_NUM_LAYERS,
                 timeframes: List[str] = None,
                 learning_rate: float = LEARNING_RATE):
        super().__init__()

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.timeframes = timeframes or TIMEFRAMES

        # Timeframe encoders
        self.timeframe_encoders = nn.ModuleDict({
            tf: nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, latent_dim)
            ) for tf in self.timeframes
        })

        # Cross-timeframe attention
        self.cross_attention = CrossTimeframeAttention(latent_dim, len(self.timeframes), n_heads)

        # Actor-Critic networks
        self.actor = Actor(latent_dim, action_dim, hidden_dim)
        self.critic = Critic(latent_dim, action_dim, hidden_dim)

        # Q-value network
        self.model = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, action_dim)
        )

        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)

        logger.info(f"âœ… MultiTimeframeEntangledComplexAgent initialized")

    def encode_timeframes(self, states: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:

        encoded = {}

        for tf in self.timeframes:
            if tf in states and states[tf] is not None:
                encoded[tf] = self.timeframe_encoders[tf](states[tf])

        return encoded

    def forward(self, states: Union[torch.Tensor, Dict[str, torch.Tensor]]) -> torch.Tensor:

        if isinstance(states, dict):
            # Multi-timeframe processing
            encoded = self.encode_timeframes(states)
            aggregated = self.cross_attention(encoded)
            return self.model(aggregated)
        else:
            # Single state processing
            if states.shape[-1] == self.latent_dim:
                return self.model(states)
            else:
                # Encode first
                encoded = self.timeframe_encoders['m'](states)
                return self.model(encoded)

    def get_q_values(self, states: Union[torch.Tensor, Dict[str, torch.Tensor]],
                     z: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Get Q-values for given states.

        Args:
            states: State tensor or dict of timeframe states
            z: Optional latent code (for compatibility, may be unused)

        Returns:
            Q-values tensor
        """
        # Simply delegate to forward method
        return self.forward(states)

logger.info("âœ… Enhanced agent classes with Actor-Critic initialized")

logger = logging.getLogger("QuantumSystemLogger")

# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import torch.nn as nn
# REMOVED DUPLICATE: import torch.nn.functional as F
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: from collections import deque
# REMOVED DUPLICATE: import logging

logger = logging.getLogger("QuantumSystemLogger")

class RobustLatentEncoder(nn.Module):
    """
    Multi-Agent, Multi-Timeframe Entangled Latent Encoder (robust/failsafe).
    - forward(agent_states) -> dict[agent_name] -> tensor(latent_dim)  (per-agent latents)
    - fuse_multi_timeframe_state(timeframe_tensors, weights=None) -> tensor(batch, latent_dim)
      Backwards-compatible: accepts dict[tf] -> tensor(batch, state_dim).
    """

    def __init__(self,
                 state_dim: int = 64,
                 latent_dim: int = 128,
                 num_agents: int = 8,
                 action_dim: int = 2,
                 num_timeframes: int = 8,
                 device: torch.device = None):
        super().__init__()

        self.state_dim = state_dim
        self.latent_dim = latent_dim
        self.num_agents = num_agents
        self.action_dim = action_dim
        self.num_timeframes = num_timeframes
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # entanglement tracking
        self.entanglement_history = deque(maxlen=1000)

        # Per-agent, per-timeframe embedding MLPs (maps state_dim -> latent_dim)
        self.agent_timeframe_emb = nn.ModuleDict({
            f"agent_{i}": nn.ModuleList([
                nn.Sequential(
                    nn.Linear(self.state_dim, self.latent_dim),
                    nn.ReLU(),
                    nn.Linear(self.latent_dim, self.latent_dim),
                    nn.ReLU()
                ) for _ in range(self.num_timeframes)
            ]) for i in range(self.num_agents)
        })

        # Timeframe-level fusion (when fusing timeframes for a single agent)
        self.timeframe_fusion = nn.Sequential(
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.ReLU(),
            nn.LayerNorm(self.latent_dim)
        )

        # Cross-agent fusion (expects concatenated agent latents: num_agents * latent_dim)
        self.cross_agent_fusion = nn.Sequential(
            nn.Linear(self.num_agents * self.latent_dim, self.latent_dim),
            nn.ReLU(),
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.LayerNorm(self.latent_dim)
        )

        # final small projector (optional)
        self.final_proj = nn.Sequential(
            nn.Linear(self.latent_dim, self.latent_dim),
            nn.GELU(),
            nn.LayerNorm(self.latent_dim)
        )

        # move to device
        self.to(self.device)
        logger.info(f"RobustLatentEncoder init: state_dim={self.state_dim}, latent_dim={self.latent_dim}, "
                    f"agents={self.num_agents}, tfs={self.num_timeframes}, device={self.device}")

    # -------------------------
    # Primary forward used by system
    # -------------------------
    def forward(self, agent_states: dict):
        """
        agent_states: dict[agent_name][tf] -> tensor(seq_len, state_dim) or tensor(batch, state_dim)
        Returns: dict[agent_name] -> tensor(batch, latent_dim)
        """
        device = self.device  # FIXED: QuantumSystemTrainer is not nn.Module
        latents = {}

        try:
            # Encode each agent's per-timeframe last-state -> latent_dim
            for i, agent_name in enumerate(sorted(agent_states.keys())):
                tf_embeddings = []
                tf_keys = list(agent_states[agent_name].keys())
                # If the provided number of timeframes differs from expected, handle gracefully
                for tf_idx, tf in enumerate(tf_keys):
                    # guard: tf_idx may exceed configured num_timeframes; wrap with modulo
                    emb_module = self.agent_timeframe_emb.get(f"agent_{i}", None)
                    if emb_module is None:
                        # fallback: create on-the-fly linear if structure mismatches (defensive)
                        linear_fallback = nn.Sequential(
                            nn.Linear(self.state_dim, self.latent_dim),
                            nn.ReLU()
                        ).to(device)
                        x = agent_states[agent_name][tf].to(device).float()
                        if x.ndim == 1:
                            x = x.unsqueeze(0)
                        last_state = x[-1, :]
                        tf_embeddings.append(linear_fallback(last_state))
                        continue

                    # clamp tf_idx to available modules
                    tf_idx_safe = tf_idx % len(emb_module)
                    x = agent_states[agent_name][tf].to(device).float()
                    if x.ndim == 1:
                        x = x.unsqueeze(0)  # (1, state_dim)
                    # take last timestep if seq present
                    last_state = x[-1, :]
                    emb = emb_module[tf_idx_safe](last_state)  # (batch or 1, latent_dim)
                    tf_embeddings.append(emb)

                # stack and fuse timeframes -> (num_tfs, batch, latent_dim) maybe mixed batch sizes -> handle
                # normalize shapes: ensure each emb has shape (batch, latent_dim)
                tf_embeddings = [e if e.ndim == 2 else e.unsqueeze(0) for e in tf_embeddings]
                stacked = torch.stack(tf_embeddings, dim=0)  # (num_tfs, batch, latent_dim)
                # fuse across timeframes by mean
                agent_latent = torch.mean(stacked, dim=0)  # (batch, latent_dim)
                # pass through timeframe fusion projector (keeps latent_dim)
                agent_latent = self.timeframe_fusion(agent_latent)
                latents[agent_name] = agent_latent

            # concat agent latents along last dim -> expect (batch, num_agents*latent_dim)
            # but batch dimension could differ across agents (defensive): broadcast or expand smallest
            batches = [v.shape[0] for v in latents.values()]
            batch_size = max(batches)
            # expand any single-row latents to batch_size if required
            for k in list(latents.keys()):
                v = latents[k]
                if v.shape[0] == 1 and batch_size > 1:
                    latents[k] = v.expand(batch_size, -1).contiguous()

            concat = torch.cat([latents[k] for k in sorted(latents.keys())], dim=-1)  # (batch, num_agents*latent_dim)
            fused = self.cross_agent_fusion(concat)  # (batch, latent_dim)
            fused = self.final_proj(fused)  # (batch, latent_dim)

            # add the fused global latent back to each agent latent
            for k in latents:
                # ensure same batch sizes
                la = latents[k]
                if la.shape[0] != fused.shape[0]:
                    if la.shape[0] == 1:
                        la = la.expand(fused.shape[0], -1)
                    else:
                        # if mismatch and cannot broadcast, truncate/expand to match (defensive)
                        min_b = min(la.shape[0], fused.shape[0])
                        la = la[:min_b, :]
                        fused = fused[:min_b, :]
                latents[k] = la + fused

            # entanglement tracking
            if self.training:
                try:
                    ent = self._measure_entanglement(fused)
                    self.entanglement_history.append(ent)
                except Exception:
                    pass

            return latents

        except Exception as e:
            logger.error(f"[LatentEncoder] forward() failed: {e}")
            # fallback: return zeros for every agent with appropriate batch dim 1
            dummy = torch.zeros(1, self.latent_dim, device=device)
            return {agent: dummy for agent in sorted(agent_states.keys())}

    # -------------------------
    # Backwards-compatible fuse for older code paths
    # -------------------------
    def fuse_multi_timeframe_state(self, tf_states: dict, weights: dict = None):
        """
        Backwards compatibility:
        tf_states: dict[tf] -> tensor(batch, state_dim)  OR tensor(state_dim) OR tensor(1, state_dim)
        Returns: tensor(batch, latent_dim)
        Implementation:
          - project each timeframe with a small shared projector (state_dim -> latent_dim)
          - stack and weighted-avg (or mean) -> (batch, latent_dim)
          - pass through timeframe_fusion and return
        """
        try:
            # collect valid tensors
            valid = {k: v for k, v in tf_states.items() if isinstance(v, torch.Tensor)}
            if not valid:
                raise ValueError("fuse_multi_timeframe_state: no valid tensors")

            keys = sorted(valid.keys())
            device = self.device  # FIXED: QuantumSystemTrainer is not nn.Module

            # Project each timeframe from state_dim -> latent_dim using agent_timeframe_emb[agent_0][0]'s first linear weights as shared fallback
            # Create a small shared linear on the fly if shapes mismatch (defensive)
            shared_proj = nn.Linear(self.state_dim, self.latent_dim).to(device)
            projected = []
            for k in keys:
                t = valid[k].to(device).float()
                if t.ndim == 1:
                    t = t.unsqueeze(0)
                # if t has seq len >1, take last
                if t.ndim == 3:
                    # (batch, seq_len, state_dim)
                    t = t[:, -1, :]
                if t.ndim == 2 and t.shape[-1] != self.state_dim:
                    # try to reshape/truncate/pad
                    if t.shape[-1] > self.state_dim:
                        t = t[:, :self.state_dim]
                    else:
                        # pad
                        pad = torch.zeros(t.shape[0], self.state_dim - t.shape[-1], device=device)
                        t = torch.cat([t, pad], dim=-1)
                projected.append(shared_proj(t))  # (batch, latent_dim)

            stacked = torch.stack(projected, dim=0)  # (num_tfs, batch, latent_dim)

            if weights:
                w = torch.tensor([weights.get(k, 1.0) for k in keys], dtype=torch.float32, device=device)
                w = w / (w.sum() + 1e-8)
                w = w.view(-1, 1, 1)
                fused = torch.sum(stacked * w, dim=0)
            else:
                fused = torch.mean(stacked, dim=0)

            # final projector for timeframe-level fused vector
            fused = self.timeframe_fusion(fused)  # (batch, latent_dim)
            return fused

        except Exception as e:
            logger.warning(f"[LatentEncoder] fuse_multi_timeframe_state error: {e}")
            return torch.zeros(1, self.latent_dim, device=self.device)

    # -------------------------
    # Entanglement helpers
    # -------------------------
    def _measure_entanglement(self, z: torch.Tensor) -> float:
        try:
            if z.dim() == 2:
                variance = float(torch.var(z, dim=0, unbiased=False).mean().item())
                mean_norm = float(torch.norm(z.mean(dim=0)).item())
            else:
                variance = 0.0
                mean_norm = float(torch.norm(z).item()) if z.numel() else 0.0
            return variance + mean_norm
        except Exception:
            return 0.0

    def get_entanglement_metrics(self):
        if len(self.entanglement_history) == 0:
            return {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'current': 0.0}
        history = np.array(self.entanglement_history)
        return {
            'mean': float(np.mean(history)),
            'std': float(np.std(history)),
            'min': float(np.min(history)),
            'max': float(np.max(history)),
            'current': float(history[-1])
        }

class MultiTimeframeEntangledComplexAgentSystem(nn.Module):
    def __init__(self,
                 state_dim: int = 58,
                 action_dim: int = 2,
                 latent_dim: int = 32,
                 num_agents: int = 8,
                 num_timeframes: int = None,
                 device: str = None,
                 learning_rate: float =  1e-6,
                 agent_names: Optional[list[str]] = None,  # type hint simplified
                 num_heads: int = 4,
                 timeframe_embed_dim: int = 64):

        super().__init__()

        # Basic config
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.latent_dim = latent_dim
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.learning_rate = learning_rate
        self.num_heads = num_heads
        self.timeframe_embed_dim = timeframe_embed_dim

        # Timeframes
        self.timeframes = EXPECTED_TIMEFRAMES if num_timeframes is None else EXPECTED_TIMEFRAMES[:num_timeframes]
        self.num_timeframes = len(self.timeframes)

        # Agent names
        if agent_names:
            self.agent_names = sorted(agent_names)
        else:
            self.agent_names = [f"agent_{i}" for i in range(num_agents)]
        self.num_agents = len(self.agent_names)

        # ---------------------------
        # Encoders
        # ---------------------------
        try:
            self.latent_encoder = RobustLatentEncoder(
                state_dim=self.state_dim,
                latent_dim=self.latent_dim,
                num_agents=self.num_agents,
                action_dim=self.action_dim,
                num_timeframes=self.num_timeframes
            ).to(self.device)
        except Exception as e:
            logger.error(f"Failed to create RobustLatentEncoder: {e}")
            self.latent_encoder = None

        try:

            self.shared_latent_encoder = MultiTimeframeSharedLatentEncoder(
                state_dim=self.state_dim,       # âœ… add this
                latent_dim=self.latent_dim,
                num_agents=self.num_agents,
                action_dim=self.action_dim,
                num_timeframes=self.num_timeframes
            ).to(self.device)

        except Exception as e:
            logger.error(f"Failed to create MultiTimeframeSharedLatentEncoder: {e}")
            self.shared_latent_encoder = None

        # Defensive fallbacks
        if self.latent_encoder is None:
            logger.warning("latent_encoder missing â€” creating fallback")
            self.latent_encoder = RobustLatentEncoder(
                state_dim=self.state_dim,
                latent_dim=self.latent_dim,
                num_agents=self.num_agents,
                action_dim=self.action_dim,
                num_timeframes=self.num_timeframes
            ).to(self.device)

        if self.shared_latent_encoder is None:
            logger.warning("shared_latent_encoder missing â€” creating fallback")

            self.shared_latent_encoder = MultiTimeframeSharedLatentEncoder(
                state_dim=self.state_dim,       # âœ… add this
                latent_dim=self.latent_dim,
                num_agents=self.num_agents,
                action_dim=self.action_dim,
                num_timeframes=self.num_timeframes
            ).to(self.device)

        # Encoder optimizers
        self.encoder_optimizer = torch.optim.Adam(self.latent_encoder.parameters(), lr=self.learning_rate)
        self.shared_encoder_optimizer = torch.optim.Adam(self.shared_latent_encoder.parameters(), lr=self.learning_rate)

        # ---------------------------
        # Per-agent entangled models
        # ---------------------------
                # Agent timeframe mapping for proper initialization

        agent_timeframe_map = {
            'xs': ['xs'],
            's': ['xs', 's'],
            'm': ['xs', 's', 'm'],
            'l': ['xs', 's', 'm', 'l'],
            'xl': ['xs', 's', 'm', 'l', 'xl'],
            'xxl': ['xs', 's', 'm', 'l', 'xl', 'xxl'],
            '5m': ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m'],
            '10m': ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']
        }

        self.agents = nn.ModuleDict({
            name: MultiTimeframeEntangledComplexAgent(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_dim=getattr(self, 'hidden_dim', 128),
                latent_dim=self.latent_dim,
                n_heads=getattr(self, 'num_heads', 4),
                n_layers=2,
                timeframes=agent_timeframe_map.get(name, ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']),
                learning_rate=self.learning_rate
            ).to(self.device) for name in self.agent_names
        })

        self.quantum_agents = self.agents

        self.agent_optimizer = torch.optim.Adam(
            [p for agent in self.agents.values() for p in agent.parameters()],
            lr=self.learning_rate
        )

        # ---------------------------
        # DIVERSITY PRESERVATION COMPONENTS (from quantum_diversity_solution.py)
        # ---------------------------
        # 1. Agent-specific latent projector - prevents z_shared dominance
        try:
            self.latent_projector = AgentSpecificLatentProjector(
                latent_dim=self.latent_dim,
                num_agents=self.num_agents,
                diversity_strength=0.3  # Configurable diversity strength
            ).to(self.device)
            logger.info("âœ… AgentSpecificLatentProjector initialized")
        except Exception as e:
            logger.error(f"Failed to create AgentSpecificLatentProjector: {e}")
            self.latent_projector = None

        # 2. Adaptive coordination modules - one per agent
        try:
            self.coordination_modules = nn.ModuleList([
                AdaptiveCoordinationModule(state_dim, action_dim).to(self.device)
                for _ in range(self.num_agents)
            ])
            logger.info(f"âœ… {self.num_agents} AdaptiveCoordinationModules initialized")
        except Exception as e:
            logger.error(f"Failed to create AdaptiveCoordinationModules: {e}")
            self.coordination_modules = None

        # 3. Diversity loss module
        try:
            self.diversity_loss = DiversityLoss(
                num_agents=self.num_agents,
                action_dim=self.action_dim,
                lambda_entropy=0.1,
                lambda_disagree=0.5,
                lambda_spread=0.3
            )
            logger.info("âœ… DiversityLoss module initialized")
        except Exception as e:
            logger.error(f"Failed to create DiversityLoss: {e}")
            self.diversity_loss = None

        # 4. Timeframe state augmenters - prevent state staleness
        try:
            self.state_augmenters = nn.ModuleDict({
                tf: TimeframeStateAugmenter(state_dim, self.timeframes).to(self.device)
                for tf in self.timeframes
            })
            logger.info(f"âœ… TimeframeStateAugmenters initialized for {len(self.timeframes)} timeframes")
        except Exception as e:
            logger.error(f"Failed to create TimeframeStateAugmenters: {e}")
            self.state_augmenters = None

        # 5. Diversity metrics tracker
        self.diversity_tracker = DiversityMetricsTracker(window_size=1000)
        
        # Diversity coefficient for training loss
        self.diversity_coef = 0.2

        # Experience replay
        try:
            self.experience_replay = getattr(self, "experience_replay", None) or ReplayBuffer(
                max_size=200_000,
                input_shape=(self.state_dim,),
                n_actions=self.action_dim,
                device=self.device
            )
        except Exception:
            self.experience_replay = None

        # Training step counter
        self.training_step = 0

        # Trainer initialization
        try:
            if isinstance(self.latent_encoder, nn.Module):
                self.trainer = QuantumSystemTrainer(
                    system=self,
                    buffer=self.experience_replay,
                    batch_size=64,
                    gamma=0.92,
                    device=self.device
                )
                logger.info("âœ“ QuantumSystemTrainer initialized")
            else:
                self.trainer = None
                logger.error("âŒ latent_encoder missing or invalid; trainer not initialized")
        except Exception as e:
            logger.exception(f"Failed to initialize QuantumSystemTrainer: {e}")
            self.trainer = None

        # Attach trainer to quantum_bridge if present
        try:
            if getattr(self, "quantum_bridge", None) is not None:
                self.quantum_bridge.quantum_trainer = self.trainer
        except Exception:
            logger.debug("Could not attach trainer to quantum_bridge at init time")

        print(f"âœ“ MultiTimeframeEntangledComplexAgentSystem initialized on {self.device} with {self.num_agents} agents")

        self.processing_lock = asyncio.Lock()

    @property
    def agents_dict(self) -> Dict[str, nn.Module]:
        return dict(self.agents)

    @torch.no_grad()
    def predict(self, states_dict: Dict[str, Dict[str, np.ndarray]]):
        return self.batched_predict([states_dict])[0]

    @torch.no_grad()
    def batched_predict(self, batch_states: List[Dict[str, Dict[str, np.ndarray]]]):
        """
        FIXED: Robust batched prediction with proper state structure handling

        Args:
            batch_states: List of state dicts with structure:
                         [{agent_name: {timeframe: state_array}}]

        Returns:
            List of tuples: [(q_values_dict, metadata_dict)]
        """
        results = []

        for states_dict in batch_states:
            # ============================================
            # DEFENSIVE: Fix incoming state structure
            # ============================================
            agent_timeframe_states = {}

            for name in self.agent_names:
                agent_states = {}

                # Get agent's data
                agent_data = states_dict.get(name)

                if agent_data is None:
                    # Agent not in states_dict - use zeros
                    for tf in self.timeframes:
                        agent_states[tf] = torch.zeros(
                            1, self.state_dim,
                            dtype=torch.float32,
                            device=self.device
                        )

                elif isinstance(agent_data, np.ndarray):
                    # CASE 1: agent_data is a numpy array (missing timeframe layer)
                    # Convert to proper structure
                    state_array = agent_data.astype(np.float32).flatten()

                    # Ensure correct size
                    if len(state_array) < self.state_dim:
                        state_array = np.pad(
                            state_array,
                            (0, self.state_dim - len(state_array)),
                            mode='constant'
                        )
                    elif len(state_array) > self.state_dim:
                        state_array = state_array[:self.state_dim]

                    # Replicate for all timeframes
                    state_tensor = torch.tensor(
                        state_array,
                        dtype=torch.float32,
                        device=self.device
                    ).unsqueeze(0)

                    for tf in self.timeframes:
                        agent_states[tf] = state_tensor

                elif isinstance(agent_data, dict):
                    # CASE 2: agent_data is a dict (correct structure)
                    for tf in self.timeframes:
                        state = agent_data.get(tf)

                        if state is None:
                            # Timeframe missing - use zeros
                            agent_states[tf] = torch.zeros(
                                1, self.state_dim,
                                dtype=torch.float32,
                                device=self.device
                            )
                        elif isinstance(state, np.ndarray):
                            state_flat = state.astype(np.float32).flatten()

                            # Ensure correct size
                            if len(state_flat) < self.state_dim:
                                state_flat = np.pad(
                                    state_flat,
                                    (0, self.state_dim - len(state_flat)),
                                    mode='constant'
                                )
                            elif len(state_flat) > self.state_dim:
                                state_flat = state_flat[:self.state_dim]

                            agent_states[tf] = torch.tensor(
                                state_flat,
                                dtype=torch.float32,
                                device=self.device
                            ).unsqueeze(0)
                        else:
                            # Unknown type - use zeros
                            agent_states[tf] = torch.zeros(
                                1, self.state_dim,
                                dtype=torch.float32,
                                device=self.device
                            )

                else:
                    # Unknown structure - use zeros
                    for tf in self.timeframes:
                        agent_states[tf] = torch.zeros(
                            1, self.state_dim,
                            dtype=torch.float32,
                            device=self.device
                        )

                agent_timeframe_states[name] = agent_states

            # ============================================
            # Independent Q-values
            # ============================================
            q_values_independent = {}
            all_q_for_encoder = []

            for name in self.agent_names:
                agent = self.agents[name]
                z_dummy = torch.zeros(1, self.latent_dim, device=self.device)

                q_per_tf = {}
                for tf in self.timeframes:
                    q_tensor = agent.get_q_values(
                        {tf: agent_timeframe_states[name][tf]},
                        z_dummy
                    )
                    q_per_tf[tf] = q_tensor.cpu().numpy().flatten()
                    all_q_for_encoder.append(q_tensor.detach().clone().flatten())

                q_values_independent[name] = q_per_tf

            # ============================================
            # Shared latent encoding
            # ============================================
            expected_dim = self.num_agents * len(self.timeframes) * self.action_dim
            all_q_concat = torch.cat([t.view(-1) for t in all_q_for_encoder]).unsqueeze(0)

            if all_q_concat.size(1) < expected_dim:
                pad = torch.zeros(
                    1, expected_dim - all_q_concat.size(1),
                    device=self.device
                )
                all_q_concat = torch.cat([all_q_concat, pad], dim=1)

            z_shared = self.shared_latent_encoder(all_q_concat)

            # ============================================
            # DIVERSITY PRESERVATION: Agent-specific latent projections
            # ============================================
            if self.latent_projector is not None:
                # Get agent-specific latents from shared latent
                z_agents = self.latent_projector(z_shared, training=self.training)
                # z_agents shape: (num_agents, batch, latent_dim)
            else:
                # Fallback to simple expansion
                z_agents = z_shared.unsqueeze(0).expand(self.num_agents, -1, -1)

            # ============================================
            # STATE AUGMENTATION: Add timeframe-specific features
            # ============================================
            augmented_states = {}
            if self.state_augmenters is not None:
                for name in self.agent_names:
                    augmented_states[name] = {}
                    for tf in self.timeframes:
                        try:
                            augmented = self.state_augmenters[tf](
                                agent_timeframe_states[name][tf], tf
                            )
                            augmented_states[name][tf] = augmented
                        except Exception:
                            augmented_states[name][tf] = agent_timeframe_states[name][tf]
            else:
                augmented_states = agent_timeframe_states

            # ============================================
            # Entangled Q-values with agent-specific z
            # ============================================
            q_values_entangled = {}
            coordination_strengths = {}
            coordination_weights = {}

            for i, name in enumerate(self.agent_names):
                agent = self.agents[name]
                states = augmented_states[name]
                
                # Use agent-specific latent
                if self.latent_projector is not None:
                    z = z_agents[i]  # (batch, latent_dim)
                    if z.dim() == 1:
                        z = z.unsqueeze(0)
                else:
                    z = z_shared

                q_per_tf_ent = {}
                for tf in self.timeframes:
                    q_tensor = agent.get_q_values({tf: states[tf]}, z)
                    q_per_tf_ent[tf] = q_tensor.cpu().numpy().flatten()

                q_values_entangled[name] = q_per_tf_ent

                # ============================================
                # ADAPTIVE COORDINATION: Blend independent and entangled
                # ============================================
                try:
                    q_ind_m = torch.tensor(
                        q_values_independent[name].get('m', np.zeros(self.action_dim)),
                        device=self.device
                    ).unsqueeze(0)
                    q_ent_m = torch.tensor(
                        q_per_tf_ent.get('m', np.zeros(self.action_dim)),
                        device=self.device
                    ).unsqueeze(0)
                    state_m = augmented_states[name].get('m', torch.zeros(1, self.state_dim, device=self.device))

                    # Use adaptive coordination module if available
                    if self.coordination_modules is not None and i < len(self.coordination_modules):
                        q_final, coord_weight = self.coordination_modules[i](
                            state_m, q_ind_m, q_ent_m
                        )
                        coordination_weights[name] = coord_weight.mean().item()
                        
                        # Update entangled Q-values with blended values
                        q_values_entangled[name]['m'] = q_final.cpu().numpy().flatten()
                    else:
                        coordination_weights[name] = 0.5  # Default

                    if hasattr(agent, "measure_coordination"):
                        coord = agent.measure_coordination(q_ind_m, q_ent_m)
                    else:
                        coord = 0.0
                except Exception:
                    coord = 0.0
                    coordination_weights[name] = 0.5

                coordination_strengths[name] = coord

            # ============================================
            # Compute Diversity Metrics (if in training mode)
            # ============================================
            diversity_metrics = {}
            if self.training and self.diversity_loss is not None:
                try:
                    # Extract actions from Q-values
                    actions_list = []
                    q_values_list = []
                    for name in self.agent_names:
                        q_vals = torch.tensor(
                            q_values_entangled[name].get('m', np.zeros(self.action_dim)),
                            device=self.device
                        )
                        actions_list.append(torch.argmax(q_vals).item())
                        q_values_list.append(q_vals.unsqueeze(0))
                    
                    actions_batch = torch.tensor(actions_list, device=self.device).unsqueeze(0)
                    _, diversity_metrics = self.diversity_loss(actions_batch, q_values_list)
                    
                    # Update tracker
                    self.diversity_tracker.update(actions_batch, q_values_list, diversity_metrics)
                except Exception as e:
                    logger.debug(f"Diversity metrics computation failed: {e}")

            # ============================================
            # Metadata with diversity info
            # ============================================
            metadata = {
                'coordination_strengths': coordination_strengths,
                'coordination_weights': coordination_weights,
                'avg_coordination': float(np.mean(list(coordination_strengths.values())))
                                   if coordination_strengths else 0.0,
                'avg_coord_weight': float(np.mean(list(coordination_weights.values())))
                                   if coordination_weights else 0.5,
                'z_shared_norm': torch.norm(z_shared).item() if isinstance(z_shared, torch.Tensor) else 0.0,
                'diversity_metrics': diversity_metrics
            }
            
            # Add z_agent variance if using projector
            if self.latent_projector is not None and isinstance(z_agents, torch.Tensor):
                metadata['z_agent_variance'] = torch.var(z_agents).item()

            results.append((q_values_entangled, metadata))


        return results

    # ----------------------------
    # Save/load
    # ----------------------------
    def save_state(self, path: str):
        state = {
            'latent_encoder': self.latent_encoder.state_dict(),
            'agents': {name: agent.state_dict() for name, agent in self.agents.items()},
            'training_step': self.training_step
        }
        torch.save(state, path)
        logger.info(f"Quantum system saved to {path}")

    def load_state(self, path: str):
        checkpoint = torch.load(path, map_location=self.device)
        self.latent_encoder.load_state_dict(checkpoint['latent_encoder'])
        for name, agent in self.agents.items():
            agent.load_state_dict(checkpoint['agents'][name])
        self.training_step = checkpoint.get('training_step', 0)
        logger.info(f"Quantum system loaded from {path}")

# ==============================================================================
# QUANTUM SYSTEM TRAINER
# ==============================================================================

class QuantumSystemTrainer:
    """Trainer for Quantum / MultiTimeframe system with diversity preservation"""

    def __init__(self, system, buffer, batch_size=64, gamma=0.92, device=None, ent_coef=0.01,
                 diversity_coef=0.2):
        self.system = system
        self.buffer = buffer
        self.batch_size = batch_size
        self.gamma = gamma
        self.ent_coef = ent_coef
        self.diversity_coef = diversity_coef  # Coefficient for diversity loss
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")

        # âœ… FIX: Convert to torch device object for consistency
        if isinstance(self.device, str):
            self.device = torch.device(self.device)

        # --- Ensure system.agents is a dict of nn.Module ---
        # Support both nn.ModuleDict and plain dict
        agents = getattr(system, 'agents_dict', None)
        if agents is None:
            if hasattr(system, 'agents') and isinstance(system.agents, torch.nn.ModuleDict):
                agents = dict(system.agents)
            elif hasattr(system, 'agents') and isinstance(system.agents, dict):
                agents = system.agents
            else:
                raise ValueError("QuantumSystemTrainer requires system.agents as dict of nn.Module")
        self.agents = agents

        # --- Collect trainable parameters ---
        self.trainable_params = []

        # Latent encoder
        if hasattr(self.system, "latent_encoder") and isinstance(self.system.latent_encoder, torch.nn.Module):
            self.trainable_params += list(self.system.latent_encoder.parameters())
        else:
            raise ValueError("QuantumSystemTrainer requires system.latent_encoder as nn.Module")

        # Agents
        for name, agent in self.agents.items():
            if isinstance(agent, torch.nn.Module):
                self.trainable_params += list(agent.parameters())
            else:
                raise ValueError(f"Agent '{name}' must be a torch.nn.Module")

        if not self.trainable_params:
            raise ValueError("No trainable parameters found in system!")

        # --- Setup optimizer ---
        self.optimizer = torch.optim.Adam(self.trainable_params, lr=1e-6)

        # Optional: learning step counter
        self.training_step = 0

        print(f"âœ… QuantumSystemTrainer initialized on {self.device} with {len(self.trainable_params)} parameters")

        # Gradient clipping
        self.max_grad_norm = 1.0

        # Q-value bounds
        self.q_clip_range = 100.0

        # Loss scaling
        self.loss_scale = 0.001

        # Reward normalization (running statistics)
        self.reward_mean = 0.0
        self.reward_std = 1.0
        self.reward_history = deque(maxlen=1000)

        # Safety monitoring
        self.max_loss_threshold = 100.0
        self.max_q_threshold = 1000.0
        self.safety_checks_enabled = True

        # Training step counter
        self.training_step = 0

        # ---------------------------
        # DIVERSITY PRESERVATION INTEGRATION
        # ---------------------------
        # Get diversity loss from system if available, otherwise create new
        if hasattr(self.system, 'diversity_loss') and self.system.diversity_loss is not None:
            self.diversity_loss = self.system.diversity_loss
        else:
            self.diversity_loss = DiversityLoss(
                num_agents=len(self.agents),
                action_dim=getattr(self.system, 'action_dim', 2),
                lambda_entropy=0.1,
                lambda_disagree=0.5,
                lambda_spread=0.3
            )
        
        # Get diversity tracker from system if available
        if hasattr(self.system, 'diversity_tracker'):
            self.diversity_tracker = self.system.diversity_tracker
        else:
            self.diversity_tracker = DiversityMetricsTracker(window_size=1000)
        
        # Add latent projector and coordination module parameters to trainable params
        if hasattr(self.system, 'latent_projector') and self.system.latent_projector is not None:
            self.trainable_params += list(self.system.latent_projector.parameters())
            logger.info("âœ… Added AgentSpecificLatentProjector params to optimizer")
        
        if hasattr(self.system, 'coordination_modules') and self.system.coordination_modules is not None:
            for coord_module in self.system.coordination_modules:
                self.trainable_params += list(coord_module.parameters())
            logger.info("âœ… Added AdaptiveCoordinationModule params to optimizer")
        
        if hasattr(self.system, 'state_augmenters') and self.system.state_augmenters is not None:
            for augmenter in self.system.state_augmenters.values():
                self.trainable_params += list(augmenter.parameters())
            logger.info("âœ… Added TimeframeStateAugmenter params to optimizer")
        
        # Recreate optimizer with all params
        self.optimizer = torch.optim.Adam(self.trainable_params, lr=1e-6)
        print(f"âœ… QuantumSystemTrainer with diversity: {len(self.trainable_params)} params, diversity_coef={diversity_coef}")
        
        # V8.6.2: Training step counter for metrics display frequency
        self._metrics_display_interval = 1  # Print every N training steps
        self._last_metrics_display = 0
        
        # V5.0.6: Entropy bonus configuration
        self._entropy_coef_initial = 0.1
        self._entropy_coef_min = 0.01
        self._entropy_warmup_steps = 100
        self._entropy_decay_start = 5000
        self._entropy_decay_rate = 0.9995
        
        # V5.0.6: Collapse recovery tracking
        self._collapse_counters = {name: 0 for name in self.agents.keys()}
        self._collapse_reset_threshold = 10  # Reset after N consecutive collapses
        self._soft_reset_noise = 0.05
        
        print(f"âœ… V5.0.6 Anti-Collapse: entropy_coef={self._entropy_coef_initial}, decay_rate={self._entropy_decay_rate}")

    # ====================================================================
    # V5.0.6: ENTROPY COEFFICIENT SCHEDULE
    # ====================================================================
    def _get_entropy_coef(self) -> float:
        """
        Get entropy coefficient with warmup and decay schedule.
        
        Schedule:
        - Steps 0-100: Warmup from 0.05 to entropy_coef_initial
        - Steps 100-5000: Hold at entropy_coef_initial
        - Steps 5000+: Decay to entropy_coef_min
        
        Returns:
            Current entropy coefficient
        """
        step = self.training_step
        
        if step < self._entropy_warmup_steps:
            # Warmup phase: linear increase
            warmup_progress = step / self._entropy_warmup_steps
            return 0.05 + (self._entropy_coef_initial - 0.05) * warmup_progress
        
        elif step < self._entropy_decay_start:
            # Hold phase: constant at initial value
            return self._entropy_coef_initial
        
        else:
            # Decay phase: exponential decay
            decay_steps = step - self._entropy_decay_start
            coef = self._entropy_coef_initial * (self._entropy_decay_rate ** decay_steps)
            return max(self._entropy_coef_min, coef)
    
    # ====================================================================
    # V5.0.6: COLLAPSE RECOVERY MECHANISM
    # ====================================================================
    def _check_and_recover_collapsed_agents(self, per_agent_metrics: Dict):
        """
        Detect collapsed agents and trigger soft reset if collapse persists.
        
        A soft reset adds noise to actor weights to break out of local minima.
        This is a last-resort recovery mechanism.
        
        Args:
            per_agent_metrics: Dict with per-agent metrics including entropy
        """
        for agent_name, metrics in per_agent_metrics.items():
            # Compute entropy from buy_ratio
            buy_ratio = metrics.get('buy_ratio', 0.5)
            buy_ratio = max(1e-8, min(1 - 1e-8, buy_ratio))
            sell_ratio = 1.0 - buy_ratio
            entropy = -(buy_ratio * np.log2(buy_ratio) + sell_ratio * np.log2(sell_ratio))
            
            if entropy < 0.05:  # Severe collapse
                self._collapse_counters[agent_name] = self._collapse_counters.get(agent_name, 0) + 1
                
                if self._collapse_counters[agent_name] >= self._collapse_reset_threshold:
                    # Trigger soft reset
                    self._soft_reset_agent(agent_name)
                    self._collapse_counters[agent_name] = 0
                    print(f"âš ï¸ V5.0.6: Soft reset triggered for collapsed agent: {agent_name}")
            else:
                # Reset counter if agent recovers
                self._collapse_counters[agent_name] = 0
    
    def _soft_reset_agent(self, agent_name: str):
        """
        Add noise to agent's actor weights to break out of local minimum.
        
        Only modifies actor layers, not critic.
        """
        if agent_name not in self.agents:
            return
        
        agent = self.agents[agent_name]
        
        with torch.no_grad():
            for name, param in agent.named_parameters():
                # Only modify actor layers
                if 'actor' in name.lower() and param.dim() > 1:
                    noise = torch.randn_like(param) * self._soft_reset_noise
                    param.add_(noise)
                    logger.info(f"[SOFT RESET] Added noise to {agent_name}.{name}")

    # ====================================================================
    # V8.6.2: COMPREHENSIVE TRAINING METRICS DISPLAY
    # V5.0.5: Enhanced with MARL Agent-Level Diagnostics
    # V5.0.6: Enhanced with exploration stats and entropy bonus
    # ====================================================================
    def _print_training_metrics(self, metrics: Dict[str, Any], batch_size: int):
        """
        Print comprehensive per-agent and per-timeframe training metrics.
        
        V5.0.5 Enhancement: Integrates AgentLevelDiagnostics to detect:
        - Policy collapse (near-deterministic action distributions)
        - Over-coordination (agents taking identical actions too often)
        - Q-value saturation (exploding value estimates)
        
        This provides visibility into:
        - Per-agent Q-value distributions
        - Per-agent action tendencies (buy/sell ratios)
        - Per-agent policy entropy (collapse detection)
        - Per-timeframe Q-value aggregates
        - Diversity metrics
        - Buffer statistics
        
        References:
        - Lowe et al. 2017 (MADDPG over-coordination)
        - Rashid et al. 2018 (QMIX value decomposition)
        - Foerster et al. 2018 (COMA credit assignment)
        """
        self.training_step += 1
        
        # Only print detailed metrics every N steps to avoid log spam
        if self.training_step % self._metrics_display_interval != 0:
            return
        
        print("\n" + "="*100)
        print(f"{'K1RL_QU1NT V5.0.6 TRAINING METRICS':^100}")
        print(f"{'Step ' + str(self.training_step) + ' | Batch Size: ' + str(batch_size):^100}")
        print("="*100)
        
        # ----------------------------------------------------------------
        # SECTION 1: LOSS BREAKDOWN
        # V5.0.6: Added entropy bonus display
        # ----------------------------------------------------------------
        print("\nğŸ“Š LOSS BREAKDOWN")
        print("-"*60)
        print(f"  {'Total Loss:':<25} {metrics.get('total_loss', 0):.6f}")
        print(f"  {'Actor Loss:':<25} {metrics.get('actor_loss', 0):.6f}")
        print(f"  {'Critic Loss:':<25} {metrics.get('critic_loss', 0):.6f}")
        # V5.0.6: Entropy bonus (negative = rewarding entropy)
        if 'entropy_bonus' in metrics:
            entropy_bonus = metrics.get('entropy_bonus', 0)
            entropy_coef = metrics.get('entropy_coef', 0)
            bonus_indicator = "âœ…" if entropy_bonus > 0 else "âš¡"
            print(f"  {'Entropy Bonus:':<25} {entropy_bonus:+.6f} {bonus_indicator} (Î²={entropy_coef:.4f})")
        print(f"  {'Entanglement Loss:':<25} {metrics.get('entanglement_loss', 0):.6f}")
        print(f"  {'Diversity Loss:':<25} {metrics.get('diversity_loss', 0):.6f}")
        if 'entropy_loss' in metrics:
            print(f"  {'  â””â”€ Entropy:':<25} {metrics.get('entropy_loss', 0):.6f}")
        if 'disagreement_loss' in metrics:
            print(f"  {'  â””â”€ Disagreement:':<25} {metrics.get('disagreement_loss', 0):.6f}")
        if 'spread_loss' in metrics:
            spread_loss = metrics.get('spread_loss', 0)
            spread_indicator = "âœ…" if spread_loss < 5.0 else "âš ï¸ HIGH"
            print(f"  {'  â””â”€ Q-Spread:':<25} {spread_loss:.6f} {spread_indicator}")
        
        # ----------------------------------------------------------------
        # V5.0.6: SECTION 1.5: EXPLORATION STATS
        # ----------------------------------------------------------------
        exploration_stats = metrics.get('exploration', {})
        if exploration_stats:
            print("\nğŸ² EXPLORATION STATS")
            print("-"*60)
            print(f"  {'Temperature:':<25} {exploration_stats.get('temperature', 1.0):.4f}")
            print(f"  {'Mean Epsilon:':<25} {exploration_stats.get('mean_epsilon', 0):.4f}")
            print(f"  {'Random Action Rate:':<25} {exploration_stats.get('random_action_rate', 0)*100:.1f}%")
            collapsed_count = exploration_stats.get('collapsed_agents_count', 0)
            if collapsed_count > 0:
                print(f"  {'Collapsed Agents:':<25} {collapsed_count} (receiving boosted exploration)")
        
        # ----------------------------------------------------------------
        # SECTION 2: PER-AGENT METRICS WITH POLICY COLLAPSE DETECTION
        # V5.0.5: Enhanced with entropy and collapse indicators
        # ----------------------------------------------------------------
        per_agent = metrics.get('per_agent', {})
        if per_agent:
            print("\nğŸ¤– PER-AGENT METRICS")
            print("-"*110)
            print(f"  {'Agent':<12} {'Q-Mean':>9} {'Q-Std':>9} {'Q-Spread':>9} {'BUY %':>8} {'SELL %':>8} {'Entropy':>8} {'Samples':>8} {'Status':<12}")
            print(f"  {'-'*12} {'-'*9} {'-'*9} {'-'*9} {'-'*8} {'-'*8} {'-'*8} {'-'*8} {'-'*12}")
            
            # Track diagnostics for aggregate analysis
            collapsed_agents = []
            saturated_agents = []
            agent_actions = {}
            agent_q_values = {}
            
            for agent_name in sorted(per_agent.keys()):
                am = per_agent[agent_name]
                buy_pct = am.get('buy_ratio', 0) * 100
                sell_pct = am.get('sell_ratio', 0) * 100
                q_mean = am.get('q_mean', 0)
                q_std = am.get('q_std', 0)
                q_max = am.get('q_max', 0)
                q_min = am.get('q_min', 0)
                q_spread = am.get('q_spread_avg', 0)
                
                # V5.0.5: Compute policy entropy from action distribution
                # Shannon entropy in bits: H = -Î£ p*log2(p)
                # Max entropy for binary = 1.0 bit (uniform)
                buy_ratio = max(1e-8, min(1 - 1e-8, am.get('buy_ratio', 0.5)))
                sell_ratio = 1.0 - buy_ratio
                entropy = -(buy_ratio * np.log2(buy_ratio) + sell_ratio * np.log2(sell_ratio))
                
                # Store for correlation analysis
                agent_actions[agent_name] = 0 if buy_ratio > 0.5 else 1
                agent_q_values[agent_name] = np.array([q_mean - q_std, q_mean + q_std])
                
                # V5.0.5: Detect policy collapse (entropy < 0.1 bits = >97% one action)
                is_collapsed = entropy < 0.15
                if is_collapsed:
                    collapsed_agents.append(agent_name)
                
                # V5.0.5: Detect Q-value saturation (|Q| > 5 is concerning)
                is_saturated = max(abs(q_max), abs(q_min)) > 5.0
                if is_saturated:
                    saturated_agents.append(agent_name)
                
                # Status indicators combining multiple checks
                status_parts = []
                if is_collapsed:
                    status_parts.append("ğŸ”´COLL")
                elif entropy < 0.5:
                    status_parts.append("ğŸŸ¡LOW-H")
                if is_saturated:
                    status_parts.append("ğŸŸ SAT")
                if not status_parts:
                    if 30 <= buy_pct <= 70:
                        status_parts.append("âœ…OK")
                    else:
                        status_parts.append("ğŸŸ¡BIAS")
                
                status_str = " ".join(status_parts)
                
                print(f"  {agent_name:<12} "
                      f"{q_mean:>9.4f} "
                      f"{q_std:>9.4f} "
                      f"{q_spread:>9.4f} "
                      f"{buy_pct:>7.1f}% "
                      f"{sell_pct:>7.1f}% "
                      f"{entropy:>8.4f} "
                      f"{am.get('samples', 0):>8} "
                      f"{status_str:<12}")
            
            # ----------------------------------------------------------------
            # V5.0.5: POLICY COLLAPSE AND OVER-COORDINATION WARNINGS
            # ----------------------------------------------------------------
            print("")
            
            # Policy collapse warning (agents with near-deterministic policies)
            if collapsed_agents:
                print(f"  âš ï¸  POLICY COLLAPSE DETECTED in {len(collapsed_agents)} agents: {', '.join(collapsed_agents)}")
                print(f"     These agents have entropy < 0.15 bits (>97% same action).")
                print(f"     This indicates insufficient exploration or gradient suppression.")
            
            # Q-value saturation warning
            if saturated_agents:
                print(f"  âš ï¸  Q-VALUE SATURATION in {len(saturated_agents)} agents: {', '.join(saturated_agents)}")
                print(f"     |Q| > 5.0 detected. Consider gradient clipping or learning rate reduction.")
            
            # Over-coordination detection (pairwise action agreement)
            if len(agent_actions) >= 2:
                action_list = list(agent_actions.values())
                unique_actions = len(set(action_list))
                
                # Compute mean pairwise agreement
                same_action_count = 0
                total_pairs = 0
                for i, a1 in enumerate(action_list):
                    for j, a2 in enumerate(action_list):
                        if i < j:
                            if a1 == a2:
                                same_action_count += 1
                            total_pairs += 1
                
                agreement_rate = same_action_count / max(1, total_pairs)
                
                if agreement_rate > 0.85:
                    print(f"  âš ï¸  OVER-COORDINATION DETECTED: {agreement_rate:.1%} action agreement")
                    print(f"     {unique_actions}/{len(agent_actions)} unique actions this batch.")
                    print(f"     Agents may be ignoring local state information (CTDE failure).")
            
            # Agent diversity check (variance in buy ratios)
            buy_ratios = [am.get('buy_ratio', 0.5) for am in per_agent.values()]
            if buy_ratios:
                buy_variance = np.var(buy_ratios)
                buy_std = np.std(buy_ratios)
                
                if buy_variance < 0.01:
                    print(f"\n  âš ï¸  LOW AGENT DIVERSITY: Variance={buy_variance:.4f}, Std={buy_std:.4f}")
                    print(f"     Agents converging to identical policies (MARL homogenisation).")
                elif buy_variance < 0.05:
                    print(f"\n  ğŸŸ¡ MODERATE DIVERSITY: Variance={buy_variance:.4f}, Std={buy_std:.4f}")
                else:
                    print(f"\n  âœ… HEALTHY DIVERSITY: Variance={buy_variance:.4f}, Std={buy_std:.4f}")
        
        # ----------------------------------------------------------------
        # SECTION 3: PER-TIMEFRAME METRICS  
        # ----------------------------------------------------------------
        per_tf = metrics.get('per_timeframe', {})
        if per_tf:
            print("\nâ±ï¸  PER-TIMEFRAME METRICS")
            print("-"*80)
            print(f"  {'Timeframe':<12} {'Q-Mean':>12} {'Q-Std':>12} {'BUY %':>12} {'SELL %':>12} {'Samples':>12}")
            print(f"  {'-'*12} {'-'*12} {'-'*12} {'-'*12} {'-'*12} {'-'*12}")
            
            tf_order = ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']
            for tf in tf_order:
                if tf in per_tf:
                    tm = per_tf[tf]
                    buy_pct = tm.get('buy_ratio', 0) * 100
                    sell_pct = tm.get('sell_ratio', 0) * 100
                    print(f"  {tf:<12} "
                          f"{tm.get('q_mean', 0):>12.4f} "
                          f"{tm.get('q_std', 0):>12.4f} "
                          f"{buy_pct:>11.1f}% "
                          f"{sell_pct:>11.1f}% "
                          f"{tm.get('samples', 0):>12}")
        
        # ----------------------------------------------------------------
        # SECTION 4: BUFFER STATISTICS
        # ----------------------------------------------------------------
        print("\nğŸ“¦ BUFFER STATISTICS")
        print("-"*60)
        print(f"  {'Buffer Size:':<25} {metrics.get('buffer_size', 'N/A')}")
        print(f"  {'Buffer Utilization:':<25} {metrics.get('buffer_utilization', 0)*100:.1f}%")
        
        buffer_per_agent = metrics.get('buffer_per_agent', {})
        if buffer_per_agent:
            print(f"  {'Per-Agent Samples:':<25}")
            for agent_name, count in sorted(buffer_per_agent.items()):
                print(f"    {agent_name:<22} {count:>8} samples")
        
        # ----------------------------------------------------------------
        # SECTION 5: TRAINING PERFORMANCE
        # ----------------------------------------------------------------
        print("\nâš¡ TRAINING PERFORMANCE")
        print("-"*60)
        print(f"  {'Training Time:':<25} {metrics.get('training_time_ms', 0):.1f} ms")
        print(f"  {'Samples/Second:':<25} {metrics.get('samples_per_second', 0):.1f}")
        print(f"  {'Actors Updated:':<25} {metrics.get('actors_updated', 0)}")
        print(f"  {'Critics Updated:':<25} {metrics.get('critics_updated', 0)}")
        
        # ----------------------------------------------------------------
        # V5.0.5: SECTION 6: MARL DIAGNOSTIC SUMMARY
        # ----------------------------------------------------------------
        if per_agent:
            # Aggregate MARL health metrics
            entropies = []
            for agent_name in per_agent:
                am = per_agent[agent_name]
                buy_ratio = max(1e-8, min(1 - 1e-8, am.get('buy_ratio', 0.5)))
                sell_ratio = 1.0 - buy_ratio
                entropy = -(buy_ratio * np.log2(buy_ratio) + sell_ratio * np.log2(sell_ratio))
                entropies.append(entropy)
            
            mean_entropy = np.mean(entropies)
            min_entropy = np.min(entropies)
            
            print("\nğŸ”¬ MARL DIAGNOSTIC SUMMARY")
            print("-"*60)
            print(f"  {'Mean Policy Entropy:':<25} {mean_entropy:.4f} bits (max=1.0)")
            print(f"  {'Min Policy Entropy:':<25} {min_entropy:.4f} bits")
            print(f"  {'Collapsed Agents:':<25} {len(collapsed_agents)}/{len(per_agent)}")
            print(f"  {'Saturated Agents:':<25} {len(saturated_agents)}/{len(per_agent)}")
            
            # Overall system health
            if len(collapsed_agents) == 0 and mean_entropy > 0.5:
                print(f"  {'System Health:':<25} âœ… HEALTHY")
            elif len(collapsed_agents) <= 2 and mean_entropy > 0.3:
                print(f"  {'System Health:':<25} ğŸŸ¡ DEGRADED")
            else:
                print(f"  {'System Health:':<25} ğŸ”´ CRITICAL - Policy collapse in progress")
            
            # ==============================================================
            # V5.0.6: AUTOMATIC COLLAPSE RECOVERY
            # ==============================================================
            # If agents are persistently collapsed, trigger soft reset
            # to break out of local minima.
            # ==============================================================
            if collapsed_agents:
                # Build per-agent metrics dict for collapse recovery
                collapse_metrics = {}
                for agent_name in per_agent:
                    am = per_agent[agent_name]
                    buy_ratio = max(1e-8, min(1 - 1e-8, am.get('buy_ratio', 0.5)))
                    sell_ratio = 1.0 - buy_ratio
                    entropy = -(buy_ratio * np.log2(buy_ratio) + sell_ratio * np.log2(sell_ratio))
                    collapse_metrics[agent_name] = {'entropy': entropy}
                
                try:
                    self._check_and_recover_collapsed_agents(collapse_metrics)
                except Exception as e:
                    logger.debug(f"[COLLAPSE RECOVERY] Check failed: {e}")
        
        print("\n" + "="*100 + "\n")

    # ------------------------------------------------------------------
    def _forward_batch_tensors(self, batch_states: List[Dict[str, Dict[str, np.ndarray]]], require_grad: bool = True
                              ) -> Tuple[List[Dict[str, Dict[str, torch.Tensor]]], List[Dict[str, Any]]]:
        """Differentiable forward pass returning q-value tensors and metadata"""
        q_tensor_list = []
        meta_list = []

        for states_dict in batch_states:
            agent_timeframe_states = {}
            for name in self.system.agent_names:
                agent_states = {}
                for tf in self.system.timeframes:
                    state = states_dict.get(name, {}).get(tf, np.zeros(self.system.state_dim, dtype=np.float32))
                    st = torch.tensor(state, dtype=torch.float32, device=self.device)
                    if st.dim() == 1:
                        st = st.unsqueeze(0)
                    agent_states[tf] = st
                agent_timeframe_states[name] = agent_states

            # Compute independent q-values
            all_q_for_encoder = []
            q_values_independent = {}
            for name in self.system.agent_names:
                agent = self.system.agents[name]
                z_zero = torch.zeros(1, self.system.latent_dim, device=self.device)
                q_vals_per_tf = {}
                for tf in self.system.timeframes:
                    tf_states = {tf: agent_timeframe_states[name][tf]}
                    q_vals = agent.get_q_values(tf_states, z_zero)
                    if not require_grad:
                        q_vals = q_vals.detach()
                    q_vals_per_tf[tf] = q_vals
                    all_q_for_encoder.append(q_vals.view(-1))
                q_values_independent[name] = {tf: q_vals.detach().cpu().numpy().flatten() for tf, q_vals in q_vals_per_tf.items()}

            all_q_concat = torch.cat(all_q_for_encoder).unsqueeze(0) if all_q_for_encoder else torch.zeros(
                1, self.system.num_agents * len(self.system.timeframes) * self.system.action_dim, device=self.device)

            # Shared latent
            if require_grad:
                z_shared = self.system.latent_encoder(all_q_concat)
            else:
                with torch.no_grad():
                    z_shared = self.system.latent_encoder(all_q_concat).detach()

            z_shared_expanded = z_shared.expand(self.system.num_agents, -1)

            # ==============================================================
            # V5.0.6: GATED ENTANGLEMENT - Reduce z_shared Influence
            # ==============================================================
            # Use GatedEntanglement to blend local vs entangled Q-values.
            # This prevents z_shared from overwhelming local state information.
            # Gate is capped at 30% to ensure local signals always dominate.
            # ==============================================================
            gated_ent = getattr(self.system, '_gated_entanglement', None)
            
            # Entangled q-values with optional gating
            q_vals_entangled_tensors = {}
            for i, name in enumerate(self.system.agent_names):
                agent = self.system.agents[name]
                z = z_shared_expanded[i:i+1]
                q_vals_per_tf = {}
                
                for tf in self.system.timeframes:
                    tf_states = {tf: agent_timeframe_states[name][tf]}
                    state_tensor = agent_timeframe_states[name][tf]
                    
                    # Get entangled Q-values
                    q_vals_ent = agent.get_q_values(tf_states, z)
                    
                    # V5.0.6: Apply gated entanglement if available
                    if gated_ent is not None and name in q_values_independent:
                        try:
                            # Get independent Q-values for this agent/timeframe
                            q_indep_np = q_values_independent[name].get(tf)
                            if q_indep_np is not None:
                                q_indep = torch.tensor(q_indep_np, dtype=torch.float32, device=self.device)
                                if q_indep.dim() == 1:
                                    q_indep = q_indep.unsqueeze(0)
                                
                                # Ensure shapes match
                                if q_indep.shape == q_vals_ent.shape:
                                    # Blend local and entangled using learned gate
                                    # Gate caps z_shared influence at 30%
                                    q_vals_ent, gate_val = gated_ent(
                                        local_q=q_indep,
                                        entangled_q=q_vals_ent,
                                        state=state_tensor
                                    )
                        except Exception as e:
                            # Fall back to pure entangled if gating fails
                            logger.debug(f"[GATED_ENT] Gating failed for {name}/{tf}: {e}")
                    
                    if not require_grad:
                        q_vals_ent = q_vals_ent.detach()
                    q_vals_per_tf[tf] = q_vals_ent
                q_vals_entangled_tensors[name] = q_vals_per_tf

            metadata = {'z_shared': z_shared, 'independent_q_values': q_values_independent}
            q_tensor_list.append(q_vals_entangled_tensors)
            meta_list.append(metadata)

        return q_tensor_list, meta_list

    # ------------------------------------------------------------------

    # ========================================================================
    # BUGFIX #1: Action Conversion Helper
    # ========================================================================
    def convert_action_to_index(self, a):
        """
        Robust action-to-index conversion for quantum system.
        Handles: int, float, one-hot array, one-hot tensor, scalar tensor

        Args:
            a: Action in various formats (int, tensor, array, list)

        Returns:
            int: Action index (0 or 1)
        """
        # Already an integer
        if isinstance(a, int):
            return a

        # Convert numpy to tensor if needed
        if isinstance(a, np.ndarray):
            if a.size == 0:
                return 0
            a = torch.tensor(a, dtype=torch.float32)

        # Handle tensor conversion
        if torch.is_tensor(a):
            # Multi-element tensor (one-hot encoded)
            if a.numel() > 1:
                try:
                    return int(torch.argmax(a).item())
                except Exception as e:
                    logger.error(f"Failed to get argmax: {e}")
                    return 0
            # Single element tensor
            elif a.numel() == 1:
                try:
                    return int(a.item())
                except Exception as e:
                    logger.error(f"Failed to get item: {e}")
                    return 0
            else:
                return 0

        # Handle lists/tuples
        if isinstance(a, (list, tuple)):
            if len(a) > 1:
                return int(np.argmax(a))
            elif len(a) == 1:
                return int(a[0])
            else:
                return 0

        # Try to convert to int directly
        try:
            return int(a)
        except:
            return 0


    def _compute_losses(self, q_tensors, q_next_tensors, actions, rewards, dones):
        """
        FIXED: Prevents exploding losses via clipping and normalization

        Key fixes:
        1. Reward normalization with running statistics
        2. Q-value clipping to [-100, 100]
        3. Target clipping
        4. Loss scaling
        5. Safety checks
        """

        # ========================================================================
        # FIX #1: NORMALIZE REWARDS FIRST
        # ========================================================================
        if isinstance(rewards, torch.Tensor):
            # Update running statistics
            batch_mean = rewards.mean().item()
            batch_std = rewards.std().item() + 1e-8

            self.reward_mean = 0.99 * self.reward_mean + 0.01 * batch_mean
            self.reward_std = 0.99 * self.reward_std + 0.01 * batch_std

            # Normalize to zero mean, unit variance
            rewards = (rewards - self.reward_mean) / (self.reward_std + 1e-8)

            # Clip to prevent extreme values
            rewards = torch.clamp(rewards, -10.0, 10.0)

            logger.debug(f"[REWARD NORM] mean={self.reward_mean:.4f}, std={self.reward_std:.4f}")

        q_loss_total = torch.tensor(0.0, device=self.device)
        ent_loss_total = torch.tensor(0.0, device=self.device)
        mse = nn.MSELoss()

        expected_timeframes = getattr(self.system, "timeframes", [])
        expected_agents = getattr(self.system, "agent_names", [])

        valid_batches = 0

        for batch_idx, (q_set, q_next_set, a, r, d) in enumerate(
            zip(q_tensors, q_next_tensors, actions, rewards, dones)
        ):
            # Validate minimum agents
            available_agents = [name for name in expected_agents
                              if name in q_set and name in q_next_set]

            min_required = max(1, len(expected_agents) // 3)
            if len(available_agents) < min_required:
                logger.debug(f"[Batch {batch_idx}] Insufficient agents, skipping")
                continue

            # Get batch size
            try:
                if isinstance(r, torch.Tensor):
                    if r.dim() == 0:
                        batch_size = 1
                    else:
                        batch_size = r.size(0)
                else:
                    batch_size = 1
            except:
                batch_size = 1

            if batch_size == 0:
                continue

            q_values_all = []
            q_next_all = []

            # Collect Q-values from all agents/timeframes
            for name in available_agents:
                for tf in expected_timeframes:
                    qv = q_set[name].get(tf, None)
                    qn = q_next_set[name].get(tf, None)

                    # Handle missing timeframes
                    if qv is None or qn is None:
                        shape = (batch_size, 2)
                        qv = torch.zeros(shape, device=self.device) if qv is None else qv
                        qn = torch.zeros(shape, device=self.device) if qn is None else qn

                    # Normalize dimensions
                    if qv.dim() == 1:
                        qv = qv.unsqueeze(0)
                    if qn.dim() == 1:
                        qn = qn.unsqueeze(0)

                    # ============================================================
                    # FIX #2: CLIP Q-VALUES TO PREVENT EXPLOSION
                    # ============================================================
                    qv = torch.clamp(qv, -self.q_clip_range, self.q_clip_range)
                    qn = torch.clamp(qn, -self.q_clip_range, self.q_clip_range)

                    q_values_all.append(qv)
                    q_next_all.append(qn)

            if not q_values_all or not q_next_all:
                continue

            # Validate batch consistency
            batch_sizes_q = [t.shape[0] for t in q_values_all]
            if len(set(batch_sizes_q)) > 1:
                logger.warning(f"[Batch {batch_idx}] Inconsistent batch sizes, skipping")
                continue

            # Concatenate Q-values
            try:
                q_values_cat = torch.cat(q_values_all, dim=-1)
                q_next_cat = torch.cat(q_next_all, dim=-1).detach()
            except RuntimeError as e:
                logger.warning(f"[Batch {batch_idx}] Concatenation failed: {e}")
                continue

            # ================================================================
            # FIX #3: CLIP CONCATENATED VALUES AGAIN (SAFETY)
            # ================================================================
            q_values_cat = torch.clamp(q_values_cat, -self.q_clip_range, self.q_clip_range)
            q_next_cat = torch.clamp(q_next_cat, -self.q_clip_range, self.q_clip_range)

            # Prepare reward and done tensors
            r_tensor = r if isinstance(r, torch.Tensor) else torch.tensor(
                r, dtype=torch.float32, device=self.device
            )
            d_tensor = d if isinstance(d, torch.Tensor) else torch.tensor(
                d, dtype=torch.float32, device=self.device
            )

            if r_tensor.dim() == 0:
                r_tensor = r_tensor.unsqueeze(0)
            if d_tensor.dim() == 0:
                d_tensor = d_tensor.unsqueeze(0)

            # ================================================================
            # FIX #4: COMPUTE TARGET WITH CLIPPING
            # ================================================================
            next_q_max = torch.max(q_next_cat, dim=-1)[0]
            next_q_max = torch.clamp(next_q_max, -self.q_clip_range, self.q_clip_range)

            # Bellman equation: Q(s,a) = r + Î³ * max Q(s',a')
            target = r_tensor + self.gamma * next_q_max * (1 - d_tensor)
            target = torch.clamp(target, -self.q_clip_range, self.q_clip_range)

            # Get predicted Q-value
            a_idx = self.convert_action_to_index(a)
            predicted = q_values_cat[..., a_idx] if q_values_cat.dim() > 1 else q_values_cat
            predicted = torch.clamp(predicted, -self.q_clip_range, self.q_clip_range)

            # ================================================================
            # FIX #5: COMPUTE LOSS WITH SAFETY CHECK
            # ================================================================
            q_loss = mse(predicted, target.detach())

            # Safety: Skip batch if loss too large
            if q_loss.item() > self.max_loss_threshold:
                logger.warning(
                    f"âš ï¸  [Batch {batch_idx}] Loss {q_loss.item():.2f} exceeds "
                    f"threshold {self.max_loss_threshold}, skipping"
                )
                continue

            # ================================================================
            # FIX #6: SCALE LOSS TO PREVENT GRADIENT EXPLOSION
            # ================================================================
            q_loss = q_loss * self.loss_scale
            q_loss_total += q_loss

            # ================================================================
            # ENTANGLEMENT LOSS (Reduced pairs for performance)
            # ================================================================
            ent_loss = torch.tensor(0.0, device=self.device)

            if len(q_values_all) > 1:
                import random

                # Sample pairs instead of computing all combinations
                num_pairs = min(50, len(q_values_all) * (len(q_values_all) - 1) // 2)

                if num_pairs > 0:
                    indices = list(range(len(q_values_all)))
                    pairs = []

                    for _ in range(num_pairs):
                        i, j = random.sample(indices, 2)
                        if i > j:
                            i, j = j, i
                        pairs.append((i, j))

                    # Compute correlations
                    for i, j in pairs:
                        try:
                            corr = torch.corrcoef(torch.stack([
                                q_values_all[i].flatten(),
                                q_values_all[j].flatten()
                            ]))[0, 1]

                            if not torch.isnan(corr):
                                ent_loss += corr.abs()
                        except:
                            pass

                    # Average instead of sum
                    if num_pairs > 0:
                        ent_loss = ent_loss / num_pairs

            ent_loss_total += ent_loss
            valid_batches += 1

        # ====================================================================
        # RETURN AVERAGE LOSSES WITH FINAL CLIPPING
        # ====================================================================
        if valid_batches > 0:
            avg_q_loss = q_loss_total / valid_batches
            avg_ent_loss = ent_loss_total / valid_batches

            # Final safety clamp
            avg_q_loss = torch.clamp(avg_q_loss, 0.0, self.max_loss_threshold)
            avg_ent_loss = torch.clamp(avg_ent_loss, 0.0, 10.0)

            logger.debug(
                f"[LOSSES] Critic={avg_q_loss.item():.6f}, "
                f"Ent={avg_ent_loss.item():.6f} "
                f"({valid_batches} batches)"
            )

            return avg_q_loss, avg_ent_loss
        else:
            logger.warning("[LOSSES] No valid batches processed!")
            return (
                torch.tensor(0.0, device=self.device),
                torch.tensor(0.0, device=self.device)
            )

    # ------------------------------------------------------------------
    def train_step(self, batch_size=None):
        """Training step with robust batch processing"""
        try:
            # Use provided batch_size or default
            effective_batch_size = batch_size if batch_size is not None else self.batch_size

            # Check buffer size
            if len(self.buffer) < effective_batch_size:
                logger.debug(f"Buffer too small: {len(self.buffer)}/{effective_batch_size}")
                return None

            # Sample raw experiences - handle both buffer types
            try:
                # Check if buffer has sample method (proper replay buffer)
                if hasattr(self.buffer, 'sample') and callable(self.buffer.sample):
                    raw_experiences = self.buffer.sample(effective_batch_size)
                # Otherwise it's a deque - sample manually
                elif hasattr(self.buffer, '__iter__'):
                    import random
                    if len(self.buffer) < effective_batch_size:
                        logger.error(f"Not enough experiences in deque: {len(self.buffer)}/{effective_batch_size}")
                        return None
                    # Convert deque to list and sample
                    raw_experiences = random.sample(list(self.buffer), effective_batch_size)
                else:
                    logger.error(f"Buffer type {type(self.buffer)} not supported for sampling")
                    return None

                logger.debug(f"Sampled {len(raw_experiences)} experiences from {type(self.buffer).__name__}")
            except Exception as e:
                logger.error(f"Failed to sample from buffer: {e}")
                import traceback
                traceback.print_exc()
                print(f"\nâŒ [TRAIN ERROR] Buffer sampling failed: {e}")
                return None

            # Initialize batch processor if not exists
            if not hasattr(self, 'batch_processor'):
                # Use RewardBatchProcessor
                self.batch_processor = RewardBatchProcessor(
                    system=self.system,
                    state_dim=self.system.state_dim,
                    action_dim=2
                )

            # Process batch with extensive validation
            try:
                batch_data = self.batch_processor.process_batch_to_tensors(
                    experiences=raw_experiences,
                    agent_names=self.system.agent_names,
                    timeframes=list(TIMEFRAME_LENGTHS.keys()),
                    device=self.device
                )

                logger.debug(f"Batch processed successfully")

            except Exception as e:
                logger.error(f"Batch processing failed: {e}")
                import traceback
                traceback.print_exc()
                print(f"\nâŒ [TRAIN ERROR] Batch processing failed: {e}")
                return None

            # Extract structured data
            states = batch_data['states']
            actions = batch_data['actions']
            rewards = batch_data['rewards']
            next_states = batch_data['next_states']
            dones = batch_data['dones']

            # Now train on properly structured batch
            try:
                metrics = self._train_on_batch(states, actions, rewards, next_states, dones)
                return metrics

            except Exception as e:
                logger.error(f"Training on batch failed: {e}")
                import traceback
                traceback.print_exc()
                print(f"\nâŒ [TRAIN ERROR] Training on batch failed: {e}")
                return None

        except Exception as e:
            logger.error(f"[TRAIN ERROR] {e}")
            import traceback
            traceback.print_exc()
            print(f"\nâŒ [TRAIN ERROR] Outer exception: {e}")
            return None

    def _train_on_batch(self, states, actions, rewards, next_states, dones):
        """
        FULLY FIXED VERSION - Proper batch structure + gradient clipping + loss stability

        CRITICAL FIXES:
        1. Loop through batch indices first, then agents
        2. Gradient clipping AFTER backward, BEFORE optimizer step
        3. Safety checks for exploding losses
        """
        try:
            # Track training time for metrics
            import time
            train_start = time.time()

            self.system.train()
            total_loss = 0.0
            actor_losses, critic_losses, entanglement_losses = [], [], []
            gamma = getattr(self, "gamma", 0.99)

            # Get batch size
            if hasattr(rewards, 'shape'):
                batch_size = rewards.shape[0]
            elif isinstance(rewards, torch.Tensor):
                batch_size = rewards.size(0)
            else:
                batch_size = len(rewards) if hasattr(rewards, '__len__') else 1

            q_tensors = []
            q_next_tensors = []

            logger.info(f"[TRAIN] Building batch for {batch_size} samples with {len(self.system.agent_names)} agents")

            # âœ… CRITICAL FIX: Loop through BATCH INDICES FIRST
            for batch_idx in range(batch_size):
                batch_sample_q = {}
                batch_sample_q_next = {}

                # Now loop through agents for THIS batch sample
                for agent_name in self.system.agent_names:
                    agent = self.system.agents[agent_name]

                    timeframe_q_values = {}
                    timeframe_q_next = {}

                    # Process each timeframe
                    for tf in ["xs", "s", "m", "l", "xl", "xxl" , "5m" , "10m"]:
                        if agent_name not in states:
                            logger.warning(f"[Batch {batch_idx}] Agent {agent_name} not in states - skipping")
                            continue

                        # Get state with fallback
                        if tf not in states[agent_name]:
                            if states[agent_name]:
                                tf_fallback = next(iter(states[agent_name].keys()))
                                logger.debug(f"[Batch {batch_idx}] Using {tf_fallback} as fallback for {agent_name}/{tf}")
                                tf_tensor_full = states[agent_name][tf_fallback]
                            else:
                                continue
                        else:
                            tf_tensor_full = states[agent_name][tf]

                        # Convert to tensor
                        if not isinstance(tf_tensor_full, torch.Tensor):
                            tf_tensor_full = torch.tensor(tf_tensor_full, dtype=torch.float32)
                        tf_tensor_full = tf_tensor_full.to(self.system.device)

                        # Extract single batch sample
                        if tf_tensor_full.dim() > 1 and tf_tensor_full.shape[0] > 1:
                            tf_tensor = tf_tensor_full[batch_idx:batch_idx+1]
                        else:
                            tf_tensor = tf_tensor_full.unsqueeze(0) if tf_tensor_full.dim() == 1 else tf_tensor_full

                        # Compute Q-values
                        try:
                            q_values, critic = call_agent_policy(
                                agent,
                                tf_tensor,
                                timeframe_states={tf: tf_tensor}
                            )
                            timeframe_q_values[tf] = q_values
                        except Exception as e:
                            logger.warning(f"[TRAIN] Q-values failed for {agent_name}/{tf}: {e}")
                            continue

                        # Compute next Q-values
                        if agent_name in next_states and tf in next_states[agent_name]:
                            tf_next_full = next_states[agent_name][tf]

                            if not isinstance(tf_next_full, torch.Tensor):
                                tf_next_full = torch.tensor(tf_next_full, dtype=torch.float32)
                            tf_next_full = tf_next_full.to(self.system.device)

                            if tf_next_full.dim() > 1 and tf_next_full.shape[0] > 1:
                                tf_next = tf_next_full[batch_idx:batch_idx+1]
                            else:
                                tf_next = tf_next_full.unsqueeze(0) if tf_next_full.dim() == 1 else tf_next_full

                            with torch.no_grad():
                                try:
                                    if hasattr(agent, "actor_target"):
                                        q_next = agent.actor_target(tf_next)
                                    elif hasattr(agent, "get_q_values"):
                                        q_next, _ = call_agent_policy(agent, tf_next, timeframe_states={tf: tf_next})
                                    else:
                                        q_next = q_values.detach().clone()

                                    timeframe_q_next[tf] = q_next
                                except:
                                    timeframe_q_next[tf] = q_values.detach().clone()

                    # Add this agent to the batch sample
                    if timeframe_q_values:
                        batch_sample_q[agent_name] = timeframe_q_values
                        batch_sample_q_next[agent_name] = timeframe_q_next

                # Append complete batch sample with ALL agents
                if len(batch_sample_q) > 0:
                    q_tensors.append(batch_sample_q)
                    q_next_tensors.append(batch_sample_q_next)

            # Validation
            if not q_tensors:
                logger.error("[TRAIN] No batch samples created!")
                return None

            logger.info(f"[TRAIN] Created {len(q_tensors)} batch samples")

            # ====================================================================
            # COMPUTE LOSSES
            # ====================================================================
            try:
                # CRITIC LOSS: TD error (with clipping and normalization)
                q_loss, ent_loss = self._compute_losses(q_tensors, q_next_tensors, actions, rewards, dones)

                # ACTOR LOSS: Policy gradient for discrete actions
                actor_loss = torch.tensor(0.0, device=self.device)
                num_actor_samples = 0

                for batch_idx, q_dict in enumerate(q_tensors):
                    # Get action for this batch
                    if isinstance(actions, torch.Tensor):
                        if actions.dim() > 0 and batch_idx < len(actions):
                            action_sample = actions[batch_idx]
                            if action_sample.numel() == 2:
                                action_idx = int(torch.argmax(action_sample).item())
                            elif action_sample.numel() == 1:
                                action_idx = int(action_sample.item())
                            else:
                                action_idx = 0
                        else:
                            action_idx = 0
                    else:
                        action_idx = 0

                    # Accumulate Q-values for taken action
                    batch_q_sum = 0.0
                    batch_q_count = 0

                    for agent_name, timeframes in q_dict.items():
                        for tf, q_vals in timeframes.items():
                            if isinstance(q_vals, torch.Tensor) and q_vals.numel() > 0:
                                if q_vals.dim() > 1 and q_vals.shape[1] >= 2:
                                    selected_q = q_vals[0, action_idx]
                                elif q_vals.dim() == 1 and len(q_vals) >= 2:
                                    selected_q = q_vals[action_idx]
                                else:
                                    selected_q = q_vals.mean()

                                batch_q_sum += selected_q
                                batch_q_count += 1

                    if batch_q_count > 0:
                        actor_loss += -(batch_q_sum / batch_q_count)
                        num_actor_samples += 1

                # Average actor loss
                if num_actor_samples > 0:
                    actor_loss = actor_loss / num_actor_samples
                else:
                    actor_loss = torch.tensor(0.1, device=self.device)
                    logger.warning("[TRAIN] No actor samples processed!")

                # ================================================================
                # V5.0.6: ENTROPY BONUS FOR ACTOR LOSS
                # ================================================================
                # Add entropy regularization to encourage exploration
                # actor_loss = -Q(s,a) - Î² * H(Ï€)
                # where H(Ï€) = -Î£ p(a) log p(a) is policy entropy
                # ================================================================
                entropy_bonus = torch.tensor(0.0, device=self.device)
                entropy_bonus_value = 0.0
                
                try:
                    # Compute entropy bonus from Q-values
                    policy_entropies = []
                    
                    for batch_sample in q_tensors:
                        for agent_name, timeframes in batch_sample.items():
                            for tf, q_vals in timeframes.items():
                                if isinstance(q_vals, torch.Tensor) and q_vals.numel() >= 2:
                                    # Get Q-values for first 2 actions
                                    if q_vals.dim() > 1:
                                        q_vals = q_vals.flatten()[:2]
                                    else:
                                        q_vals = q_vals[:2]
                                    
                                    # Convert to action probabilities via softmax
                                    action_probs = F.softmax(q_vals, dim=-1)
                                    
                                    # Shannon entropy: H = -Î£ p log p
                                    # Higher entropy = more exploration
                                    entropy = -(action_probs * torch.log(action_probs + 1e-8)).sum()
                                    policy_entropies.append(entropy)
                    
                    if policy_entropies:
                        mean_entropy = torch.stack(policy_entropies).mean()
                        
                        # Entropy coefficient schedule
                        # Starts high (encourage exploration), decays over time
                        entropy_coef = self._get_entropy_coef()
                        
                        # Entropy bonus (negative because we want to MAXIMIZE entropy)
                        # actor_loss = -Q - Î²*H  â†’  lower loss when H is high
                        entropy_bonus = -entropy_coef * mean_entropy
                        entropy_bonus_value = -entropy_bonus.item()  # Positive for logging
                        
                        # Add to actor loss
                        actor_loss = actor_loss + entropy_bonus
                        
                except Exception as e:
                    logger.debug(f"[ENTROPY BONUS] Computation failed: {e}")
                    entropy_bonus_value = 0.0

                logger.debug(f"[LOSS] Actor: {actor_loss.item():.4f}, Critic: {q_loss.item():.4f}, Entropy Bonus: {entropy_bonus_value:.4f}")

            except Exception as e:
                logger.error(f"[TRAIN_ON_BATCH ERROR] Loss computation failed: {e}")
                import traceback
                traceback.print_exc()
                return None

            # ====================================================================
            # DIVERSITY LOSS COMPUTATION
            # ====================================================================
            diversity_loss_value = torch.tensor(0.0, device=self.device)
            diversity_metrics = {}
            
            try:
                if self.diversity_loss is not None and len(q_tensors) > 0:
                    # Collect actions and Q-values from batch
                    batch_actions_list = []
                    batch_q_values_list = []
                    
                    for batch_sample in q_tensors:
                        sample_actions = []
                        sample_q_values = []
                        
                        for agent_name in self.system.agent_names:
                            if agent_name in batch_sample:
                                # Get Q-values for 'm' timeframe or first available
                                q_vals = None
                                for tf in ['m', 'xs', 's', 'l', 'xl']:
                                    if tf in batch_sample[agent_name]:
                                        q_vals = batch_sample[agent_name][tf]
                                        break
                                
                                if q_vals is not None and isinstance(q_vals, torch.Tensor):
                                    action = torch.argmax(q_vals.flatten())
                                    sample_actions.append(action.item())
                                    # Ensure proper shape for Q-values
                                    if q_vals.dim() == 1:
                                        q_vals = q_vals.unsqueeze(0)
                                    sample_q_values.append(q_vals[:, :2] if q_vals.size(-1) >= 2 else q_vals)
                        
                        if len(sample_actions) == self.system.num_agents:
                            batch_actions_list.append(sample_actions)
                            batch_q_values_list.append(sample_q_values)
                    
                    # Compute diversity loss if we have valid samples
                    if batch_actions_list:
                        actions_tensor = torch.tensor(batch_actions_list, device=self.device)
                        
                        # Average Q-values across batch for diversity calculation
                        avg_q_per_agent = []
                        for agent_idx in range(self.system.num_agents):
                            agent_q_list = [sample[agent_idx] for sample in batch_q_values_list if agent_idx < len(sample)]
                            if agent_q_list:
                                avg_q = torch.stack(agent_q_list).mean(dim=0)
                                avg_q_per_agent.append(avg_q)
                        
                        if len(avg_q_per_agent) == self.system.num_agents:
                            diversity_loss_value, diversity_metrics = self.diversity_loss(
                                actions_tensor, avg_q_per_agent
                            )
                            
                            # Update tracker
                            self.diversity_tracker.update(actions_tensor, avg_q_per_agent, diversity_metrics)
                            
                            logger.debug(f"[DIVERSITY] Loss: {diversity_loss_value.item():.4f}, "
                                       f"Entropy: {diversity_metrics.get('entropy_loss', 0):.4f}, "
                                       f"Disagree: {diversity_metrics.get('disagreement_loss', 0):.4f}")
            
            except Exception as e:
                logger.debug(f"[DIVERSITY] Diversity loss computation skipped: {e}")
                diversity_loss_value = torch.tensor(0.0, device=self.device)

            # ====================================================================
            # TOTAL LOSS (with diversity preservation)
            # ====================================================================
            total_loss = actor_loss + q_loss + 0.01 * ent_loss + self.diversity_coef * diversity_loss_value

            # ====================================================================
            # BACKPROPAGATION WITH GRADIENT CLIPPING
            # ====================================================================
            actors_updated = 0
            critics_updated = 0

            # Step 1: Zero all gradients
            for agent_name in self.system.agent_names:
                agent = self.system.agents[agent_name]
                if hasattr(agent, "actor_optimizer"):
                    agent.actor_optimizer.zero_grad(set_to_none=True)
                if hasattr(agent, "critic_optimizer"):
                    agent.critic_optimizer.zero_grad(set_to_none=True)
                if hasattr(agent, "optimizer"):
                    agent.optimizer.zero_grad(set_to_none=True)

            # Step 2: Single backward pass
            try:
                total_loss.backward()
            except Exception as e:
                logger.error(f"[TRAIN] Backward pass failed: {e}")
                return None

            # ====================================================================
            # ğŸ”¥ CRITICAL FIX: CLIP GRADIENTS BEFORE OPTIMIZER STEP
            # ====================================================================
            max_grad_norm = 1.0

            for agent_name in self.system.agent_names:
                agent = self.system.agents[agent_name]

                # Clip actor gradients
                if hasattr(agent, "actor") and hasattr(agent.actor, "parameters"):
                    torch.nn.utils.clip_grad_norm_(agent.actor.parameters(), max_grad_norm)

                # Clip critic gradients
                if hasattr(agent, "critic") and hasattr(agent.critic, "parameters"):
                    torch.nn.utils.clip_grad_norm_(agent.critic.parameters(), max_grad_norm)

                # Clip model gradients
                if hasattr(agent, "model") and hasattr(agent.model, "parameters"):
                    torch.nn.utils.clip_grad_norm_(agent.model.parameters(), max_grad_norm)

                # Clip all parameters (fallback)
                if isinstance(agent, torch.nn.Module) and hasattr(agent, "parameters"):
                    torch.nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)

            # ====================================================================
            # Step 3: Update parameters with clipped gradients
            # ====================================================================
            for agent_name in self.system.agent_names:
                agent = self.system.agents[agent_name]

                # ========== ACTOR UPDATE ==========
                if hasattr(agent, "actor_optimizer") and hasattr(agent, "actor"):
                    try:
                        agent.actor_optimizer.step()
                        actor_losses.append(actor_loss.item())
                        actors_updated += 1
                        logger.debug(f"[TRAIN] Actor updated | Agent={agent_name}")
                    except Exception as e:
                        logger.error(f"[TRAIN] Actor update failed for {agent_name}: {e}")

                # ========== CRITIC/MODEL UPDATE ==========
                critic_updated_this_agent = False

                # Path 1: QuantumAgent style (model + optimizer)
                if hasattr(agent, "model") and hasattr(agent, "optimizer"):
                    if agent.model is not None and agent.optimizer is not None:
                        try:
                            agent.optimizer.step()
                            critic_losses.append(q_loss.item())
                            critics_updated += 1
                            critic_updated_this_agent = True
                            logger.debug(f"[TRAIN] Model (critic) updated | Agent={agent_name}")
                        except Exception as e:
                            logger.error(f"[TRAIN] Model update failed for {agent_name}: {e}")

                # Path 2: Traditional style (separate critic)
                elif hasattr(agent, "critic") and hasattr(agent, "critic_optimizer"):
                    if agent.critic is not None and agent.critic_optimizer is not None:
                        try:
                            agent.critic_optimizer.step()
                            critic_losses.append(q_loss.item())
                            critics_updated += 1
                            critic_updated_this_agent = True
                            logger.debug(f"[TRAIN] Critic updated | Agent={agent_name}")
                        except Exception as e:
                            logger.error(f"[TRAIN] Critic update failed for {agent_name}: {e}")

                # Path 3: nn.Module with generic optimizer
                elif isinstance(agent, torch.nn.Module) and hasattr(agent, "optimizer"):
                    if agent.optimizer is not None:
                        try:
                            agent.optimizer.step()
                            critic_losses.append(q_loss.item())
                            critics_updated += 1
                            critic_updated_this_agent = True
                            logger.debug(f"[TRAIN] nn.Module updated | Agent={agent_name}")
                        except Exception as e:
                            logger.error(f"[TRAIN] nn.Module update failed for {agent_name}: {e}")

                if not critic_updated_this_agent:
                    logger.warning(f"[TRAIN] Agent {agent_name} - No valid update path found!")

                # Always append entanglement loss
                entanglement_losses.append(ent_loss.item())

            # ====================================================================
            # FALLBACK: Ensure losses aren't empty
            # ====================================================================
            if not actor_losses:
                logger.critical("[TRAIN] âš ï¸  No actors updated! Using fallback.")
                actor_losses = [actor_loss.item()]

            if not critic_losses:
                logger.critical("[TRAIN] âš ï¸  No critics updated! Using fallback.")
                critic_losses = [q_loss.item()]

            # Log summary
            logger.critical(f"[TRAIN] Updates: {actors_updated} actors, {critics_updated} critics")

            # ====================================================================
            # METRICS (with diversity information)
            # V5.0.6: Added entropy bonus and exploration stats
            # ====================================================================
            metrics = {
                "total_loss": float(total_loss.item()),
                "actor_loss": float(np.mean(actor_losses)),
                "critic_loss": float(np.mean(critic_losses)),
                "entanglement_loss": float(np.mean(entanglement_losses)),
                "diversity_loss": float(diversity_loss_value.item()) if isinstance(diversity_loss_value, torch.Tensor) else 0.0,
                "diversity_loss_scaled": float(self.diversity_coef * diversity_loss_value.item()) if isinstance(diversity_loss_value, torch.Tensor) else 0.0,
                "training_time_ms": float((time.time() - train_start) * 1000),
                "samples_per_second": float(batch_size / max(0.001, time.time() - train_start)),
                "quantum_entanglement_active": True,
                "actors_updated": actors_updated,
                "critics_updated": critics_updated,
                # V5.0.6: Entropy bonus tracking
                "entropy_bonus": entropy_bonus_value,
                "entropy_coef": self._get_entropy_coef(),
            }
            
            # Add diversity breakdown metrics if available
            if diversity_metrics:
                metrics["entropy_loss"] = diversity_metrics.get('entropy_loss', 0.0)
                metrics["disagreement_loss"] = diversity_metrics.get('disagreement_loss', 0.0)
                metrics["spread_loss"] = diversity_metrics.get('spread_loss', 0.0)
            
            # V5.0.6: Add exploration manager stats if available
            exp_mgr = get_exploration_manager()
            if exp_mgr is not None:
                exp_stats = exp_mgr.get_stats()
                metrics["exploration"] = {
                    "temperature": exp_stats['temperature'],
                    "mean_epsilon": exp_stats['mean_epsilon'],
                    "random_action_rate": exp_stats['random_action_rate'],
                    "collapsed_agents_count": exp_stats['collapsed_agents'],
                }

            # ====================================================================
            # V8.6.2: COMPREHENSIVE PER-AGENT METRICS COLLECTION
            # ====================================================================
            agent_metrics = {}
            timeframe_metrics = {tf: {"q_mean": [], "q_std": [], "action_dist": [0, 0]} 
                               for tf in ["xs", "s", "m", "l", "xl", "xxl", "5m", "10m"]}
            
            for batch_idx, q_dict in enumerate(q_tensors):
                for agent_name, timeframes in q_dict.items():
                    if agent_name not in agent_metrics:
                        agent_metrics[agent_name] = {
                            "q_values_mean": [],
                            "q_values_std": [],
                            "q_values_max": [],
                            "q_values_min": [],
                            "action_taken": [],
                            "q_spread": [],
                            "timeframe_q": {tf: [] for tf in ["xs", "s", "m", "l", "xl", "xxl", "5m", "10m"]}
                        }
                    
                    for tf, q_vals in timeframes.items():
                        if isinstance(q_vals, torch.Tensor) and q_vals.numel() > 0:
                            q_np = q_vals.detach().cpu().numpy().flatten()
                            if len(q_np) >= 2:
                                agent_metrics[agent_name]["q_values_mean"].append(float(np.mean(q_np)))
                                agent_metrics[agent_name]["q_values_std"].append(float(np.std(q_np)))
                                agent_metrics[agent_name]["q_values_max"].append(float(np.max(q_np)))
                                agent_metrics[agent_name]["q_values_min"].append(float(np.min(q_np)))
                                agent_metrics[agent_name]["q_spread"].append(float(abs(q_np[0] - q_np[1])))
                                agent_metrics[agent_name]["action_taken"].append(int(np.argmax(q_np)))
                                
                                if tf in agent_metrics[agent_name]["timeframe_q"]:
                                    agent_metrics[agent_name]["timeframe_q"][tf].append(float(np.mean(q_np)))
                                
                                # Update timeframe aggregates
                                if tf in timeframe_metrics:
                                    timeframe_metrics[tf]["q_mean"].append(float(np.mean(q_np)))
                                    timeframe_metrics[tf]["q_std"].append(float(np.std(q_np)))
                                    action = int(np.argmax(q_np))
                                    timeframe_metrics[tf]["action_dist"][action] += 1

            # Compute aggregate per-agent metrics
            per_agent_summary = {}
            for agent_name, am in agent_metrics.items():
                if am["q_values_mean"]:
                    buy_count = sum(1 for a in am["action_taken"] if a == 0)
                    sell_count = sum(1 for a in am["action_taken"] if a == 1)
                    total_actions = buy_count + sell_count
                    
                    per_agent_summary[agent_name] = {
                        "q_mean": float(np.mean(am["q_values_mean"])),
                        "q_std": float(np.mean(am["q_values_std"])),
                        "q_max": float(np.max(am["q_values_max"])) if am["q_values_max"] else 0,
                        "q_min": float(np.min(am["q_values_min"])) if am["q_values_min"] else 0,
                        "q_spread_avg": float(np.mean(am["q_spread"])) if am["q_spread"] else 0,
                        "buy_ratio": buy_count / max(1, total_actions),
                        "sell_ratio": sell_count / max(1, total_actions),
                        "samples": total_actions
                    }
            
            metrics["per_agent"] = per_agent_summary
            
            # ====================================================================
            # V5.0.6: UPDATE EXPLORATION MANAGER WITH AGENT ENTROPIES
            # ====================================================================
            # This allows per-agent epsilon to be adjusted based on actual
            # policy entropy. Collapsed agents will receive boosted exploration.
            # ====================================================================
            exp_mgr = get_exploration_manager()
            if exp_mgr is not None and per_agent_summary:
                for agent_name, agent_data in per_agent_summary.items():
                    buy_ratio = agent_data.get('buy_ratio', 0.5)
                    # Clamp to avoid log(0)
                    buy_ratio = max(1e-8, min(1 - 1e-8, buy_ratio))
                    sell_ratio = 1.0 - buy_ratio
                    # Shannon entropy in bits
                    agent_entropy = -(buy_ratio * np.log2(buy_ratio) + sell_ratio * np.log2(sell_ratio))
                    # Update exploration manager with this agent's entropy
                    exp_mgr.agent_entropies[agent_name] = agent_entropy
            
            # Compute per-timeframe aggregates
            per_timeframe_summary = {}
            for tf, tm in timeframe_metrics.items():
                if tm["q_mean"]:
                    total_tf_actions = sum(tm["action_dist"])
                    per_timeframe_summary[tf] = {
                        "q_mean": float(np.mean(tm["q_mean"])),
                        "q_std": float(np.mean(tm["q_std"])),
                        "buy_ratio": tm["action_dist"][0] / max(1, total_tf_actions),
                        "sell_ratio": tm["action_dist"][1] / max(1, total_tf_actions),
                        "samples": total_tf_actions
                    }
            
            metrics["per_timeframe"] = per_timeframe_summary

            # ====================================================================
            # V8.6.2: MULTI-AGENT BUFFER STATISTICS
            # ====================================================================
            if hasattr(self, 'buffer') and self.buffer is not None:
                if hasattr(self.buffer, 'get_stats'):
                    buffer_stats = self.buffer.get_stats()
                    metrics["buffer_size"] = buffer_stats.get('size', len(self.buffer))
                    metrics["buffer_utilization"] = buffer_stats.get('utilization', 0)
                    if 'per_agent_samples' in buffer_stats:
                        metrics["buffer_per_agent"] = buffer_stats['per_agent_samples']
                else:
                    metrics["buffer_size"] = len(self.buffer)

            # ====================================================================
            # V8.6.2: PRINT COMPREHENSIVE TRAINING METRICS
            # ====================================================================
            self._print_training_metrics(metrics, batch_size)

            logger.critical(
                f"[TRAIN] âœ… Complete | "
                f"Total={metrics['total_loss']:.6f} | "
                f"Actor={metrics['actor_loss']:.6f} | "
                f"Critic={metrics['critic_loss']:.6f} | "
                f"Ent={metrics['entanglement_loss']:.6f} | "
                f"Div={metrics['diversity_loss']:.6f}"
            )

            return metrics

        except Exception as e:
            logger.error(f"[TRAIN_ON_BATCH ERROR] {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            return None

    def train_step_flexible(self, min_batch_size=8):
        """
        Flexible training that adapts to available experience buffer size.
        Trains progressively even with partial data.
        """
        current_size = len(self.buffer)
        if current_size < min_batch_size:
            logger.info(f"[TRAIN] Not enough data for training: {current_size}/{min_batch_size}")
            return None

        # Dynamic batch scaling
        if current_size < 16:
            effective_batch = min_batch_size
        elif current_size < 32:
            effective_batch = 16
        elif current_size < 64:
            effective_batch = 32
        else:
            effective_batch = self.batch_size

        logger.info(f"[TRAIN] Progressive training â€” using {effective_batch}/{current_size} experiences")
        return self.train_step(batch_size_override=effective_batch)

class QuantumReplayBuffer:
    """Quantum-compatible replay buffer for temporal sequence storage."""
    def __init__(self, max_size: int = 5000000):
        self.max_size = max_size
        self.buffer = []
        self.mem_cntr = 0

    def __len__(self):
        return len(self.buffer)

    def add_experience(self, state_seq, action, reward, next_state_seq, done, signal_key=None):
        """Store experience with validation"""
        try:
            # Validate inputs
            if not isinstance(state_seq, np.ndarray):
                state_seq = np.array(state_seq, dtype=np.float32)

            if state_seq.ndim == 1:
                # If 1D, it's already flattened - good
                pass
            elif state_seq.ndim == 2:
                # If 2D, flatten
                state_seq = state_seq.flatten()

            # Store
            if len(self.buffer) >= self.max_size:
                self.buffer.pop(0)

            self.buffer.append({
                'state_seq': state_seq,
                'action': action,
                'reward': reward,
                'next_state_seq': next_state_seq,
                'done': done,
                'signal_key': signal_key
            })
            self.mem_cntr += 1

        except Exception as e:
            logger.error(f"Experience storage failed: {e}")
            raise

    def sample(self, batch_size: int = 32):
        if len(self.buffer) < batch_size:
            return None
        return random.sample(self.buffer, batch_size)

# REMOVED DUPLICATE: import os
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import torch
# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: from collections import deque

logger = logging.getLogger(__name__)

# Replace this with your actual mapping
ACTION_MAP = {0: "BUY", 1: "SELL"}


class QuantumAgent(nn.Module):



    def __init__(self, name: str = None, seq_len: int = 32, state_dim: int = DEFAULT_STATE_DIM,
                 action_dim: int = DEFAULT_ACTION_DIM, hidden_dim: int = DEFAULT_HIDDEN_DIM,
                 n_qubits: int = QUANTUM_N_QUBITS, device=None, learning_rate: float = LEARNING_RATE,
                 base_path: Optional[str] = None, gcs_bucket: Optional[str] = None,
                 gcs_path: Optional[str] = None):
        super().__init__()

        self.name = name or f"quantum_agent_{id(self)}"
        self.seq_len = seq_len
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.n_qubits = n_qubits
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.base_path = base_path
        self.gcs_bucket = gcs_bucket
        self.gcs_path = gcs_path

        # Feature/state caches (for backward compatibility)
        self.last_valid_feature = None
        self.latest_features = None
        self.feature_history = deque(maxlen=self.seq_len)
        self.recent_q_values = deque(maxlen=25)
        self.state_cache = {}

        # ============================================================
        # CRITICAL: Actor-Critic Networks (V6 FIX)
        # ============================================================
        self.actor = Actor(state_dim, action_dim, hidden_dim).to(self.device)
        self.critic = Critic(state_dim, action_dim, hidden_dim).to(self.device)

        logger.info(f"âœ… {self.name}: Actor and Critic networks created")

        # Q-value network (for DQN compatibility)
        self.model = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, action_dim)
        ).to(self.device)

        # Target networks
        self.target_model = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, action_dim)
        ).to(self.device)
        self.target_model.load_state_dict(self.model.state_dict())

        # ============================================================
        # CRITICAL: Separate Optimizers (V6 FIX)
        # ============================================================
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

        logger.info(f"âœ… {self.name}: All optimizers initialized")

        # Training state
        self.loss_fn = nn.MSELoss()
        self.train_step_counter = 0
        self.epsilon = EPSILON_START

        logger.info(f"âœ… QuantumAgent '{self.name}' fully initialized with Actor-Critic")
        
        # V8.6.2: Track action bias for rebalancing
        self.action_history = deque(maxlen=100)
        self.rebalance_counter = 0

    def check_and_rebalance_q_values(self, force: bool = False) -> bool:
        """
        V8.6.2: Check for Q-value bias and rebalance if needed.
        
        This prevents all agents from converging to the same action by:
        1. Tracking recent action history
        2. Detecting bias (>75% same action)
        3. Adding noise to the biased Q-value to restore balance
        
        Returns True if rebalancing was applied.
        """
        if len(self.action_history) < 20 and not force:
            return False
        
        # Calculate action distribution
        buy_count = sum(1 for a in self.action_history if a == 0)
        sell_count = sum(1 for a in self.action_history if a == 1)
        total = len(self.action_history)
        
        if total == 0:
            return False
        
        buy_ratio = buy_count / total
        sell_ratio = sell_count / total
        
        # Check for bias (>75% one action)
        bias_threshold = 0.75
        
        if buy_ratio > bias_threshold or sell_ratio > bias_threshold:
            self.rebalance_counter += 1
            logger.warning(f"[{self.name}] Q-VALUE BIAS DETECTED: BUY={buy_ratio:.1%}, SELL={sell_ratio:.1%}")
            
            # Apply small random perturbation to model weights
            with torch.no_grad():
                for param in self.model.parameters():
                    if param.requires_grad:
                        noise = torch.randn_like(param) * 0.01  # Small noise
                        param.add_(noise)
            
            # Increase exploration temporarily
            self.epsilon = min(self.epsilon * 1.5, 0.5)
            logger.info(f"[{self.name}] Rebalanced weights, epsilon now {self.epsilon:.3f}")
            
            # Clear action history to start fresh
            self.action_history.clear()
            return True
        
        return False

    def forward(self, x: torch.Tensor) -> torch.Tensor:

        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32, device=self.device)
        if x.dim() == 1:
            x = x.unsqueeze(0)
        return self.model(x)

    # ============================================================
    # Feature handling (backward compatibility)
    # ============================================================
    def update_features(self, features_input):

        feature_array = None
        if features_input is None:
            feature_array = self.last_valid_feature if self.last_valid_feature is not None else np.zeros(self.state_dim, dtype=np.float32)
        elif isinstance(features_input, np.ndarray):
            feature_array = features_input.astype(np.float32).flatten()
        elif isinstance(features_input, dict):
            feature_values = []
            for v in features_input.values():
                try:
                    feature_values.append(float(v))
                except:
                    feature_values.append(0.0)
            feature_array = np.array(feature_values, dtype=np.float32)
        elif isinstance(features_input, (list, tuple)):
            feature_array = np.array([float(v) if not callable(v) else 0.0 for v in features_input], dtype=np.float32)
        else:
            feature_array = np.zeros(self.state_dim, dtype=np.float32)

        # Pad/truncate to state_dim
        if len(feature_array) < self.state_dim:
            feature_array = np.pad(feature_array, (0, self.state_dim - len(feature_array)))
        elif len(feature_array) > self.state_dim:
            feature_array = feature_array[:self.state_dim]

        self.latest_features = feature_array
        self.feature_history.append(feature_array)
        self.last_valid_feature = feature_array
        self.state_cache[self.name] = feature_array.copy()
        return feature_array

    def get_current_state_sequence(self):

        if len(self.feature_history) >= self.seq_len:
            return np.array(list(self.feature_history)[-self.seq_len:], dtype=np.float32)
        else:
            padding_needed = self.seq_len - len(self.feature_history)
            pad_val = self.last_valid_feature if self.last_valid_feature is not None else np.zeros(self.state_dim, dtype=np.float32)
            padded_seq = [pad_val] * padding_needed + list(self.feature_history)
            return np.array(padded_seq, dtype=np.float32)

    # ============================================================
    # Prediction & action selection
    # ============================================================
    def predict(self, add_noise: bool = False) -> np.ndarray:
        """
        Predict Q-values for the latest state, with optional noise,
        and apply quantum advisor adjustments if available.
        """
        state_seq = self.get_current_state_sequence()[-1]
        state_tensor = torch.tensor(state_seq, dtype=torch.float32, device=self.device).unsqueeze(0)

        with torch.no_grad():
            q_values = self.forward(state_tensor).cpu().numpy().flatten()

        if add_noise:
            q_values += np.random.normal(0, 0.01, size=q_values.shape)

        # ============================================================
        # ğŸ”§ PATCH #3: Quantum Advisor Logging & Adjustment (CRITICAL)
        # ============================================================
        if hasattr(self, 'quantum_advisor') and self.quantum_advisor is not None:
            try:
                # Prepare state tensor for quantum advisor
                state = self.get_current_state_sequence()
                if isinstance(state, np.ndarray):
                    state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device)
                    if state_tensor.dim() == 2:
                        state_tensor = state_tensor.unsqueeze(0)
                else:
                    state_tensor = state

                # Run quantum advisor
                with torch.no_grad():
                    forecast, confidence = self.quantum_advisor(state_tensor)
                    forecast = float(forecast.cpu().numpy())
                    confidence = float(confidence.cpu().numpy())

                # ====================================================
                # CRITICAL LOGGING
                # ====================================================
                logger.critical("ğŸ”® [QuantumAdvisor] %s", self.name)
                logger.critical("    Original Q-values: BUY=%.6f | SELL=%.6f", q_values[0], q_values[1])
                logger.critical("    Forecast: %+0.6f | Confidence: %.6f", forecast, confidence)

                # Apply adjustment
                adjustment = forecast * confidence
                q_values = q_values + adjustment

                logger.critical("    Adjustment Applied: %+0.6f", adjustment)
                logger.critical("    Adjusted Q-values: BUY=%.6f | SELL=%.6f", q_values[0], q_values[1])
                logger.critical("-------------------------------------------------------------")

            except Exception as e:
                logger.critical(f"[{self.name}] âŒ Quantum Advisor failed: {e}")
                import traceback
                traceback.print_exc()

        # Store recent Q-values
        self.recent_q_values.append(q_values)
        return q_values


    def get_discrete_action(self, q_values: Optional[np.ndarray] = None) -> int:

        if q_values is None:
            q_values = self.predict()
        return int(np.argmax(q_values[:self.action_dim]))


    # AFTER
    def get_action(self, state: torch.Tensor, epsilon: Optional[float] = None) -> int:

        if epsilon is None:
            epsilon = self.epsilon

        # ğŸ”§ NEW: Force minimum exploration even when epsilon is low
        effective_epsilon = max(epsilon, 0.4)  # Never drop below 15% random

        if random.random() < effective_epsilon:
            return random.randint(0, self.action_dim - 1)

        with torch.no_grad():
            if state.dim() == 1:
                state = state.unsqueeze(0)
            q_values = self.forward(state)
            return torch.argmax(q_values, dim=-1).item()

    def select_action(self, features_input=None, add_noise: bool = False):

        if features_input is not None:
            self.update_features(features_input)
        q_values = self.predict(add_noise=add_noise)
        action = self.get_discrete_action(q_values)
        
        # V8.6.2: Track action for bias detection
        if hasattr(self, 'action_history'):
            self.action_history.append(action)
            
            # Check for rebalancing every 50 actions
            if len(self.action_history) >= 50 and len(self.action_history) % 50 == 0:
                self.check_and_rebalance_q_values()
        
        return action, q_values

    # ============================================================
    # Training methods
    # ============================================================
    def train_step(self, state_seq, action, reward, next_state_seq, done):

        try:
            self.model.train()
            state_tensor = torch.tensor(state_seq, dtype=torch.float32, device=self.device).unsqueeze(0)
            next_state_tensor = torch.tensor(next_state_seq, dtype=torch.float32, device=self.device).unsqueeze(0)
            action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)
            reward_tensor = torch.tensor([reward], dtype=torch.float32, device=self.device)

            # Compute target Q-value
            with torch.no_grad():
                next_q = self.target_model(next_state_tensor).max(1)[0]
                target_q = reward_tensor + (GAMMA * next_q * (1 - done))

            # Current Q-value
            current_q = self.model(state_tensor).gather(1, action_tensor.unsqueeze(1)).squeeze()

            # Compute loss and update
            loss = self.loss_fn(current_q, target_q)

            self.optimizer.zero_grad()
            loss.backward()
            clip_grad_norm_(self.model.parameters(), GRADIENT_CLIP)
            self.optimizer.step()

            self.train_step_counter += 1

            return loss.item()

        except Exception as e:
            logger.error(f"{self.name} training error: {e}")
            return 0.0

    def update_epsilon(self, decay: float = EPSILON_DECAY):

        self.epsilon = max(EPSILON_END, self.epsilon * decay)

    def update_target(self, tau: float = TAU):

        for target_param, param in zip(self.target_model.parameters(),
                                       self.model.parameters()):
            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)


# ==============================================================================
# QUANTUM VOTING SYSTEM (Drop-In Replacement for VotingTransformer)
# ==============================================================================

# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import logging
from collections import Counter, deque
from typing import Dict, Tuple, Optional

# REMOVED DUPLICATE: import numpy as np
# REMOVED DUPLICATE: import torch

class QuantumVotingSystem:
    """
    Robust QuantumVotingSystem with safe array handling and enhanced logging.
    Prevents boolean ambiguity errors and ensures fallback signals.
    V8.6.3 DIAGNOSTIC: Added fallback tracking and verbose error logging.
    """

    def __init__(self, quantum_bridge, device: str = None):
        self.quantum_bridge = quantum_bridge
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # State storage
        self.last_valid_vote = 0  # Default BUY
        self.last_confidence = torch.tensor([0.5, 0.5], device=self.device)
        self.last_valid_feature = np.zeros(1, dtype=np.float32)

        # V8.6.3 DIAGNOSTIC: Track fallback statistics
        self.fallback_stats = {
            'empty_q_values_dict': 0,
            'empty_actions_dict': 0,
            'no_quantum_bridge': 0,
            'agent_prediction_failed': 0,
            'no_quantum_actions': 0,
            'majority_vote_empty': 0,
            'majority_vote_invalid': 0,
            'majority_vote_exception': 0,
            'total_predictions': 0,
            'successful_predictions': 0,
        }

        # Queue for training
        self.voting_sample_queue = deque(maxlen=20000)
        self.voting_collection_lock = threading.Lock()

        logger.info("QuantumVotingSystem initialized (V8.6.3 DIAGNOSTIC MODE)")

    # ======================================================================
    # PREDICTION
    # ======================================================================
    def predict(self, q_values_dict: Dict[str, np.ndarray],
                actions_dict: Dict[str, int]) -> Tuple[int, Optional[torch.Tensor]]:
        """
        V8.6.3 DIAGNOSTIC: Predict with VERBOSE error logging.
        No silent fallbacks - all failures are logged as CRITICAL.
        """
        self.fallback_stats['total_predictions'] += 1
        
        try:
            # ========== VALIDATION WITH CRITICAL LOGGING ==========
            if not isinstance(q_values_dict, dict) or len(q_values_dict) == 0:
                self.fallback_stats['empty_q_values_dict'] += 1
                logger.critical(
                    f"âŒ FALLBACK TRIGGERED: empty_q_values_dict\n"
                    f"   Type: {type(q_values_dict)}\n"
                    f"   Value: {q_values_dict}\n"
                    f"   Fallback count: {self.fallback_stats['empty_q_values_dict']}/{self.fallback_stats['total_predictions']}\n"
                    f"   Stack trace:", exc_info=True
                )
                raise ValueError(f"q_values_dict is empty or invalid: {type(q_values_dict)}")

            if not isinstance(actions_dict, dict) or len(actions_dict) == 0:
                self.fallback_stats['empty_actions_dict'] += 1
                logger.critical(
                    f"âŒ FALLBACK TRIGGERED: empty_actions_dict\n"
                    f"   Type: {type(actions_dict)}\n"
                    f"   Value: {actions_dict}\n"
                    f"   q_values_dict keys: {list(q_values_dict.keys()) if isinstance(q_values_dict, dict) else 'N/A'}\n"
                    f"   Fallback count: {self.fallback_stats['empty_actions_dict']}/{self.fallback_stats['total_predictions']}"
                )
                raise ValueError(f"actions_dict is empty or invalid: {type(actions_dict)}")

            if not self.quantum_bridge:
                self.fallback_stats['no_quantum_bridge'] += 1
                logger.critical(
                    f"âŒ FALLBACK TRIGGERED: no_quantum_bridge\n"
                    f"   quantum_bridge is: {self.quantum_bridge}\n"
                    f"   Fallback count: {self.fallback_stats['no_quantum_bridge']}/{self.fallback_stats['total_predictions']}"
                )
                raise ValueError("quantum_bridge is None or not set")

            # ========== PROCESS EACH AGENT ==========
            quantum_actions = {}
            agent_errors = []
            
            # V5.0.6: Get exploration manager for entropy-aware exploration
            exploration_mgr = get_exploration_manager()
            
            for agent_name, q_vals in q_values_dict.items():
                try:
                    q_vals_safe = q_vals

                    if hasattr(q_vals_safe, "numpy"):
                        q_vals_safe = q_vals_safe.numpy()
                    elif hasattr(q_vals_safe, "detach"):
                        import torch
                        q_vals_safe = q_vals_safe.detach().cpu().numpy()

                    q_vals_safe = np.array(q_vals_safe, dtype=np.float32).flatten()

                    if q_vals_safe.size < 2:
                        logger.warning(
                            f"âš ï¸ [{agent_name}] q_vals too short (size={q_vals_safe.size}), "
                            f"original: {q_vals}, using default [0.5, 0.5]"
                        )
                        q_vals_safe = np.array([0.5, 0.5], dtype=np.float32)

                    # V8.6.3: Log Q-values for EVERY agent
                    q_buy, q_sell = q_vals_safe[0], q_vals_safe[1]
                    q_diff = abs(q_buy - q_sell)
                    
                    # ==============================================================
                    # V5.0.6: ENTROPY-AWARE ACTION SELECTION
                    # ==============================================================
                    # Use ExplorationManager if available for:
                    # - Temperature scaling (softmax instead of argmax)
                    # - Per-agent epsilon based on entropy
                    # - Collapsed agents get boosted exploration
                    # ==============================================================
                    
                    if exploration_mgr is not None:
                        # Compute policy entropy from Q-values
                        q_tensor = torch.tensor(q_vals_safe[:2], dtype=torch.float32)
                        action_probs = F.softmax(q_tensor, dim=-1)
                        agent_entropy = -(action_probs * torch.log(action_probs + 1e-8)).sum().item()
                        
                        # Use exploration manager for action selection
                        action, metadata = exploration_mgr.select_action_with_exploration(
                            q_values=q_tensor,
                            agent_name=agent_name,
                            agent_entropy=agent_entropy,
                            training=True
                        )
                        
                        quantum_actions[agent_name] = action
                        
                        if metadata.get('was_random', False):
                            action_source = f"EXPLORATION (Îµ={metadata.get('epsilon', 0):.2f})"
                        else:
                            action_source = f"TEMP-SOFTMAX (Ï„={metadata.get('temperature', 1):.2f})"
                        
                        # Log entropy status
                        if agent_entropy < 0.15:
                            action_source += " ğŸ”´COLLAPSED"
                        elif agent_entropy < 0.5:
                            action_source += " ğŸŸ¡LOW-H"
                    
                    else:
                        # Fallback to original logic
                        exploration_rate = getattr(self, 'voting_exploration_rate', 0.15)
                        if random.random() < exploration_rate:
                            quantum_actions[agent_name] = random.randint(0, 1)
                            action_source = "EXPLORATION"
                        elif q_diff < 0.05:
                            quantum_actions[agent_name] = random.randint(0, 1)
                            action_source = f"LOW_SPREAD (diff={q_diff:.4f})"
                        else:
                            quantum_actions[agent_name] = int(np.argmax(q_vals_safe[:2]))
                            action_source = "ARGMAX"

                    # V8.6.3: Always log agent decisions
                    logger.info(
                        f"ğŸ“Š [{agent_name}] Q=[{q_buy:.4f}, {q_sell:.4f}] diff={q_diff:.4f} "
                        f"â†’ {['BUY','SELL'][quantum_actions[agent_name]]} ({action_source})"
                    )

                except Exception as e:
                    self.fallback_stats['agent_prediction_failed'] += 1
                    agent_errors.append((agent_name, str(e)))
                    logger.critical(
                        f"âŒ [{agent_name}] AGENT PREDICTION FAILED:\n"
                        f"   Error: {e}\n"
                        f"   q_vals type: {type(q_vals)}\n"
                        f"   q_vals value: {q_vals}\n"
                        f"   Total agent failures: {self.fallback_stats['agent_prediction_failed']}",
                        exc_info=True
                    )
                    # Don't silently default - skip this agent
                    continue

            if not quantum_actions:
                self.fallback_stats['no_quantum_actions'] += 1
                logger.critical(
                    f"âŒ FALLBACK TRIGGERED: no_quantum_actions\n"
                    f"   All agents failed: {agent_errors}\n"
                    f"   Input q_values_dict keys: {list(q_values_dict.keys())}\n"
                    f"   Input actions_dict keys: {list(actions_dict.keys())}\n"
                    f"   Fallback count: {self.fallback_stats['no_quantum_actions']}/{self.fallback_stats['total_predictions']}"
                )
                raise ValueError(f"No quantum actions could be extracted. Agent errors: {agent_errors}")

            # ========== VOTING ==========
            action_votes = Counter(quantum_actions.values())
            buy_count = action_votes.get(0, 0)
            sell_count = action_votes.get(1, 0)
            total_agents = len(quantum_actions)
            
            # V8.6.3: Log vote distribution
            logger.info(
                f"ğŸ—³ï¸ VOTE DISTRIBUTION: BUY={buy_count} ({100*buy_count/total_agents:.1f}%), "
                f"SELL={sell_count} ({100*sell_count/total_agents:.1f}%) "
                f"from {total_agents} agents"
            )
            
            # Check for homogenization
            buy_ratio = buy_count / total_agents if total_agents > 0 else 0.5
            sell_ratio = sell_count / total_agents if total_agents > 0 else 0.5
            
            if buy_ratio >= 0.875 or sell_ratio >= 0.875:
                logger.warning(
                    f"âš ï¸ HOMOGENIZATION DETECTED: {100*buy_ratio:.1f}% BUY, {100*sell_ratio:.1f}% SELL\n"
                    f"   Agent votes: {dict(quantum_actions)}\n"
                    f"   Q-values: {dict(q_values_dict)}"
                )

            most_common_action, vote_count = action_votes.most_common(1)[0]
            voting_confidence = vote_count / total_agents if total_agents else 0.0

            # Metadata confidence
            metadata = getattr(self.quantum_bridge, "last_metadata", {})
            entanglement = float(metadata.get("entanglement", {}).get("mean", 0.0))
            coordination = float(metadata.get("avg_coordination", 0.0))
            quantum_confidence = 0.4*voting_confidence + 0.3*min(entanglement,1.0) + 0.3*min(coordination,1.0)
            confidence_tensor = torch.tensor([quantum_confidence, 1-quantum_confidence], device=self.device)

            self.last_valid_vote = most_common_action
            self.last_confidence = confidence_tensor
            self.fallback_stats['successful_predictions'] += 1

            logger.info(
                f"âœ… VOTING RESULT: {['BUY','SELL'][most_common_action]} "
                f"(confidence={quantum_confidence:.3f}, votes={dict(action_votes)})"
            )

            return most_common_action, confidence_tensor

        except Exception as e:
            # V8.6.3: Log the exception but DON'T silently fall back
            logger.critical(
                f"âŒâŒâŒ QUANTUM VOTING FAILED âŒâŒâŒ\n"
                f"   Error: {e}\n"
                f"   Fallback stats: {self.fallback_stats}\n"
                f"   This should NOT happen - investigate the root cause!",
                exc_info=True
            )
            # Re-raise to make the failure visible
            raise

    # ======================================================================
    # Fallback Logic - V8.6.3 DIAGNOSTIC: Raises exceptions instead of silent defaults
    # ======================================================================
    def _majority_vote_fallback(self, actions_dict: Dict[str, int]) -> Tuple[int, Optional[torch.Tensor]]:
        """
        V8.6.3 DIAGNOSTIC: Fallback that logs CRITICAL and raises exceptions.
        We want to SEE why fallbacks are being triggered, not hide them.
        """
        logger.critical(
            f"âŒâŒâŒ _majority_vote_fallback CALLED âŒâŒâŒ\n"
            f"   This means the main prediction path FAILED.\n"
            f"   actions_dict: {actions_dict}\n"
            f"   Investigate why predict() couldn't complete!",
            exc_info=True
        )
        
        try:
            if not actions_dict:
                self.fallback_stats['majority_vote_empty'] += 1
                raise ValueError(
                    f"_majority_vote_fallback called with empty actions_dict! "
                    f"Fallback count: {self.fallback_stats['majority_vote_empty']}"
                )
            
            action_counts = Counter(actions_dict.values())
            majority_action = action_counts.most_common(1)[0][0]
            
            if majority_action not in [0, 1]:
                self.fallback_stats['majority_vote_invalid'] += 1
                raise ValueError(
                    f"Invalid majority_action={majority_action} from actions_dict={actions_dict}. "
                    f"Fallback count: {self.fallback_stats['majority_vote_invalid']}"
                )
            
            self.last_valid_vote = majority_action
            logger.warning(f"âš ï¸ FALLBACK: Using majority vote = {majority_action} from {action_counts}")
            return majority_action, None
            
        except Exception as e:
            self.fallback_stats['majority_vote_exception'] += 1
            logger.critical(
                f"âŒ _majority_vote_fallback EXCEPTION:\n"
                f"   Error: {e}\n"
                f"   actions_dict: {actions_dict}\n"
                f"   Fallback stats: {self.fallback_stats}",
                exc_info=True
            )
            raise

    # ======================================================================
    # V8.6.3 DIAGNOSTIC: Print fallback statistics
    # ======================================================================
    def print_diagnostic_stats(self):
        """Print diagnostic statistics to help identify inference pipeline issues."""
        total = self.fallback_stats['total_predictions']
        success = self.fallback_stats['successful_predictions']
        success_rate = (success / total * 100) if total > 0 else 0
        
        print("\n" + "=" * 70)
        print("QUANTUM VOTING SYSTEM - DIAGNOSTIC STATS")
        print("=" * 70)
        print(f"Total predictions:       {total}")
        print(f"Successful predictions:  {success} ({success_rate:.1f}%)")
        print("-" * 70)
        print("FALLBACK TRIGGERS:")
        print(f"  empty_q_values_dict:   {self.fallback_stats['empty_q_values_dict']}")
        print(f"  empty_actions_dict:    {self.fallback_stats['empty_actions_dict']}")
        print(f"  no_quantum_bridge:     {self.fallback_stats['no_quantum_bridge']}")
        print(f"  agent_prediction_fail: {self.fallback_stats['agent_prediction_failed']}")
        print(f"  no_quantum_actions:    {self.fallback_stats['no_quantum_actions']}")
        print(f"  majority_vote_empty:   {self.fallback_stats['majority_vote_empty']}")
        print(f"  majority_vote_invalid: {self.fallback_stats['majority_vote_invalid']}")
        print(f"  majority_vote_except:  {self.fallback_stats['majority_vote_exception']}")
        print("=" * 70)
        
        # Identify the most common failure
        failures = {k: v for k, v in self.fallback_stats.items() 
                   if k not in ['total_predictions', 'successful_predictions'] and v > 0}
        if failures:
            worst = max(failures, key=failures.get)
            print(f"âš ï¸ MOST COMMON FAILURE: {worst} ({failures[worst]} times)")
            print("=" * 70 + "\n")
        
        return self.fallback_stats

    # ======================================================================
    # SAFE PREDICTION WRAPPER FOR TENSORS
    # ======================================================================
    def predict_safe(self, q_values_dict: Dict[str, np.ndarray],
                     actions_dict: Dict[str, int]) -> Tuple[int, Optional[torch.Tensor]]:
        """Convert tensors to numpy arrays before calling predict."""
        safe_q_values = {}
        for agent_name, q_vals in q_values_dict.items():
            if hasattr(q_vals, "numpy"):
                safe_q_values[agent_name] = q_vals.numpy()
            else:
                safe_q_values[agent_name] = q_vals

        safe_actions = {}
        for agent_name, action in actions_dict.items():
            if hasattr(action, "numpy"):
                safe_actions[agent_name] = action.numpy()
            else:
                safe_actions[agent_name] = action

        return self.predict(safe_q_values, safe_actions)

    # ======================================================================
    # COLLECT TRAINING SAMPLE
    # ======================================================================
    def collect_training_sample(self, q_values_dict: Dict[str, np.ndarray],
                                actions_dict: Dict[str, int], final_action: int,
                                reward: float, was_correct: bool):
        """Collect samples for training."""
        try:
            with self.voting_collection_lock:
                sample = {
                    "q_values_dict": q_values_dict,
                    "actions_dict": actions_dict,
                    "final_action": final_action,
                    "reward": reward,
                    "was_correct": was_correct,
                    "timestamp": time.time(),
                }
                self.voting_sample_queue.append(sample)
                if len(self.voting_sample_queue) >= 64:
                    self._train_voting_system()
        except Exception as e:
            logger.error(f"Failed to collect quantum voting sample: {e}", exc_info=True)

    def _train_voting_system(self):
        """Train quantum voting subsystem if supported."""
        with self.voting_collection_lock:
            if len(self.voting_sample_queue) < 3:
                return
            samples = list(self.voting_sample_queue)
            self.voting_sample_queue.clear()
        try:
            if hasattr(self.quantum_bridge.quantum_system, "ensemble_fusion"):
                logger.info(f"Training quantum voting with {len(samples)} samples")
            else:
                logger.debug("Quantum system does not support ensemble fusion.")
        except Exception as e:
            logger.error(f"Quantum voting training failed: {e}", exc_info=True)

# ==============================================================================
# QUANTUM SYSTEM BRIDGE
# ==============================================================================

logger = logging.getLogger(__name__)

# Insert/replace this whole block into your engine file

# Fallback TIMEFRAME_LENGTHS if not defined elsewhere in the codebase
try:
    TIMEFRAME_LENGTHS  # noqa: F821
except Exception:

    TIMEFRAME_LENGTHS = {
   
        # === High-Frequency Zone (Volatility Capture) ===
        'xs': 5,     # tick
        's': 10,     # ultra
        'm': 20,     # fast
    
        # === Critical Trading Zones ===
        'l': 30,     # scalp
        'xl': 60,    # 1min
        'xxl': 120,  # 2min
    
        # === Structure & Regime Detection ===
        '5m': 300,   # 5min
        '10m': 600,  # 10min
    }

class QuantumSystemBridge:
    """Thread-safe Quantum system bridge with robust state caching and prediction."""

    def __init__(self, agent_names: List[str], state_dim: int = 58,
                 action_dim: int = 2, latent_dim: int = 32, device: Optional[str] = None):
        # =====================================================================
        # EXTENSIVE BUFFER INITIALIZATION - PRIORITY #1
        # =====================================================================
        logger.critical("="*80)
        logger.critical("QUANTUM BRIDGE INITIALIZATION - BUFFER PRIORITY MODE")
        logger.critical("="*80)

        # Basic attributes
        self.agent_names = list(agent_names)
        self.state_dim = int(state_dim)
        self.action_dim = int(action_dim)
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

        # Thread lock for cache safety
        self.cache_lock = threading.Lock()

        # Index maps
        self.agent_to_idx: Dict[str, int] = {name: idx for idx, name in enumerate(self.agent_names)}
        self.timeframe_to_idx: Dict[str, int] = {tf: idx for idx, tf in enumerate(TIMEFRAME_LENGTHS.keys())}

        # Initialize numpy state cache: (agents, timeframes, state_dim)
        self.state_cache = np.zeros((max(1, len(self.agent_names)), len(self.timeframe_to_idx), self.state_dim),
                                    dtype=np.float32)

        # Prediction caching
        self._last_full_prediction: Dict[str, np.ndarray] = {}
        self._last_prediction_timestamp: float = 0.0
        self._cache_ttl = 1  # seconds

        # Stats
        self.stats = {
            'predictions_made': 0,
            'predictions_failed': 0,
            'states_cached': 0,
            'cache_misses': 0
        }

        # Quantum system placeholders (DO NOT reset hybrid_buffer here!)
        self.quantum_system = None
        # self.hybrid_buffer = None  â† REMOVED: This was resetting the buffer!
        self.quantum_trainer = None

        # Try to initialize quantum system (best-effort)
        try:
            logger.info("Initializing MultiTimeframeEntangledComplexAgentSystem...")
            # NOTE: MultiTimeframeEntangledComplexAgentSystem must be defined/imported elsewhere
            self.quantum_system = MultiTimeframeEntangledComplexAgentSystem(
                agent_names=self.agent_names,
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                latent_dim=latent_dim,
                device=self.device
            )
            logger.info(f"âœ… Quantum system created with {len(self.agent_names)} agents")
        except Exception as e:
            logger.error(f"âŒ Failed to create quantum system: {e}")
            import traceback
            traceback.print_exc()
            self.quantum_system = None

        # =====================================================================
        # CRITICAL: CREATE HYBRID BUFFER WITH EXTENSIVE VERIFICATION
        # V8.6.2 FIX: Use MultiAgentExperienceReplay for proper CTDE
        # =====================================================================
        logger.critical("ğŸ”§ Creating hybrid experience buffer (CRITICAL COMPONENT)...")
        logger.critical("   V8.6.2: Using MultiAgentExperienceReplay for agent attribution")
        self.hybrid_buffer = None
        
        # V8.6.2: Create concurrent state collector
        try:
            self.state_collector = ConcurrentStateCollector(
                agent_names=self.agent_names,
                timeframes=list(TIMEFRAME_LENGTHS.keys()),
                state_dim=self.state_dim,
                staleness_threshold_ms=100.0
            )
            logger.critical("âœ… V8.6.2: ConcurrentStateCollector created")
        except Exception as e:
            logger.warning(f"âš ï¸  ConcurrentStateCollector failed: {e}")
            self.state_collector = None

        try:
            # V8.6.2: Prefer MultiAgentExperienceReplay (best option)
            try:
                self.hybrid_buffer = MultiAgentExperienceReplay(
                    capacity=100_000,
                    device=self.device,
                    agent_names=self.agent_names
                )
                logger.critical("âœ… V8.6.2: MultiAgentExperienceReplay created (capacity: 100k)")
                logger.critical("   â€¢ Per-agent experience retrieval enabled")
                logger.critical("   â€¢ Agent attribution tracking enabled")
                logger.critical("   â€¢ Priority sampling enabled")
            except Exception as e1:
                logger.warning(f"âš ï¸  MultiAgentExperienceReplay failed: {e1}, trying ExperienceReplay...")
                
                # Fallback 1: Standard ExperienceReplay
                try:
                    self.hybrid_buffer = ExperienceReplay(capacity=100_000)
                    logger.critical("âœ… ExperienceReplay buffer created (capacity: 100k)")
                except Exception as e2:
                    logger.warning(f"âš ï¸  ExperienceReplay failed: {e2}, trying deque...")

                    # Fallback 2: Deque
                    try:
                        self.hybrid_buffer = deque(maxlen=100_000)
                        logger.critical("âœ… Deque buffer created (capacity: 100k)")
                    except Exception as e3:
                        logger.error(f"âš ï¸  Deque failed: {e3}, using emergency fallback...")
                        self.hybrid_buffer = deque(maxlen=10_000)
                        logger.critical("âš ï¸  Emergency deque buffer created (capacity: 10k)")

        except Exception as e:
            logger.critical(f"âŒ ALL BUFFER CREATION FAILED: {e}")
            self.hybrid_buffer = deque(maxlen=10_000)
            logger.critical("ğŸ†˜ Last resort buffer created")

        # VERIFY BUFFER WAS CREATED
        if self.hybrid_buffer is None:
            logger.critical("âŒ CRITICAL ERROR: Buffer is still None after creation!")
            self.hybrid_buffer = deque(maxlen=10_000)
            logger.critical("ğŸ†˜ Force-created emergency buffer")

        buffer_type = type(self.hybrid_buffer).__name__
        buffer_cap = getattr(self.hybrid_buffer, 'capacity', getattr(self.hybrid_buffer, 'maxlen', 'unlimited'))
        logger.critical(f"âœ… BUFFER VERIFIED: Type={buffer_type}, Capacity={buffer_cap}")
        # =====================================================================

        # =====================================================================
        # CREATE QUANTUM TRAINER AND LINK BUFFER
        # =====================================================================
        try:
            if self.quantum_system is not None:
                logger.critical("ğŸ”§ Creating QuantumSystemTrainer...")
                # V8.5: Create base trainer first
                base_trainer = QuantumSystemTrainer(
                    system=self.quantum_system,
                    buffer=self.hybrid_buffer,  # Pass OUR buffer
                    batch_size=64,
                    gamma=0.92
                )

                # CRITICAL: Ensure base trainer uses our buffer (not a copy)
                if hasattr(base_trainer, 'buffer'):
                    if base_trainer.buffer is not self.hybrid_buffer:
                        logger.warning("âš ï¸  Trainer has different buffer, linking to hybrid_buffer...")
                        base_trainer.buffer = self.hybrid_buffer
                else:
                    logger.warning("âš ï¸  Trainer missing buffer attribute, adding it...")
                    base_trainer.buffer = self.hybrid_buffer

                # Verify link
                if base_trainer.buffer is self.hybrid_buffer:
                    logger.critical("âœ… Trainer buffer LINKED to hybrid_buffer (same object)")
                else:
                    logger.error("âŒ Trainer buffer is NOT linked to hybrid_buffer!")

                logger.critical("âœ… Base quantum trainer initialized successfully")

                # =====================================================================
                # V8.5: WRAP WITH VOTING ENSEMBLE TRAINER
                # =====================================================================
                logger.critical("ğŸ—³ï¸  V8.5: Wrapping trainer with VotingEnsembleTrainer...")
                logger.critical("   â€¢ Adds 3 training strategies that vote on gradient updates")
                logger.critical("   â€¢ Conservative (LR=5e-5), Aggressive (LR=5e-4), Adaptive (LR=1e-4)")
                logger.critical("   â€¢ Only applies updates when strategies reach consensus (60%)")
                logger.critical("   â€¢ Your QuantumVotingSystem (action voting) remains unchanged")

                self.quantum_trainer = VotingEnsembleTrainer(
                    base_trainer=base_trainer,
                    voting_threshold=0.6,  # Need 60% agreement
                    max_gradient_norm=3.0,
                    performance_window=1000,
                    device=base_trainer.device
                )

                logger.critical("âœ… V8.5: Voting Ensemble Trainer activated!")
                logger.critical("   â€¢ Training now uses DUAL VOTING SYSTEMS:")
                logger.critical("   â€¢ Level 1: Gradient voting (VotingEnsembleTrainer) - NEW")
                logger.critical("   â€¢ Level 2: Action voting (QuantumVotingSystem) - UNCHANGED")

                # ================================================================
                # V8.5.8 FIX: EXPLICIT BUFFER VERIFICATION AND LINKING
                # ================================================================
                logger.critical("")
                logger.critical("ğŸ”§ V8.5.8: Verifying trainer buffer linkage...")

                # Test if buffer property works
                buffer_property_works = False
                try:
                    test_buffer = self.quantum_trainer.buffer
                    buffer_property_works = True
                    logger.critical(f"   âœ… Buffer property accessible: {type(test_buffer).__name__ if test_buffer else 'None'}")

                    if test_buffer is None:
                        logger.warning("   âš ï¸  Buffer property returns None, setting explicitly...")
                        self.quantum_trainer.buffer = self.hybrid_buffer
                        logger.critical("   âœ… Buffer set via property setter")
                    elif test_buffer is not self.hybrid_buffer:
                        logger.warning("   âš ï¸  Buffer is different object, relinking...")
                        self.quantum_trainer.buffer = self.hybrid_buffer
                        logger.critical("   âœ… Buffer relinked to hybrid_buffer")
                    else:
                        logger.critical("   âœ… Buffer already correctly linked!")

                except AttributeError as e:
                    logger.error(f"   âŒ Buffer property not accessible: {e}")
                    logger.critical("   ğŸ”§ Setting buffer directly on base_trainer...")
                    if hasattr(self.quantum_trainer, 'base_trainer'):
                        self.quantum_trainer.base_trainer.buffer = self.hybrid_buffer
                        logger.critical("   âœ… Buffer set on base_trainer")
                    buffer_property_works = False
                except Exception as e:
                    logger.error(f"   âŒ Unexpected error accessing buffer: {e}")
                    buffer_property_works = False

                # Final verification
                try:
                    final_buffer = self.quantum_trainer.buffer
                    if final_buffer is self.hybrid_buffer:
                        logger.critical("   âœ… FINAL CHECK: Trainer buffer correctly linked!")
                    elif final_buffer is None:
                        logger.error("   âŒ FINAL CHECK: Buffer is None!")
                    else:
                        logger.warning(f"   âš ï¸  FINAL CHECK: Buffer objects don't match")
                        logger.warning(f"      Trainer buffer: {id(final_buffer)}")
                        logger.warning(f"      Hybrid buffer:  {id(self.hybrid_buffer)}")
                except Exception as e:
                    logger.error(f"   âŒ FINAL CHECK FAILED: {e}")

                logger.critical("=" * 80)
                # ================================================================
            else:
                logger.warning("âš ï¸ Quantum system is None, skipping trainer initialization")
                self.quantum_trainer = None
        except Exception as e:
            logger.error(f"âŒ Failed to create quantum trainer: {e}")
            import traceback
            traceback.print_exc()
            self.quantum_trainer = None

        # =====================================================================
        # FINAL VERIFICATION AND SUMMARY
        # =====================================================================
        logger.critical("=" * 80)
        logger.critical("QUANTUM SYSTEM BRIDGE - INITIALIZATION COMPLETE")
        logger.critical("=" * 80)

        # Verify quantum system
        logger.critical(f"Quantum System: {'âœ… OK' if self.quantum_system is not None else 'âŒ FAILED'}")

        # CRITICAL BUFFER VERIFICATION
        buffer_ok = self.hybrid_buffer is not None
        buffer_size = len(self.hybrid_buffer) if buffer_ok else 0
        buffer_type = type(self.hybrid_buffer).__name__ if buffer_ok else 'None'
        logger.critical(f"Hybrid Buffer: {'âœ… OK' if buffer_ok else 'âŒ FAILED'} | Type={buffer_type} | Size={buffer_size}")

        # Verify trainer
        trainer_ok = self.quantum_trainer is not None
        logger.critical(f"Quantum Trainer: {'âœ… OK' if trainer_ok else 'âŒ FAILED'}")

        # Verify trainer buffer link
        if trainer_ok and hasattr(self.quantum_trainer, 'buffer'):
            trainer_buffer_ok = self.quantum_trainer.buffer is not None
            trainer_buffer_size = len(self.quantum_trainer.buffer) if trainer_buffer_ok else 0
            trainer_buffer_linked = self.quantum_trainer.buffer is self.hybrid_buffer
            logger.critical(f"Trainer Buffer: {'âœ… OK' if trainer_buffer_ok else 'âŒ FAILED'} | Size={trainer_buffer_size} | Linked={'âœ… YES' if trainer_buffer_linked else 'âŒ NO'}")
        else:
            logger.warning("Trainer Buffer: âš ï¸ NOT FOUND")

        logger.critical(f"Agents: {len(self.agent_names)} | State Dim: {self.state_dim} | Device: {self.device}")
        logger.critical("=" * 80)

        # CRITICAL: Final emergency check
        if self.hybrid_buffer is None:
            logger.critical("âŒ CRITICAL: BUFFER IS STILL NONE! EMERGENCY RECOVERY...")
            self.hybrid_buffer = deque(maxlen=10_000)
            logger.critical("âœ… Emergency buffer created successfully")

            # Re-link to trainer if it exists
            if self.quantum_trainer and hasattr(self.quantum_trainer, 'buffer'):
                self.quantum_trainer.buffer = self.hybrid_buffer
                logger.critical("âœ… Emergency buffer linked to trainer")

    # =====================================================================
    # BUFFER MANAGEMENT & HEALTH MONITORING METHODS
    # =====================================================================

    def _verify_buffer_exists(self):
        """
        CRITICAL: Verify buffer exists and auto-recover if None.
        Call this before ANY buffer operation.
        """
        if self.hybrid_buffer is None:
            logger.critical("âš ï¸  BUFFER MISSING - EMERGENCY AUTO-RECOVERY")
            self.hybrid_buffer = deque(maxlen=10_000)
            logger.critical("âœ… Emergency buffer recovered")

            # Re-link to trainer
            if self.quantum_trainer and hasattr(self.quantum_trainer, 'buffer'):
                self.quantum_trainer.buffer = self.hybrid_buffer
                logger.critical("âœ… Buffer re-linked to trainer")

            return True
        return True

    def get_buffer_stats(self):
        """Get detailed buffer statistics for monitoring."""
        self._verify_buffer_exists()

        try:
            stats = {
                'exists': self.hybrid_buffer is not None,
                'type': type(self.hybrid_buffer).__name__,
                'size': len(self.hybrid_buffer),
                'capacity': getattr(self.hybrid_buffer, 'maxlen', 'unlimited'),
                'is_functional': True,
                'trainer_linked': False
            }

            # Check trainer link
            if self.quantum_trainer and hasattr(self.quantum_trainer, 'buffer'):
                stats['trainer_linked'] = self.quantum_trainer.buffer is self.hybrid_buffer
                stats['trainer_buffer_size'] = len(self.quantum_trainer.buffer) if self.quantum_trainer.buffer else 0

            return stats
        except Exception as e:
            logger.error(f"âŒ Error getting buffer stats: {e}")
            return {'exists': False, 'error': str(e)}

    def log_buffer_health(self):
        """Log detailed buffer health for diagnostics."""
        stats = self.get_buffer_stats()
        logger.critical("="*60)
        logger.critical("BUFFER HEALTH CHECK")
        logger.critical("="*60)
        logger.critical(f"âœ… Exists: {stats.get('exists', False)}")
        logger.critical(f"âœ… Type: {stats.get('type', 'None')}")
        logger.critical(f"âœ… Size: {stats.get('size', 0)}")
        logger.critical(f"âœ… Capacity: {stats.get('capacity', 0)}")
        logger.critical(f"âœ… Functional: {stats.get('is_functional', False)}")
        logger.critical(f"âœ… Trainer Linked: {stats.get('trainer_linked', False)}")
        if 'trainer_buffer_size' in stats:
            logger.critical(f"âœ… Trainer Buffer Size: {stats.get('trainer_buffer_size', 0)}")
        logger.critical("="*60)

    # -------------------------------
    # safe_quantum_predict
    # -------------------------------
    def safe_quantum_predict(self, state, agent_name: Optional[str] = None, return_confidence: bool = False):
        """
        Whitepaper-compliant robust quantum-CNN prediction.
        Handles multi-dimensional CNN outputs, multi-timeframe aggregation,
        normalization, and optional confidence estimation.
        """
        try:
            import torch
            import numpy as np

            # -------------------------------
            # Validate & convert input state
            # -------------------------------
            if state is None or isinstance(state, str):
                raise ValueError(f"Invalid state type ({type(state)}): {state}")

            if isinstance(state, np.ndarray):
                if not np.issubdtype(state.dtype, np.number):
                    raise ValueError("Non-numeric numpy state detected")
                state_t = torch.tensor(state, dtype=torch.float32)
            elif torch.is_tensor(state):
                state_t = state.detach().cpu().float()
            elif isinstance(state, (list, tuple)):
                if not all(isinstance(x, (int, float, np.floating, np.integer)) for x in state):
                    raise ValueError("Non-numeric list state detected")
                state_t = torch.tensor(state, dtype=torch.float32)
            else:
                raise ValueError(f"Unsupported state type: {type(state)}")

            if state_t.numel() == 0:
                raise ValueError("Empty state tensor")
            state_t = state_t.to(self.device)

            # -------------------------------
            # Forward pass through model
            # -------------------------------
            output = None
            model = getattr(self, "quantum_model", None)

            if callable(model):
                output = model(state_t)
            elif hasattr(self, "predict") and callable(self.predict):
                output = self.predict(state_t)
            elif hasattr(self, "quantum_system") and hasattr(self.quantum_system, "predict_single"):
                output = self.quantum_system.predict_single(state_t)
            else:
                raise AttributeError("No callable prediction model found in bridge")

            # -------------------------------
            # Convert output to numpy array
            # -------------------------------
            if isinstance(output, torch.Tensor):
                arr = output.detach().cpu()
                # Collapse all but batch dimension (global avg pooling)
                if arr.ndim > 1:
                    arr = arr.mean(dim=tuple(range(1, arr.ndim)))
                arr = arr.numpy()
            else:
                arr = np.array(output, dtype=np.float32)
                if arr.ndim > 1:
                    arr = arr.mean(axis=tuple(range(1, arr.ndim)))

            # -------------------------------
            # Handle multi-timeframe dict outputs
            # -------------------------------
            if isinstance(arr, dict):
                arrs = [np.asarray(v, dtype=np.float32).ravel() for v in arr.values()]
                arr = np.mean(np.stack(arrs, axis=0), axis=0)

            # -------------------------------
            # Clean, normalize, reduce to scalar
            # -------------------------------
            arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)
            scalar_output = float(np.mean(arr)) if arr.size > 1 else float(arr.item())
            scalar_output = float(np.clip(scalar_output, 0.0, 1.0))

            # -------------------------------
            # Optional confidence estimation
            # -------------------------------
            if return_confidence:
                conf = 0.0
                try:
                    probs = torch.softmax(torch.tensor(arr, dtype=torch.float32), dim=-1)
                    entropy = -torch.sum(probs * torch.log(probs + 1e-8))
                    conf = 1.0 - float(entropy.item()) / np.log(len(arr) + 1e-8)
                    conf = np.clip(conf, 0.0, 1.0)
                except Exception:
                    conf = 0.0
                return scalar_output, conf

            return scalar_output

        except Exception as e:
            logger.error(f"[{agent_name or 'Bridge'}] Quantum prediction failed: {e}")
            import traceback; traceback.print_exc()
            if return_confidence:
                return 0.0, 0.0
            return 0.0

    # -------------------------------
    # Thread-safe agent state update
    # -------------------------------
    def update_agent_state(self, agent_name: str, state: Any):
        """Store agent state using integer indexing and cache safety."""
        if self.state_cache is None:
            logger.warning("QuantumSystemBridge not initialized (no cache)")
            return

        try:
            # convert to numpy
            if isinstance(state, torch.Tensor):
                state = state.detach().cpu().numpy()
            state_arr = np.asarray(state, dtype=np.float32).flatten()

            # resize/pad/truncate
            if state_arr.size < self.state_dim:
                state_arr = np.pad(state_arr, (0, self.state_dim - state_arr.size), mode='constant')
            elif state_arr.size > self.state_dim:
                state_arr = state_arr[:self.state_dim]

            # ensure agent exists
            with self.cache_lock:
                if agent_name not in self.agent_to_idx:
                    new_idx = len(self.agent_to_idx)
                    self.agent_to_idx[agent_name] = new_idx
                    # expand state_cache
                    new_size = max(new_idx + 1, self.state_cache.shape[0])
                    new_cache = np.zeros((new_size, self.state_cache.shape[1], self.state_dim), dtype=np.float32)
                    new_cache[:self.state_cache.shape[0]] = self.state_cache
                    self.state_cache = new_cache

                agent_idx = self.agent_to_idx[agent_name]
                # store same state across all timeframes
                for tf, tf_idx in self.timeframe_to_idx.items():
                    self.state_cache[agent_idx, tf_idx] = state_arr.copy()

                self.stats['states_cached'] += 1
                logger.debug(f"[{agent_name}] State cached for all timeframes")

        except Exception as e:
            logger.error(f"[{agent_name}] Failed to update state cache: {e}")
            import traceback
            traceback.print_exc()

    # -------------------------------
    # Convert cache to dict for prediction
    # -------------------------------
    def _get_state_cache_for_prediction(self) -> Dict[str, Dict[str, np.ndarray]]:
        """Return dictionary {agent: {timeframe: array}} built from the internal cache."""
        with self.cache_lock:
            states_dict: Dict[str, Dict[str, np.ndarray]] = {}
            for agent_name, agent_idx in self.agent_to_idx.items():
                states_dict[agent_name] = {}
                for tf, tf_idx in self.timeframe_to_idx.items():
                    if 0 <= agent_idx < self.state_cache.shape[0] and 0 <= tf_idx < self.state_cache.shape[1]:
                        states_dict[agent_name][tf] = self.state_cache[agent_idx, tf_idx].copy()
                    else:
                        states_dict[agent_name][tf] = np.zeros(self.state_dim, dtype=np.float32)
                        logger.debug(f"[{agent_name}][{tf}] Index out of bounds, returning zeros")
            return states_dict

    # -------------------------------
    # Predict for a single agent (uses predict_all_agents_with_metadata)
    # V5.0: Added diversity injection for stuck agents
    # -------------------------------
    @torch.no_grad()
    def predict_single_agent(self, agent_name: str, states_dict: Optional[dict] = None) -> np.ndarray:
        """
        V5.0 ENHANCED: predict_single_agent with DIVERSITY INJECTION.
        
        Tracks per-agent vote history and injects small noise if agent
        votes the same way 20 times in a row to break stuck patterns.
        """
        import time
        
        logger.critical(f"ğŸ¯ [SINGLE AGENT] Predicting for agent: {agent_name}")

        # Initialize per-agent vote history tracking
        if not hasattr(self, '_agent_vote_history'):
            self._agent_vote_history = {}

        try:
            # Get all Q-values with quantum enhancement
            all_q_vals, metadata = self.predict_all_agents_with_metadata()

            # Check if quantum advisor was applied
            if metadata.get('quantum_forecast', {}).get('applied', False):
                logger.critical(f"âœ… [SINGLE AGENT] {agent_name}: Using quantum-adjusted Q-values")
            else:
                logger.critical(f"âš ï¸  [SINGLE AGENT] {agent_name}: Using base Q-values (no quantum adjustment)")

            q_vals = all_q_vals.get(agent_name, np.array([0.5, 0.5], dtype=np.float32))
            q_vals = np.asarray(q_vals, dtype=np.float32).ravel()

            # Ensure 2-element array for binary action compatibility
            if q_vals.size == 1:
                q_vals = np.array([1.0 - q_vals.item(), q_vals.item()], dtype=np.float32)
            elif q_vals.size == 0:
                q_vals = np.array([0.5, 0.5], dtype=np.float32)

            # V5.0: DIVERSITY INJECTION
            # Track this agent's recent votes
            if agent_name not in self._agent_vote_history:
                self._agent_vote_history[agent_name] = []
            
            action = 0 if q_vals[0] > q_vals[1] else 1
            self._agent_vote_history[agent_name].append(action)
            
            # Keep only last 20 votes
            if len(self._agent_vote_history[agent_name]) > 20:
                self._agent_vote_history[agent_name].pop(0)
            
            # Check if agent is stuck (same vote 20 times in a row)
            recent = self._agent_vote_history[agent_name]
            if len(recent) >= 20 and len(set(recent)) == 1:
                # Agent is stuck! Inject small noise to encourage exploration
                noise = np.random.uniform(-0.05, 0.05, size=2).astype(np.float32)
                q_vals_noisy = q_vals + noise
                
                new_action = 0 if q_vals_noisy[0] > q_vals_noisy[1] else 1
                if new_action != action:
                    print(f"[{time.strftime('%H:%M:%S')}] ğŸ”„ [{agent_name}] DIVERSITY INJECTION: {['BUY','SELL'][action]} â†’ {['BUY','SELL'][new_action]}")
                    q_vals = q_vals_noisy
                    # Clear history to reset
                    self._agent_vote_history[agent_name].clear()

            # Log the Q-values with action indication
            if q_vals.size >= 2:
                final_action = 'BUY' if q_vals[0] > q_vals[1] else 'SELL'
                print(f"[{time.strftime('%H:%M:%S')}] âœ… [{agent_name}] Q=[{q_vals[0]:.4f}, {q_vals[1]:.4f}] â†’ {final_action}")

            return q_vals

        except Exception as e:
            logger.critical(f"âŒ [SINGLE AGENT] {agent_name}: Prediction error: {e}")
            import traceback
            traceback.print_exc()
            return np.array([0.5, 0.5], dtype=np.float32)

    @torch.no_grad()
    def predict_all_agents_with_metadata(self) -> Tuple[Dict[str, np.ndarray], Dict[str, Any]]:
        """
        V8.6.3 DIAGNOSTIC: Return safe q-values dict with VERBOSE error logging.
        No silent empty returns - all failures are logged as CRITICAL.
        """
        # ========== VALIDATION WITH CRITICAL LOGGING ==========
        if self.quantum_system is None:
            logger.critical(
                f"âŒ predict_all_agents_with_metadata: quantum_system is None!\n"
                f"   This will cause empty Q-values â†’ fallback to BUY\n"
                f"   Was the quantum system initialized properly?",
                exc_info=True
            )
            raise ValueError("quantum_system is None - cannot predict")
        
        if self.state_cache is None:
            logger.critical(
                f"âŒ predict_all_agents_with_metadata: state_cache is None!\n"
                f"   This will cause empty Q-values â†’ fallback to BUY\n"
                f"   Was update_agent_state() called for each agent?",
                exc_info=True
            )
            raise ValueError("state_cache is None - cannot predict")

        states_dict = self._get_state_cache_for_prediction()
        if not states_dict:
            logger.critical(
                f"âŒ predict_all_agents_with_metadata: states_dict is EMPTY!\n"
                f"   state_cache shape: {self.state_cache.shape if self.state_cache is not None else 'None'}\n"
                f"   agent_to_idx: {self.agent_to_idx}\n"
                f"   timeframe_to_idx: {self.timeframe_to_idx}\n"
                f"   This will cause empty Q-values â†’ fallback to BUY",
                exc_info=True
            )
            raise ValueError(f"states_dict is empty. agent_to_idx={self.agent_to_idx}")

        # V8.6.3: Log state cache status
        logger.info(
            f"ğŸ“Š State cache status: {len(states_dict)} agents, "
            f"cache_shape={self.state_cache.shape if self.state_cache is not None else 'None'}"
        )

        agent_q_values: Dict[str, np.ndarray] = {}
        metadata: Dict[str, Any] = {}

        try:
            self.quantum_system.eval()

            # V8.1 FIX: Get base Q-values from system's native predict
            # Check if we have the original predict method saved
            if hasattr(self.quantum_system, '_original_predict'):
                # Use saved original to avoid recursion
                q_values_entangled, base_metadata = self.quantum_system._original_predict(states_dict)
            else:
                # Use current predict (which might be patched or not)
                q_values_entangled, base_metadata = self.quantum_system.predict(states_dict)

            # V8.1 FIX: Apply quantum forecast if quantum advisor exists
            if hasattr(self.quantum_system, 'quantum_advisor') and self.quantum_system.quantum_advisor is not None:
                # Apply quantum tempering

                q_values_entangled, quantum_metadata = apply_quantum_forecast(
                    self.quantum_system,
                    q_values_entangled,
                    states_dict
                )
                # Merge metadata
                metadata = {**base_metadata, **quantum_metadata}
                metadata['quantum_tempering_applied'] = True
            else:
                # No quantum advisor, use base values
                metadata = base_metadata
                metadata['quantum_tempering_applied'] = False

            for agent_name, tf_map in q_values_entangled.items():
                # Prefer 'm' timeframe or first available
                if isinstance(tf_map, dict):
                    q_tensor = tf_map.get('m', next(iter(tf_map.values())))
                else:
                    q_tensor = tf_map

                # Convert to numpy and flatten
                if isinstance(q_tensor, torch.Tensor):
                    arr = q_tensor.detach().cpu().numpy()
                else:
                    arr = np.array(q_tensor, dtype=np.float32)
                arr = np.nan_to_num(arr, nan=0.0, posinf=1.0, neginf=0.0)

                # Collapse multi-dim arrays safely
                if arr.ndim > 1:
                    arr = np.mean(arr, axis=tuple(range(1, arr.ndim)))

                arr = np.asarray(arr, dtype=np.float32).ravel()

                # Ensure at least 2 elements for binary action
                if arr.size == 1:
                    arr = np.array([1.0 - arr.item(), arr.item()], dtype=np.float32)
                elif arr.size == 0:
                    arr = np.array([0.5, 0.5], dtype=np.float32)

                agent_q_values[agent_name] = arr

            # =========================================================
            # V8.6.2: ANTI-HOMOGENIZATION - Check for Q-value bias
            # =========================================================
            if len(agent_q_values) >= 4:
                actions = [int(np.argmax(q[:2])) for q in agent_q_values.values()]
                buy_count = sum(1 for a in actions if a == 0)
                sell_count = len(actions) - buy_count
                
                # If 87.5%+ agents would vote same way, perturb Q-values
                if buy_count >= len(actions) * 0.875 or sell_count >= len(actions) * 0.875:
                    logger.warning(f"âš ï¸ Q-VALUE HOMOGENIZATION: {buy_count} BUY, {sell_count} SELL")
                    
                    # Perturb 25% of agents toward minority action
                    agents_list = list(agent_q_values.keys())
                    num_to_perturb = max(2, len(agents_list) // 4)
                    agents_to_perturb = random.sample(agents_list, num_to_perturb)
                    
                    for agent_name in agents_to_perturb:
                        q = agent_q_values[agent_name]
                        if len(q) >= 2:
                            # Add noise that slightly favors minority action
                            minority_idx = 1 if buy_count > sell_count else 0
                            noise = np.random.uniform(0.02, 0.08)
                            q[minority_idx] += noise
                            agent_q_values[agent_name] = q
                            logger.debug(f"  â†’ Perturbed {agent_name} toward {'SELL' if minority_idx==1 else 'BUY'}")
                    
                    metadata['diversity_perturbation_applied'] = True

            self._last_full_prediction = agent_q_values
            self._last_prediction_timestamp = time.time()
            self.stats['predictions_made'] += 1

            try:
                self.quantum_system.train()
            except Exception:
                pass

            return agent_q_values, metadata

        except Exception as e:
            logger.error(f"Quantum prediction error: {e}")
            import traceback; traceback.print_exc()
            self.stats['predictions_failed'] += 1
            try:
                self.quantum_system.train()
            except Exception:
                pass
            return {}, {}

    # -------------------------------
    # Store experience
    # -------------------------------
    def store_experience_for_agent(self, agent_name: str, state: Any,
                                   action: Any, reward: float, next_state: Any,
                                   done: bool = False, 
                                   agent_multipliers: Dict[str, float] = None,
                                   all_agent_actions: Dict[str, int] = None,
                                   all_agent_q_values: Dict[str, np.ndarray] = None) -> bool:
        """
        Safely build and store a multi-agent experience into hybrid buffer(s).
        
        V8.6.2 FIX: Creates MultiAgentExperience with full agent attribution
        for proper CTDE (Centralized Training, Decentralized Execution).
        
        Args:
            agent_name: Name of the originating agent
            state: Current state for this agent
            action: Action taken by this agent
            reward: Global reward
            next_state: Next state for this agent
            done: Episode termination flag
            agent_multipliers: Dict of agent_name â†’ credit multiplier (V8.6.1)
            all_agent_actions: Dict of all agent actions (V8.6.2)
            all_agent_q_values: Dict of all agent Q-values (V8.6.2)
        """
        try:
            # CRITICAL: Verify buffer exists FIRST
            self._verify_buffer_exists()

            if state is None:
                return False

            # Convert and validate state/next_state
            s = np.asarray(state, dtype=np.float32).flatten()
            if s.size == 0:
                return False
            ns = np.asarray(next_state if next_state is not None else state, dtype=np.float32).flatten()
            if ns.size == 0:
                ns = s.copy()

            # Normalize sizes
            if s.size < self.state_dim:
                s = np.pad(s, (0, self.state_dim - s.size), mode='constant')
            elif s.size > self.state_dim:
                s = s[:self.state_dim]
            if ns.size < self.state_dim:
                ns = np.pad(ns, (0, self.state_dim - ns.size), mode='constant')
            elif ns.size > self.state_dim:
                ns = ns[:self.state_dim]

            # Convert action to int and one-hot
            try:
                if isinstance(action, (list, tuple, np.ndarray)):
                    action_val = int(np.asarray(action).ravel()[0])
                else:
                    action_val = int(action)
                action_val = max(0, min(self.action_dim - 1, action_val))
            except Exception:
                action_val = 0

            action_array = np.zeros(self.action_dim, dtype=np.float32)
            action_array[action_val] = 1.0

            # Build full multi-agent states
            states_dict: Dict[str, Dict[str, np.ndarray]] = {}
            next_states_dict: Dict[str, Dict[str, np.ndarray]] = {}
            
            for ag_name in self.agent_names:
                if ag_name == agent_name:
                    ag_state = s.copy()
                    ag_next = ns.copy()
                else:
                    # Read from cache safely - V8.6.2: Use state_collector if available
                    if hasattr(self, 'state_collector') and self.state_collector is not None:
                        collected, _ = self.state_collector.collect_all_states()
                        ag_state_dict = collected.get(ag_name, {})
                        # Use first timeframe's state
                        ag_state = next(iter(ag_state_dict.values()), np.zeros(self.state_dim))
                    else:
                        with self.cache_lock:
                            ag_idx = self.agent_to_idx.get(ag_name, 0)
                            if 0 <= ag_idx < self.state_cache.shape[0]:
                                ag_state = self.state_cache[ag_idx, 0].copy()
                            else:
                                ag_state = np.zeros(self.state_dim, dtype=np.float32)
                    ag_next = ag_state.copy()

                # Ensure sizes
                if ag_state.size != self.state_dim:
                    if ag_state.size < self.state_dim:
                        ag_state = np.pad(ag_state, (0, self.state_dim - ag_state.size))
                    else:
                        ag_state = ag_state[:self.state_dim]
                if ag_next.size != self.state_dim:
                    if ag_next.size < self.state_dim:
                        ag_next = np.pad(ag_next, (0, self.state_dim - ag_next.size))
                    else:
                        ag_next = ag_next[:self.state_dim]

                # Replicate to all timeframes
                states_dict[ag_name] = {tf: ag_state.copy() for tf in TIMEFRAME_LENGTHS.keys()}
                next_states_dict[ag_name] = {tf: ag_next.copy() for tf in TIMEFRAME_LENGTHS.keys()}

            # V8.6.2: Compute per-agent rewards using credit assignment
            if agent_multipliers:
                per_agent_rewards = {
                    name: reward * agent_multipliers.get(name, 1.0) 
                    for name in self.agent_names
                }
            else:
                per_agent_rewards = {name: reward for name in self.agent_names}

            # V8.6.2: Build per-agent actions dict
            if all_agent_actions:
                actions_dict = {}
                for ag_name in self.agent_names:
                    ag_action_val = all_agent_actions.get(ag_name, 0)
                    ag_action_arr = np.zeros(self.action_dim, dtype=np.float32)
                    ag_action_arr[ag_action_val] = 1.0
                    actions_dict[ag_name] = ag_action_arr
            else:
                # Only have originating agent's action
                actions_dict = {agent_name: action_array}

            # V8.6.2: Build per-agent Q-values dict
            q_values_dict = all_agent_q_values if all_agent_q_values else {}

            # =====================================================================
            # V8.6.2: CREATE MULTI-AGENT EXPERIENCE WITH FULL ATTRIBUTION
            # =====================================================================
            try:
                exp = create_multi_agent_experience(
                    originating_agent=agent_name,
                    agent_names=self.agent_names,
                    states_dict=states_dict,
                    actions=actions_dict,
                    rewards=per_agent_rewards,
                    global_reward=reward,
                    q_values=q_values_dict,
                    done=done,
                    next_states_dict=next_states_dict
                )
                logger.debug(f"[{agent_name}] V8.6.2: Created MultiAgentExperience")
            except Exception as e:
                # Fallback to legacy Experience format
                logger.debug(f"[{agent_name}] Falling back to legacy Experience: {e}")
                scaled_reward = per_agent_rewards.get(agent_name, reward)
                exp = Experience(
                    states=states_dict, 
                    action=action_array, 
                    reward=float(scaled_reward),
                    next_states=next_states_dict, 
                    done=bool(done),
                    originating_agent_id=agent_name,
                    agent_multipliers=agent_multipliers,
                    timestamp=time.time(),
                    global_reward=reward
                )

            # =====================================================================
            # STORE EXPERIENCE WITH EXTENSIVE VERIFICATION
            # =====================================================================
            stored = False

            if self.hybrid_buffer is None:
                logger.error(f"[{agent_name}] âŒ BUFFER IS NONE! Emergency recovery...")
                self._verify_buffer_exists()

            if self.hybrid_buffer is not None:
                try:
                    before_size = len(self.hybrid_buffer)
                    self.hybrid_buffer.append(exp)
                    after_size = len(self.hybrid_buffer)
                    stored = True
                    logger.info(f"[{agent_name}] âœ… Stored to hybrid_buffer (size: {before_size}â†’{after_size})")

                    if self.quantum_trainer and hasattr(self.quantum_trainer, 'buffer'):
                        if self.quantum_trainer.buffer is not self.hybrid_buffer:
                            logger.warning(f"[{agent_name}] âš ï¸  Trainer buffer not linked, re-linking...")
                            self.quantum_trainer.buffer = self.hybrid_buffer

                    return True
                except Exception as e:
                    logger.error(f"[{agent_name}] âŒ hybrid_buffer append failed: {e}")
                    stored = False
            else:
                logger.error(f"[{agent_name}] âŒ hybrid_buffer is STILL None after recovery!")

            # Fallback: try trainer.buffer directly
            if not stored and self.quantum_trainer is not None and hasattr(self.quantum_trainer, 'buffer'):
                try:
                    if self.quantum_trainer.buffer is not None:
                        self.quantum_trainer.buffer.append(exp)
                        stored = True
                        logger.info(f"[{agent_name}] âœ… Stored to trainer.buffer ({len(self.quantum_trainer.buffer)})")
                        return True
                except Exception as e:
                    logger.error(f"[{agent_name}] âŒ trainer.buffer append failed: {e}")

            if not stored:
                logger.critical(f"[{agent_name}] âŒ FAILED TO STORE - NO FUNCTIONAL BUFFER!")
                self._verify_buffer_exists()
                if self.hybrid_buffer:
                    try:
                        self.hybrid_buffer.append(exp)
                        logger.critical(f"[{agent_name}] ğŸ†˜ Stored after emergency recovery")
                        return True
                    except:
                        pass

            return stored

        except Exception as e:
            logger.error(f"[{agent_name}] FAILED to store experience: {e}")
            import traceback
            traceback.print_exc()
            return False

    # -------------------------------
    # Training, save/load, metrics
    # -------------------------------
    def verify_experience_pipeline(self):
        """Verify that all components are properly connected"""
        issues = []
        warnings = []

        print("\n" + "="*80)
        print("EXPERIENCE PIPELINE VERIFICATION")
        print("="*80 + "\n")

        # Check exp_manager
        if not hasattr(self, 'exp_manager') or self.exp_manager is None:
            issues.append("exp_manager is missing")
        else:
            print("âœ… exp_manager exists")

        # Check store_experience_for_agent
        if not hasattr(self, 'store_experience_for_agent'):
            issues.append("store_experience_for_agent method missing")
        else:
            print("âœ… store_experience_for_agent exists")

        # Check batch_processor
        if hasattr(self, 'batch_processor') and self.batch_processor:
            if self.batch_processor.exp_manager is None:
                issues.append("batch_processor.exp_manager not connected")
            else:
                print("âœ… batch_processor.exp_manager connected")
        else:
            warnings.append("batch_processor not initialized")

        # Check hybrid buffer
        if hasattr(self, 'quantum_bridge') and self.quantum_bridge:
            if hasattr(self.quantum_bridge, 'quantum_system'):
                qs = self.quantum_bridge.quantum_system
                if hasattr(qs, 'hybrid_buffer') and qs.hybrid_buffer is not None:
                    print("âœ… hybrid_buffer connected")
                else:
                    issues.append("hybrid_buffer is None or missing")

        # Print results
        if warnings:
            print("\nâš ï¸  WARNINGS:")
            for w in warnings:
                print(f"   - {w}")

        if issues:
            print("\nâŒ CRITICAL ISSUES:")
            for i in issues:
                print(f"   - {i}")
        else:
            print("\nâœ… All components properly connected!")

        print("="*80 + "\n")

        return len(issues) == 0

    def train_all_agents(self):
        try:
            if self.quantum_trainer is None:
                return
            # ensure we have enough samples if trainer expects that
            buf_len = len(self.hybrid_buffer) if self.hybrid_buffer is not None else 0
            if hasattr(self.quantum_trainer, 'batch_size') and buf_len < getattr(self.quantum_trainer, 'batch_size', 1):
                return
            self.quantum_trainer.train_step()
        except Exception as e:
            logger.error(f"Quantum training error: {e}")
            import traceback
            traceback.print_exc()

    def save_quantum_state(self, path: str):
        if self.quantum_system is None:
            logger.warning("Quantum system not initialized; cannot save")
            return
        try:
            self.quantum_system.save_state(path)
        except Exception as e:
            logger.error(f"Failed to save quantum state: {e}")

    def load_quantum_state(self, path: str) -> bool:
        if self.quantum_system is None:
            logger.warning("Quantum system not initialized; cannot load")
            return False
        if os.path.exists(path):
            try:
                self.quantum_system.load_state(path)
                return True
            except Exception as e:
                logger.error(f"Failed to load quantum state: {e}")
                return False
        return False

    def get_system_metrics(self) -> Dict[str, Any]:
        """Get quantum system metrics with safe error handling."""
        if self.quantum_system is None:
            return {}
        try:
            metrics: Dict[str, Any] = {
                'buffer_size': len(self.hybrid_buffer) if self.hybrid_buffer else 0,
                'training_step': getattr(self.quantum_system, 'training_step', None),
                'device': str(self.device),
                'cache_shape': self.state_cache.shape if self.state_cache is not None else None,
                'agents_cached': len(self.agent_to_idx)
            }

            # entanglement / encoder metrics best-effort
            try:
                encoder = getattr(self.quantum_system, 'latent_encoder', None)
                if encoder is not None and hasattr(encoder, 'get_entanglement_metrics'):
                    metrics['entanglement'] = encoder.get_entanglement_metrics()
                else:
                    metrics['entanglement'] = {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'current': 0.0}
            except Exception as e:
                logger.debug(f"Failed to get entanglement metrics: {e}")
                metrics['entanglement'] = {'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'current': 0.0}

            return metrics
        except Exception as e:
            logger.error(f"Failed to get system metrics: {e}")
            import traceback
            traceback.print_exc()
            return {}



# ==============================================================================
# MIGRATION AND INTEGRATION FUNCTIONS
# ==============================================================================

import gc
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: from datetime import datetime

# ======================================================================
# MIGRATE_TO_QUANTUM_SYSTEM
# ======================================================================
def migrate_to_quantum_system(state_dim: int = 58,
                              action_dim: int = 2,
                              timeframe_lengths: Dict[str, int] = None,
                              device: str = None,
                              existing_agents: Optional[List] = None,
                              base_path: str = "/content/drive/MyDrive/RLTradingBot/Quantum",
                              **kwargs):
    """
    Convert classical multi-timeframe RL system to full Quantum system.
    Uses QuantumAgent, QuantumSystemBridge, and QuantumVotingSystem.
    """

    print("âš›ï¸ [Quantum Migration] Starting migration to pure Quantum system...")
    timeframe_lengths = timeframe_lengths or {
    
        # === High-Frequency Zone (Volatility Capture) ===
        'xs': 5,     # tick
        's': 10,     # ultra
        'm': 20,     # fast
    
        # === Critical Trading Zones ===
        'l': 30,     # scalp
        'xl': 60,    # 1min
        'xxl': 120,  # 2min
    
        # === Structure & Regime Detection ===
        '5m': 300,   # 5min
        '10m': 600,  # 10min
    }

    device = device or ("cuda" if torch.cuda.is_available() else "cpu")

    # Agent names based on timeframe keys
    agent_names = [f"quantum_agent_{tf}" for tf in timeframe_lengths.keys()]

    # === 1. Initialize bridge and system ===
    print("   â†’ Building QuantumSystemBridge...")
    quantum_bridge = QuantumSystemBridge(
        agent_names=agent_names,
        state_dim=state_dim,
        action_dim=action_dim,
        latent_dim=32,  # âœ… ADD THIS
        device=device
    )

    # === 2. Create Quantum Agents ===
    quantum_agents = []
    for name, tf_len in zip(agent_names, timeframe_lengths.values()):
        agent = QuantumAgent(
            name=name,
            seq_len=32,
            state_dim=state_dim,
            action_dim=action_dim,
            base_path=base_path,
            device=device,
            quantum_bridge=quantum_bridge
        )
        quantum_agents.append(agent)

    # === 3. Create Quantum Voting ===
    quantum_voting = QuantumVotingSystem(
        quantum_bridge=quantum_bridge,
        device=device
    )

    print(f"âœ… [Quantum Migration] Completed. Agents: {len(quantum_agents)}, Device: {device}")
    return quantum_agents, quantum_bridge, quantum_voting

# ==============================================================================
# QUANTUM MIGRATION INTEGRATION
# ==============================================================================

def migrate_system_to_quantum(existing_system, device: str = None):
    """
    Migrates an existing classical multi-timeframe system to a pure Quantum system.
    Returns: quantum_system_dict with keys: ['agents', 'bridge', 'voting']
    """
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    logger.info("âš›ï¸ [Quantum Migration] Starting migration to pure Quantum system...")

    # ---------------------------
    # Step 1: Build Quantum Bridge
    # ---------------------------
    agent_names = [agent.name for agent in existing_system.agents]
    quantum_bridge = QuantumSystemBridge(
        agent_names=agent_names,
        state_dim=existing_system.agents[0].state_dim,
        action_dim=existing_system.agents[0].action_dim,
        latent_dim=32,     # âœ… add this line
        device=device
    )

    logger.info(f"QuantumSystemBridge initialized with agents: {agent_names}")

    # ---------------------------
    # Step 2: Wrap existing agents as QuantumAgents
    # ---------------------------
    quantum_agents = []
    for agent in existing_system.agents:
        q_agent = QuantumAgent(
            name=agent.name,
            seq_len=agent.seq_len,
            state_dim=agent.state_dim,
            action_dim=agent.action_dim,
            device=device,
            quantum_bridge=quantum_bridge
        )
        quantum_agents.append(q_agent)
    logger.info(f"{len(quantum_agents)} QuantumAgents initialized")

    # ---------------------------
    # Step 3: Initialize Quantum Voting System
    # ---------------------------
    quantum_voting = QuantumVotingSystem(
        quantum_bridge=quantum_bridge,
        device=device
    )

    # ---------------------------
    # Step 4: Integrate states from existing classical system
    # ---------------------------
    for agent, q_agent in zip(existing_system.agents, quantum_agents):
        # Transfer feature history if available
        if hasattr(agent, "feature_history"):
            q_agent.feature_history = agent.feature_history.copy()
        if hasattr(agent, "latest_features"):
            q_agent.latest_features = agent.latest_features
        logger.debug(f"Transferred state for agent: {agent.name}")

    # ---------------------------
    # Step 5: Build quantum system dictionary
    # ---------------------------
    quantum_system = {
        "agents": quantum_agents,
        "bridge": quantum_bridge,
        "voting": quantum_voting
    }

    logger.info(f"âœ… [Quantum Migration] Completed. Agents: {len(quantum_agents)}, Device: {device}")
    return quantum_system

def integrate_quantum_with_system(system):
    """
    Migrate an existing TimeframeAgent-based system to the pure Quantum system.
    Maintains agents as a DICT for compatibility.
    """
    try:
        if not hasattr(system, "agents") or len(system.agents) == 0:
            logger.error("No agents found in system to integrate with Quantum")
            return

        # Handle both dict and list formats
        if isinstance(system.agents, dict):
            agent_list = list(system.agents.values())
            agent_names = list(system.agents.keys())
        else:
            agent_list = system.agents
            agent_names = [agent.name for agent in agent_list]

        # Determine state and action dimensions from first agent
        first_agent = agent_list[0]
        state_dim = getattr(first_agent, "state_dim", 58)
        action_dim = getattr(first_agent, "action_dim", 2)

        logger.info(f"Integrating {len(agent_names)} agents into QuantumSystemBridge...")

        # Create the Quantum bridge
        quantum_bridge = QuantumSystemBridge(
            agent_names=agent_names,
            state_dim=state_dim,
            action_dim=action_dim,
            latent_dim=32,
            device=None
        )

        # Wrap all agents with QuantumAgent interface - KEEP AS DICT
        quantum_agents_dict = {}  # Changed from list
        for old_agent in agent_list:
            q_agent = QuantumAgent(
                name=old_agent.name,
                seq_len=old_agent.seq_len,
                state_dim=state_dim,
                action_dim=action_dim,
                device=None,
                quantum_bridge=quantum_bridge
            )
            # Copy feature history from old agent if exists
            if hasattr(old_agent, "feature_history"):
                q_agent.feature_history = old_agent.feature_history.copy()

            quantum_agents_dict[old_agent.name] = q_agent  # Store as dict

        # Replace system agents with QuantumAgents AS DICT
        system.agents = quantum_agents_dict

        # Add a QuantumVotingSystem to the system
        quantum_voting = QuantumVotingSystem(quantum_bridge=quantum_bridge)
        system.quantum_bridge = quantum_bridge
        system.quantum_voting = quantum_voting

        logger.info(f"Quantum integration complete: {len(system.agents)} agents now Quantum-enabled.")

        return system, quantum_bridge, quantum_voting

    except Exception as e:
        logger.critical(f"Quantum integration failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None
    def quantum_voting_predict(q_vals, actions):
        return system.quantum_voting.predict(q_vals, actions)

    system._fast_voting_predict = quantum_voting_predict

    # Patch collect_voting_training_sample_supervised
    if hasattr(system, 'collect_voting_training_sample_supervised'):
        original_collect = system.collect_voting_training_sample_supervised

        def quantum_collect_sample(q_values_dict, actions_dict, final_action, reward, was_correct):
            try:
                original_collect(q_values_dict, actions_dict, final_action, reward, was_correct)
            except:
                pass

            system.quantum_voting.collect_training_sample(
                q_values_dict, actions_dict, final_action, reward, was_correct
            )

        system.collect_voting_training_sample_supervised = quantum_collect_sample

    logger.info("âœ“ Quantum system integrated with IntegratedSignalSystem")

def migrate_system_to_quantum(
    existing_system,
    agents_dict: Dict[str, Any] = None,  # Make optional
    gcs_bucket = None,
    gcs_meta_dir: str = "quantum_meta",
    device: str = None  # Add device parameter
):
    """Migrate with optional device specification"""

    # Use agents from system if not provided
    if agents_dict is None:
        agents_dict = existing_system.agents

    # Use device from system if not provided
    if device is None:
        device = getattr(existing_system, 'device', 'cuda' if torch.cuda.is_available() else 'cpu')

    # Rest of function...
    """
    MAIN MIGRATION FUNCTION - Drop-in replacement for existing system.

    Args:
        existing_system: Your existing IntegratedSignalSystem instance
        agents_dict: Your existing agents dictionary
        gcs_bucket: GCS bucket for state storage
        gcs_meta_dir: GCS directory for quantum state

    Returns:
        Modified system with quantum components
    """

    logger.info("="*80)
    logger.info("QUANTUM SYSTEM MIGRATION STARTED")
    logger.info("="*80)

    # Extract configuration from existing agents
    sample_agent = next(iter(agents_dict.values()))
    state_dim = getattr(sample_agent, 'state_dim', 58)
    action_dim = getattr(sample_agent, 'action_dim', 2)

    # Get timeframe lengths from agents
    timeframe_lengths = {
        name: agent.seq_len
        for name, agent in agents_dict.items()
    }

    # Migrate to quantum
    quantum_agents, quantum_bridge, quantum_voting = migrate_to_quantum_system(
        timeframe_lengths=timeframe_lengths,
        state_dim=state_dim,
        action_dim=action_dim,
        gcs_bucket=gcs_bucket,
        gcs_meta_dir=gcs_meta_dir
    )

    # Replace agents in system
    existing_system.agents = quantum_agents

    # Integrate quantum components
    integrate_quantum_with_system(existing_system)

    logger.info("="*80)
    logger.info("QUANTUM MIGRATION COMPLETE")
    logger.info("="*80)
    logger.info("System now using pure quantum predictions!")
    logger.info(f"  â€¢ {len(quantum_agents)} quantum agents")
    logger.info(f"  â€¢ Quantum voting system")
    logger.info(f"  â€¢ Entanglement-based coordination")
    logger.info(f"  â€¢ All helper functions unchanged")
    logger.info("="*80)

    return existing_system

# ==============================================================================
# MONITORING AND DIAGNOSTICS
# ==============================================================================

def start_quantum_monitoring(system, quantum_bridge, interval=60):
    """Monitor quantum-specific metrics"""
    def monitor():
        while True:
            time.sleep(interval)
            try:
                metrics = quantum_bridge.get_system_metrics()

                logger.info(
                    f"Quantum Metrics: "
                    f"Entanglement={metrics['entanglement']['mean']:.4f}, "
                    f"Buffer={metrics['buffer_size']}, "
                    f"Step={metrics['training_step']}"
                )
            except Exception as e:
                logger.error(f"Quantum monitoring error: {e}")

    threading.Thread(target=monitor, daemon=True).start()
    logger.info("Quantum monitoring started")

def validate_quantum_integration(system):
    """Validate quantum integration"""
    checks = {
        'quantum_bridge_exists': hasattr(system, 'quantum_bridge'),
        'quantum_voting_exists': hasattr(system, 'quantum_voting'),
        'quantum_agents': all(isinstance(agent, QuantumAgent) for agent in system.agents.values()),
        'voting_patched': hasattr(system, '_fast_voting_predict')
    }

    all_passed = all(checks.values())

    logger.info("=== QUANTUM INTEGRATION VALIDATION ===")
    for check, passed in checks.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        logger.info(f"{status} {check.replace('_', ' ').title()}")

    if all_passed:
        logger.info("ğŸ¯ ALL QUANTUM INTEGRATION CHECKS PASSED")
    else:
        logger.warning("âš ï¸ SOME QUANTUM INTEGRATION CHECKS FAILED")

    return all_passed

class IntegratedSignalSystem:
    def __init__(
        self,
        agents,
        state_dim,
        action_dim,
        google_drive_base_path="",
        ably_realtime=None,
        gcs_bucket=None,
        gcs_meta_dir="meta",
        device=None,
        buffer_size: int = 5000000
    ):
        self.lock = threading.Lock()
        self.partial_experiences = {}
        self.latest_features = {}
        self.agents = agents or {}
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device or ("cuda" if hasattr(torch, "cuda") and torch.cuda.is_available() else "cpu")

        # --- CRITICAL: Ensure quantum_bridge attribute exists early (prevents AttributeError) ---
        self.quantum_bridge = None
        self.quantum_agents = {}       # will be populated from quantum_bridge.quantum_system
        self.latent_encoder = None     # will be exposed from quantum_system if present

        # PROGRESSIVE GATING ADDITION - NEW
        self.training_threshold = 500000
        self.total_agent_training_steps = 0
        self.confidence_threshold = 0.00
        self.confidence_enabled = True
        self.meta_gating_enabled = False
        self.confirmation_required = False
        self.gating_status_logged = False

        # Track individual agent training steps - NEW
        self.agent_training_steps = {name: 0 for name in self.agents.keys()}
        self.last_valid_features = {}  # per-agent last valid features
        self.last_valid_timeframe_states = {}  # per-agent last valid timeframe tensors
        # Ably setup
        self.ably = ably_realtime
        if self.ably:
            self.confirmation_request_channel = self.ably.channels.get("confirmation-request")
            self.confirmation_response_channel = self.ably.channels.get("confirmation-response")
            logger.info(f"Confirmation channels initialized")
        else:
            logger.warning("No Ably client provided - confirmation system disabled")
            self.confirmation_request_channel = None
            self.confirmation_response_channel = None

        # Paths and storage
        self.base_path = google_drive_base_path if google_drive_base_path else "./saves"
        self.gcs_bucket = gcs_bucket
        self.gcs_meta_dir = gcs_meta_dir
        ensure_dir(self.base_path)

        # === CRITICAL FIXES INTEGRATION ===
        # Create task manager for safe asyncio operations
        self.safe_task_manager = SafeTaskManager()

        # Create Ably connection stabilizer
        if ably_realtime:
            self.ably_stabilizer = AblyConnectionStabilizer(ably_realtime)
        else:
            self.ably_stabilizer = None

        # Ensure all directories exist immediately
        emergency_create_directories()

        # Apply safe plotting wrappers
        self._plot_rewards = safe_plot_wrapper(self._plot_rewards)
        logger.critical("Critical fixes integrated into IntegratedSignalSystem")

        # File paths
        self.REWARD_HISTORY_PATH = os.path.join(self.base_path, "reward_history.txt")
        self.PLOT_SAVE_PATH = os.path.join(self.base_path, "reward_plot.png")
        self.META_MODEL_PATH = os.path.join(self.base_path, "meta_model.keras")
        self.VOTING_MODEL_PATH = os.path.join(self.base_path, "voting_model.pth")
        self.REWARD_PATH = os.path.join(self.base_path, "reward_history.pkl")
        self.META_DATA_PATH = os.path.join(self.base_path, "meta_data.pkl")

        # Email / notifier configuration
        rate_config = RateLimitConfig(
            per_chat_interval=1.5,
            global_bulk_per_second=20,
            max_retries=2
        )

        self.discord_sender = DiscordWebhookSender(
            webhook_url="https://discord.com/api/webhooks/1422481378649581448/Y29MiP207JzJAHz6WyeC_C8r55Q4nRF6ughn0F9sFKMV7CcbFN04mA_ODf7dHU13Rnl6",
            config=rate_config
        )
        self.discord_sender.start()
        self.discord_notifier = DiscordNotifier(self.discord_sender)
        logger.critical("Discord notifications initialized")

        # Models and data
        self.reward_history = []
        self.voted_signals_list = []
        self.meta_data = []
        self.training_enabled = True

        # Meta-model setup
        self.meta_model = build_meta_model(input_dim=30, seq_len=5, embed_dim=6, num_heads=2, ff_dim=128, num_blocks=2)
        self.meta_optimizer = self.meta_model.optimizer
        self.meta_loss_fn = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05)
        self.meta_model_trained = True
        self.meta_trainer = MetaModelTrainer()
        self.meta_model_buffer = MetaModelExperienceBuffer(max_size=5000)

        # Voting model setup (single init â€” removed duplicates)
        self.voting_model = VotingTransformer(num_agents=len(self.agents), features_per_agent=6).to(self.device)
        self.voting_optimizer = torch.optim.Adam(self.voting_model.parameters(), lr=1e-4)
        self.voting_training_data = []
        self.voting_lock = threading.Lock()

        # Validate device placement
        model_device = next(self.voting_model.parameters()).device
        logger.info(f"Voting model initialized on device: {model_device}")
        logger.info(f"System device: {self.device}")

        if str(model_device) != str(self.device):
            logger.warning(f"Device mismatch detected! Moving voting model to {self.device}")
            self.voting_model = self.voting_model.to(self.device)
            model_device = next(self.voting_model.parameters()).device
            logger.info(f"Voting model now on: {model_device}")

        # ... rest of initialization
        self.voting_sample_queue = deque(maxlen=500)  # Fast collection
        self.voting_training_batch = []  # For training
        self.voting_collection_lock = threading.Lock()  # Separate locks
        self.voting_training_lock = threading.Lock()
        self.last_voted_signal = None

        # Trading logic
        self.last_final_action = None
        self.last_final_price = None
        self.pullback_threshold = 0.003
        self.recent_prices = deque(maxlen=100)
        self.pullback_window = 5
        self.experience_replay = QuantumReplayBuffer(max_size=buffer_size)

        # === Latent encoder (required by QuantumSystemTrainer) ===
        self.latent_encoder = nn.Sequential(
            nn.Linear(STATE_DIM, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )

        self.quantum_trainer = QuantumSystemTrainer(system=self, buffer=self.experience_replay)
        # Meta-model thresholds
        self.dynamic_threshold = DynamicThreshold(window=50, min_threshold=0.05, max_threshold=0.5)
        self.uncertainty_threshold = 0.4

        # Channels and processing
        self.meta_features_channels = {}
        self.feature_channels = {}
        self.last_processing_time = 0

        # FIX: Initialize processing_lock immediately as None, but add a flag to track initialization

        self.processing_lock = asyncio.Lock()

        self.async_locks_initialized = False

        # Initialize asyncio event loop
        try:
            self.loop = asyncio.get_event_loop()
        except RuntimeError:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)

        # PROGRESSIVE LOADING
        self._load_training_progress()
        self._update_progressive_status()
        # In __init__, after device setup
        if torch.cuda.is_available():
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"GPU: {torch.cuda.get_device_name(0)} | Total Memory: {total_memory:.2f} GB")
            logger.info("Using aggressive memory cleanup (no CPU offloading)")

            self._start_aggressive_gpu_monitor()

        # Start background processes
        logger.info("Initializing IntegratedSignalSystem...")
        self._load_reward_history()
        self._load_meta_model()
        self._start_cleanup_thread()
        self._start_autosave_thread()
        self._start_voting_model_trainer(interval_sec=60)
        self._start_event_loop()

        # Initialize experience manager for batch processor (FIXED)
        self.exp_manager = QuantumExperienceCollector(max_age_seconds=3600)

        # âœ… ARCHITECTURAL FIX: Connect hybrid buffer to quantum system
        if hasattr(self, 'quantum_bridge') and self.quantum_bridge:
            if hasattr(self.quantum_bridge, 'quantum_system'):
                self.quantum_bridge.quantum_system.hybrid_buffer = self.experience_replay
                logger.info("âœ… Hybrid buffer connected to quantum system")
        logger.info("âœ… Experience manager initialized for batch processor")
        # Batch processor initialization
        self.batch_processor = None
        self.batch_processing_enabled = True
        self._init_feature_buffers()

        logger.info("Batch processing will be enabled after async initialization")

        # Start Ably listeners if available
        if self.ably:
            asyncio.run_coroutine_threadsafe(self._start_ably_listeners(), self.loop)

        self.last_health_check = time.time()
        self.health_check_interval = 30  # 5 minutes

        logger.info(f"Progressive system initialized: {self.total_agent_training_steps}/{self.training_threshold} steps")

        # ============================================================
        # QUANTUM BRIDGE INITIALIZATION (EARLY, CONSISTENT & SAFE)
        # ============================================================
        try:
            logger.info("Initializing QuantumSystemBridge...")
            agent_names = list(self.agents.keys())
            self.quantum_bridge = QuantumSystemBridge(
                agent_names=agent_names,
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                latent_dim=32,
                device=self.device
            )

            # Ensure quantum_system exists on bridge
            if hasattr(self.quantum_bridge, "quantum_system"):
                qs = self.quantum_bridge.quantum_system

                # Expose quantum_agents mapping to system level
                if hasattr(qs, "quantum_agents"):
                    self.quantum_agents = qs.quantum_agents
                    logger.critical("âœ“ quantum_agents exposed on system from quantum_system")
                else:
                    logger.warning("quantum_system exists but has no 'quantum_agents' attribute")

                # Expose latent_encoder at system level for trainer (nested path)
                if hasattr(qs, "latent_encoder"):
                    self.latent_encoder = qs.latent_encoder
                    logger.critical("âœ“ latent_encoder exposed on system from quantum_system")
                else:
                    logger.error("âŒ quantum_system missing 'latent_encoder' - trainer cannot be created until fixed")

                # Attach bridge reference to each agent (so agents can access bridge/system)
                for name, agent in self.agents.items():
                    try:
                        setattr(agent, "quantum_bridge", self.quantum_bridge)
                    except Exception:
                        logger.exception(f"Failed to attach quantum_bridge to agent {name}")

            else:
                logger.error("QuantumSystemBridge has no attribute 'quantum_system' - bridge incomplete")

        except Exception as e:
            logger.error(f"Failed to initialize QuantumSystemBridge: {e}")
            import traceback
            traceback.print_exc()
            self.quantum_bridge = None

        # --- Create trainer for differentiable quantum training (only if latent_encoder is valid) ---
        if not hasattr(self, "quantum_trainer") or self.quantum_trainer is None:
            try:
                if self.latent_encoder is None:
                    logger.error("âŒ system.latent_encoder not found - trainer will not be initialized")
                    self.quantum_trainer = None
                elif not isinstance(self.latent_encoder, nn.Module):
                    logger.error("âŒ system.latent_encoder is not an nn.Module - trainer cannot be initialized")
                    self.quantum_trainer = None
                else:
                    self.quantum_trainer = QuantumSystemTrainer(
                        system=self,
                        buffer=self.experience_replay,
                        batch_size=64,
                        gamma=0.92,
                        device=self.device
                    )
                    logger.critical("âœ“ QuantumSystemTrainer initialized successfully")

            except Exception as e:
                logger.error(f"âŒ Failed to initialize QuantumSystemTrainer: {e}")
                import traceback
                traceback.print_exc()
                self.quantum_trainer = None

    # ============================================================
    # FEATURE BUFFER RESTORATION PATCH
    # ============================================================

    def _init_feature_buffers(self):
        """Initialize per-agent price and feature buffers."""
        self.price_buffers = {name: deque(maxlen=100) for name in self.agents.keys()}
        self.latest_computed_features = {}
        self.features_lock = threading.Lock()
        logger.info("âœ… Initialized feature buffers for all agents")

    def calculate_signal_confidence(self, q_vals, actions, voting_logits=None):
        """
        Calculate signal confidence for trading bots.

        Args:
            q_vals: dict of agent_name -> [Q(BUY), Q(SELL)] (list or np.array)
            actions: dict of agent_name -> action integer (0=BUY, 1=SELL)
            voting_logits: torch.Tensor output from voting model, shape (1, 2)

        Returns:
            confidence (float in [0,1]), breakdown (dict)
        """
        try:
            import numpy as np
            import torch
            import torch.nn.functional as F
            from collections import Counter

            # --- Agent Agreement ---
            action_counts = Counter(actions.values())
            agreement_ratio = max(action_counts.values()) / len(actions) if actions else 0.0

            # --- Q-value Spread (decisiveness between actions) ---
            q_spreads = []
            q_margins = []

            for q in q_vals.values():
                q = np.array(q, dtype=np.float32)
                if len(q) < 2:
                    continue

                # Spread = abs(Q[BUY] - Q[SELL])
                q_spreads.append(abs(q[0] - q[1]))

                # Margin = best Q - second best Q
                q_sorted = np.sort(q)
                if len(q_sorted) >= 2:
                    q_margins.append(q_sorted[-1] - q_sorted[-2])

            avg_q_spread = float(np.mean(q_spreads)) if q_spreads else 0.0
            avg_q_margin = float(np.mean(q_margins)) if q_margins else 0.0

            # --- Voting Model Confidence ---
            if voting_logits is not None:
                try:
                    if isinstance(voting_logits, torch.Tensor):
                        voting_probs = F.softmax(voting_logits, dim=-1).cpu().numpy().flatten()
                    else:
                        voting_probs = np.array(voting_logits, dtype=np.float32).flatten()

                    voting_confidence = float(np.max(voting_probs))  # max([p_buy, p_sell])
                except Exception as e:
                    logger.debug(f"Voting confidence calculation error: {e}")
                    voting_confidence = 0.5
            else:
                voting_confidence = 0.5  # fallback if voting_logits not provided

            # --- Weighted Composite Confidence ---
            weights = {
                "agreement": 0.35,   # consensus
                "spread": 0.25,      # decisiveness
                "voting": 0.25,      # model output certainty
                "margin": 0.15       # clarity of choice
            }

            confidence = (
                weights["agreement"] * agreement_ratio +
                weights["spread"] * min(avg_q_spread, 1.0) +  # Cap at 1.0
                weights["voting"] * voting_confidence +
                weights["margin"] * min(avg_q_margin, 1.0)    # Cap at 1.0
            )

            # Clamp confidence to [0, 1]
            confidence = max(0.0, min(confidence, 1.0))

            breakdown = {
                "agent_agreement": float(agreement_ratio),
                "q_value_spread": float(avg_q_spread),
                "voting_confidence": float(voting_confidence),
                "q_margin": float(avg_q_margin),
                "weights": weights,
                "voting_logits_available": voting_logits is not None
            }

            return confidence, breakdown

        except Exception as e:
            logger.error(f"Confidence calculation failed: {e}")
            import traceback
            traceback.print_exc()

            # Return safe fallback
            return 0.5, {
                "agent_agreement": 0.5,
                "q_value_spread": 0.0,
                "voting_confidence": 0.5,
                "q_margin": 0.0,
                "voting_logits_available": False,
                "error": str(e)
            }

    # FIND THIS METHOD in IntegratedSignalSystem class (around line 2500-2600)
    # REPLACE the entire safe_quantum_predict method with this corrected version:

    def safe_quantum_predict(self, state, agent_name=None, return_confidence=False):
        """
        Safely run a quantum-enhanced prediction through the bridge.
        Handles tensor/array ambiguity and model fallback.
        """
        try:
            # --- Convert to torch tensor safely ---
            import torch, numpy as np
            if isinstance(state, np.ndarray):
                state = torch.tensor(state, dtype=torch.float32)
            elif not torch.is_tensor(state):
                state = torch.tensor(np.array(state, dtype=np.float32))

            # --- Validate shape ---
            if state is None or state.numel() == 0:
                raise ValueError("Empty or None state passed to safe_quantum_predict")

            device = getattr(self, "device", "cpu")
            state = state.to(device)

            # --- Model forward ---
            output = None
            if getattr(self, "quantum_model", None) is not None:
                output = self.quantum_model(state)
            elif hasattr(self, "predict") and callable(self.predict):
                output = self.predict(state)
            else:
                raise AttributeError("No quantum prediction function found in bridge")

            # --- Confidence (optional) ---
            if return_confidence:
                probs = torch.softmax(output, dim=-1)
                entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)
                conf = 1.0 - torch.mean(entropy).item()
                return output, conf

            return output

        except Exception as e:
            import torch
            logger.error(f"[{agent_name or 'Bridge'}] Quantum prediction failed: {e}")
            fallback = torch.zeros((1, getattr(self, "output_dim", 2)), dtype=torch.float32)
            if return_confidence:
                return fallback, 0.0
            return fallback

            return fallback

    # ============================================================
    # FEATURE BUFFER RESTORATION PATCH
    # ============================================================
    import numpy as np
    from collections import deque

    def _init_feature_buffers(self):
        """Initialize per-agent price and feature buffers."""
        self.price_buffers = {name: deque(maxlen=2000) for name in self.agents.keys()}
        self.latest_computed_features = {}
        self.features_lock = threading.Lock()
        logger.info("âœ… Initialized feature buffers for all agents")

    def process_raw_tick(self, agent_name: str, price_data: dict):
        """Process incoming raw tick data and compute rolling features."""
        try:
            if not isinstance(price_data, dict):
                logger.warning(f"[{agent_name}] Invalid price data format: {type(price_data)}")
                return

            close_price = float(price_data.get("close", 0.0))
            if close_price <= 0:
                return

            # Append to price buffer
            self.price_buffers[agent_name].append({
                "Close": close_price,
                "High": float(price_data.get("high", close_price)),
                "Low": float(price_data.get("low", close_price)),
                "Open": float(price_data.get("open", close_price)),
                "Volume": float(price_data.get("volume", 0))
            })

            buffer_size = len(self.price_buffers[agent_name])
            if buffer_size < 30:
                if buffer_size % 10 == 0:
                    logger.debug(f"[{agent_name}] Filling buffer: {buffer_size}/30")
                return

            # --- Compute simple rolling statistics ---
            closes = np.array([x["Close"] for x in self.price_buffers[agent_name]])
            highs = np.array([x["High"] for x in self.price_buffers[agent_name]])
            lows = np.array([x["Low"] for x in self.price_buffers[agent_name]])

            mean_close = np.mean(closes[-20:])
            std_close = np.std(closes[-20:])
            range_ratio = (highs[-1] - lows[-1]) / max(1e-6, mean_close)

            computed = {
                "ema20": float(mean_close),
                "volatility": float(std_close),
                "range_ratio": float(range_ratio),
                "price": float(close_price)
            }

            # Thread-safe update
            with self.features_lock:
                self.latest_computed_features[agent_name] = computed

            if buffer_size % 50 == 0:
                logger.info(f"[{agent_name}] Buffer OK ({buffer_size}) | Features: {list(computed.keys())}")

        except Exception as e:
            logger.error(f"[{agent_name}] process_raw_tick failed: {e}")

    def get_latest_state_features(self, agent_name: str = None):
        """Safely retrieve the latest computed state features."""
        with self.features_lock:
            if agent_name:
                features = self.latest_computed_features.get(agent_name, {})
                if not features:
                    logger.debug(f"[{agent_name}] No computed features yet (buffer={len(self.price_buffers[agent_name])})")
                return features

            # Combine averages from all agents
            if not self.latest_computed_features:
                logger.debug("No features computed for any agent yet")
                return {}

            combined = {}
            for feats in self.latest_computed_features.values():
                for k, v in feats.items():
                    combined[k] = combined.get(k, 0.0) + v
            for k in combined.keys():
                combined[k] /= len(self.latest_computed_features)
            return combined

    def init_async_locks(self):
        """Initialize async locks - must be called from within event loop"""
        if not self.async_locks_initialized:
            self.processing_lock = asyncio.Lock()
            self.async_locks_initialized = True
            logger.info("Async locks initialized")
        else:
            logger.info("Async locks already initialized")

    async def start_batch_processor(self):
        """Initialize and start the batch reward processor (FIXED)"""
        try:
            if not self.batch_processor and self.batch_processing_enabled:
                # Create batch processor with ALL required parameters (FIXED)
                self.batch_processor = RewardBatchProcessor(
                    system=self,                # System reference
                    state_dim=self.state_dim,   # State dimension
                    action_dim=self.action_dim, # Action dimension
                    batch_size=64,              # Optimized for trading system
                    flush_interval=0.15,
                    exp_manager=self.exp_manager  # âœ… Pass during init         # Quick response time
                )

                # Connect experience manager (FIXED)
                self.batch_processor.exp_manager = self.exp_manager
                logger.info("âœ… Experience manager connected to batch processor")

                # Start the processor
                await self.batch_processor.start()
                logger.info("ğŸš€ Batch reward processor initialized and started")
            else:
                logger.warning("Batch processor already initialized or disabled")
        except Exception as e:
            logger.error(f"Failed to start batch processor: {e}")
            import traceback
            traceback.print_exc()
    def store_experience_for_agent(self, signal_key: str, agent_name: str,
                                    states_dict: dict, action: int,
                                    q_values):#np.ndarray):
        """
        Store partial experience when signal is created.
        Bridges signal creation to exp_manager.

        Args:
            signal_key: Unique signal identifier (e.g., "xs_1761196759395107_1141")
            agent_name: Agent name (e.g., "xs")
            states_dict: Multi-timeframe states for this agent
            action: Action taken (0=BUY, 1=SELL)
            q_values: Q-values from the agent

        Returns:
            bool: True if stored successfully
        """
        try:
            # Use exp_manager if available
            if hasattr(self, 'exp_manager') and self.exp_manager is not None:
                success = self.exp_manager.store_signal_experience(
                    signal_key=signal_key,
                    agent_name=agent_name,
                    states_dict=states_dict,
                    action=action,
                    q_values=q_values
                )

                if success:
                    logger.debug(f"[{agent_name}] âœ… Stored experience: {signal_key}")
                else:
                    logger.warning(f"[{agent_name}] âš ï¸  Failed to store: {signal_key}")

                return success
            else:
                logger.error("âŒ exp_manager not available - cannot store experience")
                return False

        except Exception as e:
            logger.error(f"[{agent_name}] Error storing {signal_key}: {e}")
            return False

    # NEW PROGRESSIVE METHODS
    def _load_training_progress(self):
        """Load training progress from file"""
        progress_path = os.path.join(self.base_path, "training_progress.pkl")
        try:
            if os.path.exists(progress_path):
                with open(progress_path, "rb") as f:
                    progress = pickle.load(f)
                self.total_agent_training_steps = progress.get('total_steps', 0)
                self.agent_training_steps = progress.get('agent_steps', {name: 0 for name in self.agents.keys()})
                logger.info(f"Loaded training progress: {self.total_agent_training_steps} total steps")
            else:
                # Initialize from individual agent counters if they exist
                total = 0
                for name, agent in self.agents.items():
                    if hasattr(agent, 'train_step'):
                        self.agent_training_steps[name] = agent.train_step
                        total += agent.train_step
                self.total_agent_training_steps = total
                logger.info(f"Initialized training progress from agents: {total} total steps")
        except Exception as e:
            logger.warning(f"Failed to load training progress: {e}")
            self.total_agent_training_steps = 0
            self.agent_training_steps = {name: 0 for name in self.agents.keys()}

    def _save_training_progress(self):
        """Save training progress to file"""
        progress_path = os.path.join(self.base_path, "training_progress.pkl")
        try:
            ensure_dir(os.path.dirname(progress_path))
            progress = {
                'total_steps': self.total_agent_training_steps,
                'agent_steps': self.agent_training_steps,
                'meta_gating_enabled': self.meta_gating_enabled,
                'confirmation_required': self.confirmation_required
            }
            with open(progress_path, "wb") as f:
                pickle.dump(progress, f)
        except Exception as e:
            logger.warning(f"Failed to save training progress: {e}")

    def _update_progressive_status(self):
        """Update both meta-gating and confirmation status based on training progress"""
        previous_gating = self.meta_gating_enabled
        previous_confirmation = self.confirmation_required

        # Both features enable at the same threshold
        threshold_reached = self.total_agent_training_steps >= self.training_threshold
        self.meta_gating_enabled = threshold_reached
        self.confirmation_required = threshold_reached

        # Log status changes
        if self.meta_gating_enabled != previous_gating or self.confirmation_required != previous_confirmation:
            if threshold_reached:
                logger.info(f"ğŸ”’ FULL PROTECTION MODE ENABLED after {self.total_agent_training_steps} training steps")
                logger.info(f"   âœ“ Meta-model gating: ENABLED")
                logger.info(f"   âœ“ Confirmation required: ENABLED")
            else:
                logger.info(f"ğŸš€ FAST LEARNING MODE: No gating or confirmation ({self.total_agent_training_steps}/{self.training_threshold} steps)")
            self.gating_status_logged = True

    def increment_agent_training_step(self, agent_name):
        """Increment training step counter for an agent"""
        if agent_name in self.agent_training_steps:
            self.agent_training_steps[agent_name] += 1
            self.total_agent_training_steps += 1

            # Check for threshold crossing
            previous_total = self.total_agent_training_steps - 1
            if previous_total < self.training_threshold <= self.total_agent_training_steps:
                self._update_progressive_status()
                self._save_training_progress()
                # Send email notification about mode change
                self._send_mode_change_notification()

            # Periodic progress logging
            if self.total_agent_training_steps % 50 == 0:
                remaining = max(0, self.training_threshold - self.total_agent_training_steps)
                if remaining > 0:
                    logger.info(f"Training progress: {self.total_agent_training_steps}/{self.training_threshold} "
                               f"({remaining} steps until full protection mode)")

    def _send_mode_change_notification(self):
        try:
            mode = "Full Protection" if self.meta_gating_enabled else "Fast Learning"
            self.discord_notifier.notify_mode_change(mode, self.total_agent_training_steps, self.training_threshold)
        except Exception as e:
            logger.error(f"Mode notification failed: {e}")

    # HELPER METHODS FOR SPEED OPTIMIZATION
    def _extract_price_fast(self, snapshot):
        """Enhanced price extraction with fallback and logging"""
        if not snapshot:
            logger.debug("Empty snapshot in price extraction")
            return getattr(self, '_last_extracted_price', 0.0)

        # Try to extract from features
        for agent_name, feats in snapshot.items():
            if isinstance(feats, dict):
                # Try multiple price keys
                for price_key in ['price', 'close_scaled', 'close', 'last_price']:
                    if price_key in feats:
                        try:
                            price = float(feats[price_key])
                            if price > 0:
                                self._last_extracted_price = price
                                logger.debug(f"Price extracted from {agent_name}.{price_key}: {price}")
                                return price
                        except (ValueError, TypeError) as e:
                            logger.warning(f"Invalid price value in {price_key}: {feats[price_key]}")
                            continue

        # Fallback to last known price
        last_price = getattr(self, '_last_extracted_price', 0.0)
        if last_price > 0:
            logger.debug(f"Using last known price: {last_price}")
        else:
            logger.warning("âš ï¸ No price found in features and no fallback available")

        return last_price

    def _fast_voting_predict(self, q_vals, actions):
        """
        V5.0 ENHANCED: Voting prediction with CONFIDENCE-BASED TIE BREAKING.
        
        Key improvements:
        1. Tracks vote history for deadlock detection
        2. Uses Q-value spread (confidence) to break ties instead of BUY bias
        3. Injects random exploration after 10 consecutive ties
        4. Logs all vote distributions for debugging
        """
        import torch
        import torch.nn.functional as F
        from collections import Counter
        import traceback
        import random
        import time

        # Initialize vote history tracking
        if not hasattr(self, '_vote_history'):
            self._vote_history = []

        try:
            # --- Defensive: handle empty or None inputs ---
            if q_vals is None or actions is None or len(actions) == 0:
                logger.warning("Empty q_vals or actions for voting - using fallback")
                if actions:
                    action_counts = Counter(actions.values())
                    majority_action = action_counts.most_common(1)[0][0]
                    logger.info(f"Using agent majority vote fallback: {ACTION_MAP.get(majority_action, 'UNKNOWN')}")
                    return majority_action, None
                else:
                    logger.warning("No agent actions available - defaulting to HOLD")
                    return 2, None  # HOLD

            # --- Defensive: convert Tensor to dict if needed ---
            if isinstance(q_vals, torch.Tensor):
                q_vals = {
                    name: q_vals[i].detach().cpu().numpy()
                    for i, name in enumerate(self.agents.keys())
                }
            elif not isinstance(q_vals, dict):
                logger.warning("q_vals not a dict or tensor - using fallback")
                q_vals = {name: np.array([0.5, 0.5]) for name in self.agents}

            # --- Count votes and log distribution ---
            action_counts = Counter(actions.values())
            buy_count = action_counts.get(0, 0)
            sell_count = action_counts.get(1, 0)
            total = len(actions)
            
            buy_pct = buy_count / max(total, 1) * 100
            sell_pct = sell_count / max(total, 1) * 100
            print(f"[{time.strftime('%H:%M:%S')}] ğŸ—³ï¸ PRE-VOTE: BUY={buy_pct:.1f}%, SELL={sell_pct:.1f}% {dict(action_counts)}")
            
            # Track vote history for deadlock detection
            self._vote_history.append((buy_count, sell_count))
            if len(self._vote_history) > 20:
                self._vote_history.pop(0)

            # --- V5.0: CHECK FOR TIE AND USE CONFIDENCE-BASED TIE BREAKER ---
            if buy_count == sell_count and buy_count > 0:
                # Calculate confidence scores based on Q-value spread
                buy_confidence = 0.0
                sell_confidence = 0.0
                
                for agent_name, action in actions.items():
                    q = q_vals.get(agent_name, np.array([0.5, 0.5]))
                    if isinstance(q, np.ndarray) and len(q) >= 2:
                        spread = abs(float(q[0]) - float(q[1]))
                    else:
                        spread = 0.0
                    
                    if action == 0:  # BUY
                        buy_confidence += spread
                    else:  # SELL
                        sell_confidence += spread
                
                # Check for persistent deadlock (10+ ties in a row)
                recent_ties = sum(1 for b, s in self._vote_history[-10:] if b == s)
                if recent_ties >= 10:
                    print(f"[{time.strftime('%H:%M:%S')}] âš ï¸ DEADLOCK DETECTED ({recent_ties} ties)! Injecting exploration...")
                    result = random.randint(0, 1)
                    print(f"[{time.strftime('%H:%M:%S')}] ğŸ² EXPLORATION: {'BUY' if result == 0 else 'SELL'}")
                    return result, None
                
                # Normal tie-breaking by confidence
                if abs(buy_confidence - sell_confidence) > 0.01:
                    if buy_confidence > sell_confidence:
                        result = 0
                        print(f"[{time.strftime('%H:%M:%S')}] âš–ï¸ TIE-BREAK (confidence): BUY wins ({buy_confidence:.3f} > {sell_confidence:.3f})")
                    else:
                        result = 1
                        print(f"[{time.strftime('%H:%M:%S')}] âš–ï¸ TIE-BREAK (confidence): SELL wins ({sell_confidence:.3f} > {buy_confidence:.3f})")
                    return result, None
                else:
                    # True random if confidences also tied
                    result = random.randint(0, 1)
                    print(f"[{time.strftime('%H:%M:%S')}] ğŸ² TIE-BREAK (random): {'BUY' if result == 0 else 'SELL'}")
                    return result, None

            # --- No tie - use normal voting model ---
            valid_agents = sorted(set(q_vals.keys()) & set(actions.keys()))
            if not valid_agents:
                logger.warning("No valid agents for voting - using fallback")
                if actions:
                    majority_action = action_counts.most_common(1)[0][0]
                    logger.info(f"Using agent majority vote fallback: {ACTION_MAP.get(majority_action, 'UNKNOWN')}")
                    return majority_action, None
                else:
                    return 2, None  # HOLD

            # --- Ensure voting model on correct device ---
            device = next(self.voting_model.parameters()).device

            # --- Build input tensors ---
            q_values_tensor = torch.stack([
                torch.tensor(q_vals[a], dtype=torch.float32) for a in valid_agents
            ]).unsqueeze(0).to(device)

            agent_actions_tensor = torch.tensor([
                actions[a] for a in valid_agents
            ], dtype=torch.long).unsqueeze(0).to(device)

            # --- Encode agent outputs ---
            inputs = encode_agent_outputs(q_values_tensor, agent_actions_tensor)

            # --- Run voting model ---
            self.voting_model.eval()
            with torch.no_grad():
                logits = self.voting_model(inputs)
                prediction = torch.argmax(F.softmax(logits, dim=-1), dim=-1).item()

            print(f"[{time.strftime('%H:%M:%S')}] âœ… FINAL: {ACTION_MAP.get(prediction, 'UNKNOWN')} (from voting model)")
            return prediction, logits

        except Exception as e:
            logger.error(f"Voting model failed: {e}")
            traceback.print_exc()

            # --- Fallback: majority vote ---
            if actions:
                action_counts = Counter(actions.values())
                majority_action = action_counts.most_common(1)[0][0]
                logger.info(f"Using agent majority vote fallback: {ACTION_MAP.get(majority_action, 'UNKNOWN')}")
                return majority_action, None
            else:
                logger.warning("No agent actions available - defaulting to HOLD")
                return 2, None  # HOLD

    async def _fast_meta_check(self, agent_data, voting_pred):
        """Fast meta-model check using existing meta-model functionality"""
        try:
            meta_inputs = []
            for name, (feats, state, action, q) in agent_data.items():
                state_15 = np.pad(state[:15], (0, max(0, 15 - len(state[:15]))), 'constant')
                state_15 = np.nan_to_num(state_15, nan=0.0)

                onehot = np.eye(2)[action] if action in [0, 1] else np.zeros(2)
                q_stats = [np.max(q), np.min(q), np.std(q)]
                voting_onehot = np.eye(2)[voting_pred] if voting_pred in [0, 1] else np.zeros(2)

                sr_feats = [
                    feats.get('close_scaled', 0.0),
                    feats.get('distance_to_nearest_support_scaled', 0.0),
                    feats.get('distance_to_nearest_resistance_scaled', 0.0),
                    float(feats.get('near_support', False)),
                    float(feats.get('near_resistance', False)),
                    feats.get('distance_to_stop_loss_scaled', 0.0),
                    feats.get('support_strength_scaled', 0.0),
                    feats.get('resistance_strength_scaled', 0.0),
                ]

                meta_input = np.concatenate([
                    state_15, onehot, q_stats, voting_onehot, sr_feats
                ]).astype(np.float32)
                meta_input = np.nan_to_num(meta_input, nan=0.0)
                meta_inputs.append(meta_input)

            if meta_inputs:
                meta_np = enforce_meta_model_input_shape(meta_inputs, expected_dim=30)

                # Use existing is_signal_reliable function
                decision, mean, std = is_signal_reliable(self.meta_model, meta_np)

                if not np.isnan(mean) and not np.isnan(std):
                    confidence_threshold = self.dynamic_threshold.get_threshold()
                    return mean >= confidence_threshold and std <= self.uncertainty_threshold

            return True  # Allow if meta fails

        except Exception as e:
            logger.warning(f"Meta-model error - allowing signal: {e}")
            return True

    async def wait_for_confirmation(self, signal_keys, expected_signal, timeout=10.0, min_agree=1):
        """
        FIXED: Complete implementation that actually waits for confirmation responses
        """
        confirmations = []
        subscription_handler = None

        # Ensure signal_keys is a list
        if isinstance(signal_keys, str):
            signal_keys = [signal_keys]

        def handler(message):
            try:
                data = message.data
                logger.info(f"Confirmation received: {data}")

                if isinstance(data, dict):
                    confirmed_signal = data.get("confirmed_signal")
                    response_keys = data.get("signal_keys", [])

                    # Ensure response_keys is a list
                    if isinstance(response_keys, str):
                        response_keys = [response_keys]

                    # Check if signal matches and keys match
                    signal_matches = confirmed_signal == expected_signal
                    keys_match = any(key in response_keys for key in signal_keys) if signal_keys else True

                    if signal_matches and keys_match:
                        logger.info(f"Confirmation matched! Signal: {confirmed_signal}, Keys match: {keys_match}")
                        confirmations.append(data)
                    else:
                        logger.info(f"Confirmation mismatch - expected: {expected_signal}, got: {confirmed_signal}, keys_match: {keys_match}")
                        logger.info(f"   Expected keys: {signal_keys}, Response keys: {response_keys}")
            except Exception as e:
                logger.error(f"Error in confirmation handler: {e}")

        try:
            # Check if confirmation_response_channel exists
            if not self.confirmation_response_channel:
                logger.error("No confirmation_response_channel available!")
                return False

            # Subscribe to confirmation responses
            logger.info(f"Subscribing to confirmation responses for signal: {expected_signal} with keys: {signal_keys}")
            subscription_handler = handler
            await self.confirmation_response_channel.subscribe("confirm-response", handler)

            # Wait for confirmations with timeout
            start_time = time.time()
            logger.info(f"Waiting for confirmations (timeout: {timeout}s, need: {min_agree})")

            while len(confirmations) < min_agree and (time.time() - start_time) < timeout:
                await asyncio.sleep(0.1)  # Check every 100ms

            # Calculate results
            elapsed = time.time() - start_time
            confirmed = len(confirmations) >= min_agree

            # Log results
            if confirmed:
                logger.info(f"Confirmation successful! Received {len(confirmations)}/{min_agree} confirmations in {elapsed:.1f}s")
            else:
                logger.warning(f"Confirmation timeout! Only received {len(confirmations)}/{min_agree} confirmations in {elapsed:.1f}s")

            return confirmed

        except Exception as e:
            logger.error(f"Error in wait_for_confirmation: {e}")
            traceback.print_exc()
            return False
        finally:
            # Clean up subscription
            if subscription_handler and self.confirmation_response_channel:
                try:
                    await self.confirmation_response_channel.unsubscribe("confirm-response", subscription_handler)
                    logger.info("Unsubscribed from confirmation channel")
                except Exception as e:
                    logger.warning(f"Error unsubscribing from confirmation channel: {e}")
    async def _start_ably_listeners(self):
        """Enhanced Ably listener startup with timeout handling and retries"""

        if not self.ably:
            logger.error("No Ably client available")
            return

        # Check connection state first
        if hasattr(self.ably, 'connection'):
            connection_state = self.ably.connection.state
            if connection_state != 'connected':
                logger.warning(f"Ably not connected (state: {connection_state}), attempting reconnection...")

                # Try to reconnect with timeout
                try:
                    self.ably.connection.connect()

                    # Wait for connection with timeout
                    for attempt in range(20):  # 10 second timeout
                        await asyncio.sleep(0.5)
                        if self.ably.connection.state == 'connected':
                            break
                    else:
                        logger.error("Failed to establish Ably connection within timeout")
                        return

                except Exception as e:
                    logger.error(f"Connection attempt failed: {e}")
                    return

        logger.info(f"Starting Ably listeners (connection state: {self.ably.connection.state})")

        # Subscribe to channels with individual error handling and retries
        successful_subscriptions = 0
        failed_subscriptions = 0

        for agent_name in self.agents:
            try:
                agent_name_str = agent_name.decode('utf-8') if isinstance(agent_name, bytes) else str(agent_name)

                # Feature channel subscription with retry - subscribe to BOTH event types
                # V5.0 FIX: Publisher sends both "feature" and "integrated-features" events
                feature_success = await self._subscribe_with_retry(
                    agent_name_str,
                    "integrated-features",  # Primary event name from main loop
                    self.on_feature_message,
                    max_retries=3,
                    timeout=10
                )
                
                # Also subscribe to "feature" event for backward compatibility
                feature_success_alt = await self._subscribe_with_retry(
                    agent_name_str,
                    "feature",  # Alternative event name from _publish_with_retry
                    self.on_feature_message,
                    max_retries=3,
                    timeout=10
                )

                if feature_success or feature_success_alt:
                    successful_subscriptions += 1
                    logger.info(f"âœ“ [{agent_name_str}] Feature channel subscribed (integrated-features={feature_success}, feature={feature_success_alt})")
                else:
                    failed_subscriptions += 1
                    logger.error(f"âœ— [{agent_name_str}] Feature channel subscription failed")

                # Meta features channel subscription with retry
                meta_success = await self._subscribe_with_retry(
                    agent_name_str,
                    "meta_features",
                    lambda msg, name=agent_name_str: self._handle_meta_features_message(name, msg),
                    max_retries=3,
                    timeout=10,
                    channel_suffix="meta_features-"
                )

                if meta_success:
                    logger.info(f"âœ“ [{agent_name_str}] Meta features channel subscribed")
                else:
                    logger.warning(f"âš  [{agent_name_str}] Meta features subscription failed")

                # Small delay between subscriptions to avoid rate limiting
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"[{agent_name_str}] Overall subscription failed: {e}")
                failed_subscriptions += 1
                continue

        logger.info(f"Channel subscriptions complete: {successful_subscriptions} successful, {failed_subscriptions} failed")

        # Subscribe to reward channels with retry
        await self._subscribe_reward_channels()

        # Subscribe to confirmation channels with retry
        await self._subscribe_confirmation_channels()

        # Subscribe to shutdown channel
        await self._subscribe_shutdown_channel()

        # Report final status
        if successful_subscriptions == 0:
            logger.error("âŒ NO SUCCESSFUL CHANNEL SUBSCRIPTIONS - Signals will not work!")
        elif failed_subscriptions > 0:
            logger.warning(f"âš  PARTIAL SUBSCRIPTION SUCCESS - {failed_subscriptions} channels failed")
        else:
            logger.info("âœ… ALL CHANNELS SUBSCRIBED SUCCESSFULLY")

    async def _subscribe_with_retry(self, agent_name, event_name, callback, max_retries=3, timeout=10, channel_suffix=""):
        """Subscribe to a channel with retry logic and timeout handling"""

        channel_name = f"{channel_suffix}{agent_name}" if channel_suffix else agent_name

        for attempt in range(max_retries):
            try:
                logger.debug(f"Attempting to subscribe to {channel_name} (attempt {attempt + 1}/{max_retries})")

                # Get channel
                channel = self.ably.channels.get(channel_name)

                # Attach with timeout
                attach_task = asyncio.create_task(channel.attach())
                try:
                    await asyncio.wait_for(attach_task, timeout=timeout)
                except asyncio.TimeoutError:
                    logger.warning(f"Channel attach timeout for {channel_name} (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        continue
                    else:
                        return False

                # Subscribe with timeout
                subscribe_task = asyncio.create_task(channel.subscribe(event_name, callback))
                try:
                    await asyncio.wait_for(subscribe_task, timeout=timeout)
                    return True

                except asyncio.TimeoutError:
                    logger.warning(f"Channel subscribe timeout for {channel_name} (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)
                        continue
                    else:
                        return False

            except Exception as e:
                logger.warning(f"Subscription attempt {attempt + 1} failed for {channel_name}: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return False

        return False

    async def _subscribe_reward_channels(self):
        """Subscribe to reward channels with error handling"""
        try:
            # Individual rewards
            reward_success = await self._subscribe_with_retry(
                "rewards",
                "new-reward",
                self._batch_reward_callback,
                max_retries=2,
                timeout=8
            )

            if reward_success:
                logger.info("âœ“ Individual reward channel subscribed")
            else:
                logger.error("âœ— Individual reward channel subscription failed")

            # Batched rewards
            batch_success = await self._subscribe_with_retry(
                "reward-batches",
                "reward-batch",
                self._batched_reward_callback,
                max_retries=2,
                timeout=8
            )

            if batch_success:
                logger.info("âœ“ Batched reward channel subscribed")
            else:
                logger.error("âœ— Batched reward channel subscription failed")

        except Exception as e:
            logger.error(f"Reward channel subscription failed: {e}")

    async def _subscribe_confirmation_channels(self):
        """Subscribe to confirmation channels with error handling"""
        try:
            if self.confirmation_response_channel:
                # Use direct channel subscription instead of retry wrapper for confirmation
                await self.confirmation_response_channel.attach()

                async def debug_confirmation_callback(message):
                    logger.info(f"DEBUG: Confirmation response received: {message.data}")

                await self.confirmation_response_channel.subscribe("confirm-response", debug_confirmation_callback)
                logger.info("âœ“ Confirmation response channel subscribed")
            else:
                logger.warning("âš  No confirmation response channel available")

        except Exception as e:
            logger.error(f"Confirmation channel subscription failed: {e}")

    async def _subscribe_shutdown_channel(self):
        """Subscribe to shutdown channel with error handling"""
        try:
            shutdown_success = await self._subscribe_with_retry(
                "shutdown",
                "shutdown",
                self._shutdown_callback,
                max_retries=1,
                timeout=5
            )

            if shutdown_success:
                logger.info("âœ“ Shutdown channel subscribed")
            else:
                logger.warning("âš  Shutdown channel subscription failed")

        except Exception as e:
            logger.error(f"Shutdown channel subscription failed: {e}")

    async def _batch_reward_callback(self, message):
        """Enhanced batch reward handler with better error handling"""

        try:
            data = message.data
            if not isinstance(data, dict):
                logger.warning("Reward message is not a dict.")
                return

            signal_key = data.get("signal_key")
            reward = data.get("reward")
            agent_multipliers = data.get("agent_multipliers", {})

            if not signal_key:
                logger.warning("Missing 'signal_key' in reward message.")
                return
            if not isinstance(reward, (int, float)):
                logger.warning(f"Invalid reward type: {type(reward)}")
                return

            logger.info(f"Reward received for batch processing: {signal_key} -> {reward}")

            if self.batch_processor:
                await self.batch_processor.add_reward(signal_key, reward, agent_multipliers)
            else:
                logger.warning("Batch processor not initialized - reward dropped")

        except Exception as e:
            logger.error(f"Batch reward callback error: {e}")

    async def _batch_reward_callback(self, message):
        """
        Enhanced reward handler with exit_price for supervised learning.
        Supports single reward messages and queues them safely to RewardBatchProcessor.
        """
        try:
            data = message.data
            if not isinstance(data, dict):
                logger.warning("[RewardCallback] Reward message is not a dict.")
                return

            signal_key = data.get("signal_key")
            reward = data.get("reward")
            agent_multipliers = data.get("agent_multipliers", {})
            entry_price = data.get("entry_price")
            exit_price = data.get("exit_price")  # Critical for supervised learning

            if not signal_key:
                logger.warning("[RewardCallback] Missing 'signal_key'.")
                return
            if not isinstance(reward, (int, float)):
                logger.warning(f"[RewardCallback] Invalid reward type: {type(reward)}")
                return

            logger.info(f"[RewardCallback] Reward received: {signal_key} -> {reward} "
                        f"(entry: {entry_price}, exit: {exit_price})")

            if self.batch_processor:
                # Queue reward as a dict for batch processing
                await self.batch_processor.queue_reward({
                    "signal_key": signal_key,
                    "reward": reward,
                    "agent_multipliers": agent_multipliers,
                    "entry_price": entry_price,
                    "exit_price": exit_price
                })
            else:
                logger.warning("[RewardCallback] Batch processor not initialized - reward dropped")

        except Exception as e:
            logger.error(f"[RewardCallback] Error processing reward: {e}")

    async def _batched_reward_callback(self, message):
        """
        Enhanced batched reward handler supporting exit_price.
        Processes multiple rewards in one batch and queues them to RewardBatchProcessor.
        """
        try:
            data = message.data
            if not isinstance(data, dict):
                logger.warning("[BatchedRewardCallback] Message is not a dict.")
                return

            rewards = data.get("rewardz", [])  # Matches batch format
            batch_id = data.get("batch_id", f"batch_{int(time.time())}")

            if not rewards:
                logger.warning(f"[BatchedRewardCallback] Empty rewards batch received: {batch_id}")
                return

            logger.info(f"[BatchedRewardCallback] Processing batch {batch_id} with {len(rewards)} rewards")

            processed = 0
            failed = 0
            with_exit_price = 0

            for i, r in enumerate(rewards):
                try:
                    signal_key = r.get("signal_key")
                    reward = r.get("reward")
                    agent_multipliers = r.get("agent_multipliers", {})
                    entry_price = r.get("entry_price")
                    exit_price = r.get("exit_price")

                    if not signal_key:
                        logger.warning(f"[Batch {batch_id}[{i}]] Missing signal_key")
                        failed += 1
                        continue
                    if not isinstance(reward, (int, float)):
                        logger.warning(f"[Batch {batch_id}[{i}]] Invalid reward type")
                        failed += 1
                        continue

                    if exit_price is not None:
                        with_exit_price += 1

                    if self.batch_processor:
                        await self.batch_processor.queue_reward({
                            "signal_key": signal_key,
                            "reward": reward,
                            "agent_multipliers": agent_multipliers,
                            "entry_price": entry_price,
                            "exit_price": exit_price
                        })
                        processed += 1
                    else:
                        logger.error(f"[Batch {batch_id}[{i}]] Batch processor not available")
                        failed += 1

                except Exception as e:
                    logger.error(f"[Batch {batch_id}[{i}]] Processing failed: {e}")
                    failed += 1

            logger.info(f"[BatchedRewardCallback] Batch {batch_id} completed: "
                        f"{processed} processed, {with_exit_price} with exit_price, {failed} failed")

        except Exception as e:
            logger.error(f"[BatchedRewardCallback] Error processing batch: {e}")

    def collect_voting_training_sample_supervised(self, q_values_dict, actions_dict,
                                                 final_action, reward, was_correct):
        """
        FIXED: Unified collection that properly stores samples for batch training
        """
        # Quick validation BEFORE acquiring lock
        if final_action not in [0, 1]:
            logger.debug(f"[VOTING] Skipping invalid final_action={final_action}")
            return

        invalid_actions = [a for a in actions_dict.values() if a not in [0, 1, 2]]
        if invalid_actions:
            logger.debug(f"[VOTING] Skipping due to invalid agent actions")
            return

        with self.voting_collection_lock:
            try:
                all_agent_names = sorted(self.agents.keys())
                max_agents = len(all_agent_names)

                q_values_padded = torch.zeros(max_agents, 2, dtype=torch.float32)
                actions_padded = torch.zeros(max_agents, dtype=torch.long)
                agent_mask = torch.zeros(max_agents, dtype=torch.bool)

                valid_agents = sorted(set(q_values_dict.keys()) & set(actions_dict.keys()))

                if not valid_agents:
                    logger.debug("[VOTING] No valid agents in sample")
                    return

                for i, agent_name in enumerate(all_agent_names):

                    if agent_name in valid_agents:
                        q_val = q_values_dict[agent_name]

                        # Validate Q-values
                        if not isinstance(q_val, (np.ndarray, list, torch.Tensor)):
                            continue

                        q_val_array = np.array(q_val, dtype=np.float32)
                        if len(q_val_array) != 2:
                            continue

                        if np.any(np.isnan(q_val_array)) or np.any(np.isinf(q_val_array)):
                            continue

                        q_values_padded[i] = torch.tensor(q_val_array, dtype=torch.float32)
                        actions_padded[i] = torch.tensor(actions_dict[agent_name], dtype=torch.long)
                        agent_mask[i] = True

                # Verify we have valid data
                if not agent_mask.any():
                    logger.debug("[VOTING] No valid agent data after processing")
                    return

                final_action_tensor = torch.tensor(final_action, dtype=torch.long)
                reward_tensor = torch.tensor(reward, dtype=torch.float32)

                # CRITICAL FIX: Append to queue
                self.voting_sample_queue.append((
                    q_values_padded,
                    actions_padded,
                    final_action_tensor,
                    reward_tensor,
                    agent_mask
                ))

                queue_size = len(self.voting_sample_queue)

                # Log progress
                if queue_size % 10 == 0:
                    logger.info(f"[VOTING QUEUE] Size: {queue_size}/500, "
                                  f"Valid agents: {agent_mask.sum().item()}/{max_agents}")

                # CRITICAL: Trigger training when threshold reached
                if queue_size >= 64:
                    logger.critical(f"[VOTING QUEUE] THRESHOLD REACHED ({queue_size}/64) - Triggering training!")
                    asyncio.create_task(self._trigger_voting_training())

            except Exception as e:
                logger.error(f"[VOTING] Collection error: {e}")
                traceback.print_exc()

    async def _trigger_voting_training(self):
        """Async trigger for voting training from queue"""
        try:

            # Transfer samples from queue
            with self.voting_collection_lock:  # â† FIXED: Changed from self.system.voting_collection_lock
                queue_size = len(self.voting_sample_queue)  # â† FIXED
                if queue_size < 3:

                    return

                # Take all samples from queue
                samples_to_train = list(self.voting_sample_queue)  # â† FIXED
                self.voting_sample_queue.clear()  # â† FIXED
                logger.critical(f"[VOTING TRIGGER] Transferred {len(samples_to_train)} samples from queue")

            # Call training

            self._train_voting_model_batch(samples_to_train)

        except Exception as e:
            logger.critical(f"[VOTING TRIGGER] Error: {e}")
            traceback.print_exc()

    def _train_voting_model_batch(self, samples):
        """Training with proper device handling"""

        logger.info(f"[VOTING] _train_voting_model_batch ENTRY with {len(samples)} samples")

        if not samples or len(samples) < 3:
            logger.info(f"[VOTING] EARLY EXIT: Not enough samples")
            return

        # CRITICAL: Ensure model on correct device BEFORE training
        model_device = next(self.voting_model.parameters()).device
        logger.infol(f"[VOTING] Model device: {model_device}, System device: {self.device}")

        if str(model_device) != str(self.device):
            logger.info(f"[VOTING] WARNING: Moving model to {self.device}")
            self.voting_model = self.voting_model.to(self.device)
            model_device = self.device

        with self.voting_training_lock:
            try:
                logger.info(f"[VOTING] Acquired training lock")

                # Unpack and validate samples
                q_values_batch = []
                actions_batch = []
                final_actions_batch = []
                rewards_batch = []
                masks_batch = []

                for idx, sample_tuple in enumerate(samples):
                    if len(sample_tuple) != 5:
                        continue

                    q_vals, acts, final_act, rew, mask = sample_tuple

                    if not isinstance(final_act, torch.Tensor):
                        final_act = torch.tensor(final_act, dtype=torch.long)

                    if final_act.item() not in [0, 1]:
                        continue

                    q_values_batch.append(q_vals)
                    actions_batch.append(acts)
                    final_actions_batch.append(final_act)
                    rewards_batch.append(rew)
                    masks_batch.append(mask)

                logger.info(f"[VOTING] Valid samples: {len(q_values_batch)}")

                if not q_values_batch:
                    logger.info("[VOTING] No valid samples")
                    return

                # Stack tensors and MOVE TO CORRECT DEVICE
                q_values_tensor = torch.stack(q_values_batch).to(model_device)
                actions_tensor = torch.stack(actions_batch).to(model_device)
                final_actions_tensor = torch.stack(final_actions_batch).to(model_device)
                masks_tensor = torch.stack(masks_batch).to(model_device)

                logger.info(f"[VOTING] Tensors on device: {q_values_tensor.device}")

                # Class distribution
                class_counts = torch.bincount(final_actions_tensor, minlength=2).float()
                logger.info(f"[VOTING] Class distribution - BUY: {class_counts[0]:.0f}, SELL: {class_counts[1]:.0f}")

                if class_counts.min() == 0:
                    logger.info("[VOTING] Only one class present!")
                    return

                # Calculate class weights on same device
                class_weights = 1.0 / (class_counts + 1e-6)
                class_weights = class_weights / class_weights.sum()
                class_weights = class_weights.to(model_device)

                # Train/val split
                n_samples = len(q_values_batch)
                n_train = max(3, int(0.8 * n_samples))

                indices = torch.randperm(n_samples)
                train_idx = indices[:n_train]

                train_q = q_values_tensor[train_idx]
                train_a = actions_tensor[train_idx]
                train_fa = final_actions_tensor[train_idx]
                train_m = masks_tensor[train_idx]

                dataset = torch.utils.data.TensorDataset(train_q, train_a, train_fa, train_m)
                dataloader = DataLoader(dataset, batch_size=min(32, n_train), shuffle=True)

                logger.info(f"[VOTING] Starting 3 epochs of training...")

                self.voting_model.train()

                for epoch in range(3):
                    epoch_loss = 0.0
                    epoch_correct = 0
                    epoch_samples = 0

                    for batch_q, batch_a, batch_fa, batch_m in dataloader:
                        # All tensors already on correct device from dataset
                        inputs = encode_agent_outputs(batch_q, batch_a)
                        logits = self.voting_model(inputs, mask=batch_m)
                        loss = F.cross_entropy(logits, batch_fa, weight=class_weights)

                        self.voting_optimizer.zero_grad()
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.voting_model.parameters(), 1.0)
                        self.voting_optimizer.step()

                        epoch_loss += loss.item()
                        predictions = logits.argmax(dim=-1)
                        epoch_correct += (predictions == batch_fa).sum().item()
                        epoch_samples += len(batch_fa)

                    epoch_acc = epoch_correct / epoch_samples if epoch_samples > 0 else 0
                    logger.critical(f"[VOTING] Epoch {epoch+1}/3: Loss={epoch_loss/len(dataloader):.4f}, Acc={epoch_acc:.1%}")

                logger.info("[VOTING] TRAINING COMPLETE")

            except Exception as e:
                logger.info(f"[VOTING] ERROR: {e}")
                traceback.print_exc()

    async def _shutdown_callback(self, message):
        """Shutdown callback"""
        try:
            logger.info("Shutdown signal received from Ably!")
            if hasattr(self, "stop") and callable(getattr(self, "stop")):
                asyncio.create_task(self._handle_shutdown())
            else:
                logger.warning("Shutdown handler not implemented.")
        except Exception as e:
            logger.error(f"Shutdown callback error: {e}")
        async def _handle_shutdown(self):
            """Handle shutdown signal properly"""
            try:
                logger.info("Processing shutdown signal...")
                # Give a brief moment for any pending operations
                await asyncio.sleep(1)
                self.stop()
            except Exception as e:
                logger.error(f"Shutdown handling error: {e}")

    def plot_buy_sell_ratios_and_email(self, voted_signals, window_size=100):
        self.track_signal_ratio(voted_signals)

    async def periodic_processor(self):
        """Periodic background processing tasks"""
        while True:
            try:
                await asyncio.sleep(60)  # Run every minute

                # Add any periodic tasks here
                # Example:
                # - Check system health
                # - Clean up old data
                # - Update statistics

                logger.debug("Periodic processor tick")

            except Exception as e:
                logger.error(f"Periodic processor error: {e}")
                await asyncio.sleep(5)

    def periodic_ably_cleanup(self):
        """Periodically clean up completed Ably tasks"""
        try:
            if hasattr(self, 'ably_tasks'):
                # Remove completed tasks
                completed_tasks = [task for task in self.ably_tasks if task.done()]
                for task in completed_tasks:
                    self.ably_tasks.discard(task)

                if completed_tasks:
                    logger.debug(f"Cleaned up {len(completed_tasks)} completed Ably tasks")

        except Exception as e:
            logger.error(f"Ably task cleanup error: {e}")

    def _start_aggressive_gpu_monitor(self):
        """Aggressive GPU memory monitoring with auto-cleanup"""
        def monitor():
            cleanup_threshold = 0.85  # Cleanup when 85% full
            critical_threshold = 0.90  # Force cleanup at 90%

            while True:
                time.sleep(15)  # Check every 15 seconds
                try:
                    if not torch.cuda.is_available():
                        continue

                    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
                    allocated_mem = torch.cuda.memory_allocated(0) / 1e9
                    reserved_mem = torch.cuda.memory_reserved(0) / 1e9
                    free_mem = total_mem - allocated_mem
                    usage_pct = (allocated_mem / total_mem) * 100

                    # Log if usage is high
                    if usage_pct > 70:
                        logger.warning(f"GPU Memory: {allocated_mem:.2f}GB/{total_mem:.2f}GB ({usage_pct:.0f}%) | "
                                     f"Reserved: {reserved_mem:.2f}GB | Free: {free_mem:.2f}GB")

                    # Auto cleanup at threshold
                    if usage_pct > cleanup_threshold * 100:
                        logger.warning(f"GPU memory high ({usage_pct:.0f}%), forcing cleanup...")
                        torch.cuda.empty_cache()

                        # Log after cleanup
                        new_allocated = torch.cuda.memory_allocated(0) / 1e9
                        freed = allocated_mem - new_allocated
                        logger.info(f"Freed {freed:.2f}GB via cache cleanup")

                    # Critical cleanup
                    if usage_pct > critical_threshold * 100:
                        logger.critical(f"CRITICAL GPU memory ({usage_pct:.0f}%)!")
                        torch.cuda.empty_cache()

                        # Reduce batch sizes globally
                        for agent in self.agents.values():
                            if hasattr(agent, 'batch_size') and agent.batch_size > 16:
                                old_batch = agent.batch_size
                                agent.batch_size = max(16, agent.batch_size // 2)
                                logger.warning(f"[{agent.name}] Emergency batch reduction: {old_batch} -> {agent.batch_size}")

                except Exception as e:
                    logger.debug(f"GPU monitor error: {e}")

        threading.Thread(target=monitor, daemon=True).start()
        logger.critical("Aggressive GPU memory monitor started")

    def diagnose_training_pipeline(self):
        """
        COMPREHENSIVE: Find where experiences are actually going
        ENHANCED: Now includes automatic buffer recovery
        """
        logger.critical("="*80)
        logger.critical("QUANTUM TRAINING PIPELINE DIAGNOSTICS (ENHANCED + AUTO-RECOVERY)")
        logger.critical("="*80)

        # === Check 1: Quantum Bridge ===
        if hasattr(self, 'quantum_bridge') and self.quantum_bridge:
            logger.critical("âœ… 1. Quantum Bridge EXISTS")

            # Check for store method
            if hasattr(self.quantum_bridge, 'store_experience_for_agent'):
                logger.critical("   âœ… store_experience_for_agent method EXISTS")
            else:
                logger.critical("   âŒ store_experience_for_agent method MISSING")

            # === Check 2: Hybrid Buffer (WITH AUTO-RECOVERY) ===
            buffer_recovered = False
            if hasattr(self.quantum_bridge, 'hybrid_buffer'):
                if self.quantum_bridge.hybrid_buffer is None:
                    logger.critical("   âŒ Hybrid Buffer: is None - ATTEMPTING RECOVERY...")
                    # Auto-recover the buffer
                    if hasattr(self.quantum_bridge, '_verify_buffer_exists'):
                        self.quantum_bridge._verify_buffer_exists()
                        buffer_recovered = True

                if self.quantum_bridge.hybrid_buffer:
                    buffer_size = len(self.quantum_bridge.hybrid_buffer)
                    buffer_type = type(self.quantum_bridge.hybrid_buffer).__name__
                    status = "âœ… (RECOVERED)" if buffer_recovered else "âœ…"
                    logger.critical(f"   {status} Hybrid Buffer: {buffer_size} experiences | Type: {buffer_type}")
                else:
                    logger.critical("   âŒ Hybrid Buffer: STILL None after recovery attempt!")
            else:
                logger.critical("   âŒ Hybrid Buffer: attribute doesn't exist")

            # === Check 3: Quantum Trainer ===
            if hasattr(self.quantum_bridge, 'quantum_trainer'):
                if self.quantum_bridge.quantum_trainer:
                    logger.critical("   âœ… Quantum Trainer EXISTS")
                    trainer = self.quantum_bridge.quantum_trainer

                    # Check trainer buffer and link
                    # V8.5.8 FIX: Handle @property buffer correctly
                    buffer_accessible = False
                    trainer_buffer = None

                    # Try to access buffer (works even if it's a @property)
                    try:
                        trainer_buffer = trainer.buffer
                        buffer_accessible = True
                    except AttributeError:
                        buffer_accessible = False
                    except Exception as e:
                        logger.warning(f"      âš ï¸  Unexpected error accessing buffer: {e}")
                        buffer_accessible = False

                    if buffer_accessible:
                        if trainer_buffer is not None:
                            buffer_size = len(trainer_buffer)
                            is_linked = trainer_buffer is self.quantum_bridge.hybrid_buffer
                            link_status = "âœ… LINKED" if is_linked else "âš ï¸  NOT LINKED"
                            logger.critical(f"      âœ… Trainer Buffer: {buffer_size} experiences | {link_status}")
                            logger.critical(f"      Can train: {buffer_size >= 64}")

                            # Auto-link if not linked
                            if not is_linked and self.quantum_bridge.hybrid_buffer:
                                logger.critical("      ğŸ”§ AUTO-LINKING trainer buffer to hybrid_buffer...")
                                try:
                                    trainer.buffer = self.quantum_bridge.hybrid_buffer
                                    logger.critical("      âœ… Buffers now linked")
                                except Exception as e:
                                    logger.error(f"      âŒ Auto-link failed: {e}")
                                    # Try setting on base_trainer if it's a wrapper
                                    if hasattr(trainer, 'base_trainer'):
                                        trainer.base_trainer.buffer = self.quantum_bridge.hybrid_buffer
                                        logger.critical("      âœ… Buffers linked via base_trainer")
                        else:
                            logger.critical("      âŒ Trainer Buffer: is None")
                            # Try to set it
                            logger.critical("      ğŸ”§ Attempting to set buffer...")
                            try:
                                trainer.buffer = self.quantum_bridge.hybrid_buffer
                                logger.critical("      âœ… Buffer set successfully")
                            except Exception as e:
                                logger.error(f"      âŒ Failed to set buffer: {e}")
                    else:
                        logger.critical("      âŒ Trainer Buffer: not accessible (no property or attribute)")
                        # Check if it's a wrapper with base_trainer
                        if hasattr(trainer, 'base_trainer'):
                            logger.critical("      ğŸ”§ Detected wrapper trainer, checking base_trainer...")
                            if hasattr(trainer.base_trainer, 'buffer'):
                                base_buffer = trainer.base_trainer.buffer
                                if base_buffer:
                                    logger.critical(f"      âœ… base_trainer.buffer exists: {len(base_buffer)} experiences")
                                else:
                                    logger.critical("      âš ï¸  base_trainer.buffer is None")
                            else:
                                logger.critical("      âŒ base_trainer has no buffer attribute")
                else:
                    logger.critical("   âŒ Quantum Trainer: is None")
            else:
                logger.critical("   âŒ Quantum Trainer: attribute doesn't exist")
        else:
            logger.critical("âŒ 1. Quantum Bridge MISSING")

        # === Check 4: Batch Processor ===
        if hasattr(self, 'batch_processor') and self.batch_processor:
            stats = self.batch_processor.get_stats()
            logger.critical(f"âœ… 2. Batch Processor: {stats['processed']} processed, {stats['errors']} errors")
        else:
            logger.critical("âŒ 2. Batch Processor MISSING")

        # === Check 5: Old Experience Replay (shouldn't be used) ===
        if hasattr(self, 'experience_replay'):
            logger.critical(f"âš ï¸ 3. Old Experience Replay: {len(self.experience_replay)} (NOT USED)")

        logger.critical("="*80)

        # === Return diagnostic results ===
        return {
            'quantum_bridge_exists': hasattr(self, 'quantum_bridge') and self.quantum_bridge,
            'store_method_exists': hasattr(self.quantum_bridge, 'store_experience_for_agent') if hasattr(self, 'quantum_bridge') else False,
            'hybrid_buffer_size': len(self.quantum_bridge.hybrid_buffer) if hasattr(self, 'quantum_bridge') and hasattr(self.quantum_bridge, 'hybrid_buffer') and self.quantum_bridge.hybrid_buffer else 0,
            'trainer_buffer_size': len(self.quantum_bridge.quantum_trainer.buffer) if hasattr(self, 'quantum_bridge') and hasattr(self.quantum_bridge, 'quantum_trainer') and self.quantum_bridge.quantum_trainer and hasattr(self.quantum_bridge.quantum_trainer, 'buffer') else 0
        }

    # ... rest of your existing methods ...
    def cleanup_ably_tasks(self):
        """Clean up any tracked Ably tasks"""
        if hasattr(self, 'ably_tasks'):
            for task in list(self.ably_tasks):
                if task.done():
                    self.ably_tasks.discard(task)

        async def batch_reward_callback(message):
            """Enhanced batch reward handler with better error handling"""
            try:
                data = message.data
                if not isinstance(data, dict):
                    logger.warning("Batch reward message is not a dict")
                    return

                rewards = data.get("rewards", [])
                batch_id = data.get("batch_id", f"batch_{int(time.time())}")

                if not rewards:
                    logger.warning(f"Empty rewards batch received: {batch_id}")
                    return

                logger.info(f"Processing reward batch {batch_id} with {len(rewards)} rewards")

                # Process rewards with validation
                processed = 0
                failed = 0

                for i, reward_data in enumerate(rewards):
                    try:
                        signal_key = reward_data.get("signal_key")
                        reward = reward_data.get("reward")
                        agent_multipliers = reward_data.get("agent_multipliers", {})

                        # Validation
                        if not signal_key:
                            logger.warning(f"Batch {batch_id}[{i}]: Missing signal_key")
                            failed += 1
                            continue

                        if not isinstance(reward, (int, float)):
                            logger.warning(f"Batch {batch_id}[{i}]: Invalid reward type: {type(reward)}")
                            failed += 1
                            continue

                        # Submit to batch processor
                        if self.batch_processor:
                            await self.batch_processor.add_reward(signal_key, reward, agent_multipliers)
                            processed += 1
                        else:
                            logger.error("Batch processor not available")
                            failed += 1

                    except Exception as e:
                        logger.error(f"Batch {batch_id}[{i}] processing failed: {e}")
                        failed += 1

                logger.info(f"Batch {batch_id} completed: {processed} processed, {failed} failed")

            except Exception as e:
                logger.error(f"Batch reward callback error: {e}")

    def _clean_signal_data(self, signal_data):
        """
        Clean numpy types and handle nested dictionaries safely for JSON serialization
        """
        def clean_value(value):
            if isinstance(value, (np.integer, np.floating)):
                return float(value)
            elif isinstance(value, np.ndarray):
                return value.tolist()
            elif isinstance(value, torch.Tensor):
                return value.cpu().numpy().tolist()
            elif isinstance(value, dict):
                return {k: clean_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [clean_value(item) for item in value]
            elif isinstance(value, (np.bool_, bool)):
                return bool(value)
            elif value is None:
                return None
            elif isinstance(value, str):
                return value
            else:
                try:
                    # Try to convert to float if it's a numeric type
                    return float(value)
                except (ValueError, TypeError):
                    # If conversion fails, convert to string
                    return str(value)

        try:
            cleaned_data = clean_value(signal_data)

            # Ensure critical fields are present and properly typed
            if 'timestamp' in cleaned_data and not isinstance(cleaned_data['timestamp'], str):
                cleaned_data['timestamp'] = str(cleaned_data['timestamp'])

            if 'final_action' in cleaned_data and not isinstance(cleaned_data['final_action'], str):
                cleaned_data['final_action'] = str(cleaned_data['final_action'])

            if 'confidence' in cleaned_data:
                cleaned_data['confidence'] = float(cleaned_data['confidence'])

            return cleaned_data

        except Exception as e:
            logger.error(f"Data cleaning failed: {e}")
            # Return minimal safe data structure
            return {
                'timestamp': signal_data.get('timestamp', time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime())),
                'final_action': str(signal_data.get('final_action', 'HOLD')),
                'price': float(signal_data.get('price', 0.0)),
                'confidence': 0.5,
                'agent_count': int(signal_data.get('agent_count', 0)),
                'confirmed': bool(signal_data.get('confirmed', False)),
                'error': 'data_cleaning_failed',
                'original_error': str(e)
            }

    async def _process_latest_features(self):
        """
        Fully integrated async-safe pipeline for QuantumAgents.
        Combines robust feature sanitization, safe state caching, multi-agent prediction,
        meta-gating, voting, and publishing â€” with last valid states fallback.
        """
        import asyncio, time, random, numpy as np

        # --- Ensure processing lock ---
        if not hasattr(self, "processing_lock") or self.processing_lock is None:
            logger.warning("processing_lock missing, creating new asyncio.Lock()")
            self.processing_lock = asyncio.Lock()

        # --- Helper: sanitize features with price preservation ---
        def sanitize_features(feats, agent):
            """Converts any feature format to clean 1D float32 array of correct length while preserving price keys."""
            state_dim = getattr(agent, "state_dim", getattr(self, "state_dim", 58))

            preserved_fields = {}
            if isinstance(feats, dict):
                for k in ['price', 'close_scaled', 'close', 'current_price', 'last_price']:
                    if k in feats:
                        try:
                            preserved_fields[k] = float(feats[k])
                        except Exception:
                            preserved_fields[k] = 0.0

            def _flatten_values(x):
                flat = []
                if x is None:
                    return flat
                if isinstance(x, dict):
                    for v in x.values():
                        flat.extend(_flatten_values(v))
                elif type(x).__name__ == "dict_values":
                    for v in list(x):
                        flat.extend(_flatten_values(v))
                elif isinstance(x, (list, tuple, np.ndarray, set)):
                    for v in x:
                        flat.extend(_flatten_values(v))
                elif callable(x):
                    flat.append(0.0)
                else:
                    try:
                        flat.append(float(x))
                    except Exception:
                        flat.append(0.0)
                return flat

            # Handle empty or invalid feats
            if feats is None or (isinstance(feats, np.ndarray) and feats.size == 0):
                last = getattr(self, "last_valid_features", {}).get(getattr(agent, "name", None))
                if isinstance(last, np.ndarray) and last.size == state_dim:
                    arr = last
                else:
                    arr = np.full(state_dim, 0.5, dtype=np.float32)
            else:
                # Flatten
                arr = np.array(_flatten_values(feats), dtype=np.float32).flatten()
                arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)
                # Match state_dim
                if arr.size < state_dim:
                    last = getattr(self, "last_valid_features", {}).get(getattr(agent, "name", None))
                    if isinstance(last, np.ndarray) and last.size == state_dim:
                        arr = last
                    else:
                        arr = np.pad(arr, (0, state_dim - arr.size), "edge")
                elif arr.size > state_dim:
                    arr = arr[:state_dim]

            # Attach preserved fields
            if preserved_fields:
                agent.last_preserved_fields = preserved_fields
                if 'price' in preserved_fields:
                    agent.last_price = preserved_fields['price']

            return arr.astype(np.float32)

        # --- Defensive snapshot retrieval ---
        with self.lock:
            snapshot = getattr(self, "latest_features", {}) or {}
            if not isinstance(snapshot, dict):
                snapshot = {}

        if len(snapshot) < 1:
            logger.debug("Waiting for features to populate...")
            return

        if not hasattr(self, "last_valid_features"):
            self.last_valid_features = {}

        async with self.processing_lock:
            # --- Extract price BEFORE sanitizing features (BUG FIX #1) ---
            price = self._extract_price_fast(snapshot)

            # --- Sanitize and update feature set ---
            for agent_name, agent in self.agents.items():
                feats = snapshot.get(agent_name)
                feats = sanitize_features(feats, agent)
                self.last_valid_features[agent_name] = feats
                snapshot[agent_name] = feats

                # --- Update quantum bridge cache ---
                try:
                    self.quantum_bridge.update_agent_state(agent_name, feats)
                except Exception as e:
                    logger.critical(f"[{agent_name}] Failed to update state cache: {e}")

            # --- Ensure all agents cached ---
            all_cached = all(
                name in self.quantum_bridge.agent_to_idx
                for name in self.agents.keys()
            )
            if not all_cached:
                logger.debug(f"â³ Waiting: {sum(name in self.quantum_bridge.state_cache for name in self.agents)}/{len(self.agents)} agents cached.")
                return

            # --- Run safe quantum predictions per agent ---
            # V8.6.3 DIAGNOSTIC: Log everything, raise on failure
            agent_q_values = {}
            metadata = {}
            prediction_failures = []
            
            logger.info(f"ğŸ”„ Starting Q-value predictions for {len(self.agents)} agents...")
            
            for agent_name in self.agents.keys():
                feats = self.last_valid_features.get(agent_name)
                try:
                    if feats is not None:
                        self.quantum_bridge.update_agent_state(agent_name, feats)
                    
                    q_vals = self.quantum_bridge.predict_single_agent(agent_name)
                    
                    # V8.6.3: Check if we got default values (indicates failure)
                    if np.allclose(q_vals, [0.5, 0.5], atol=0.001):
                        logger.warning(
                            f"âš ï¸ [{agent_name}] Got DEFAULT Q-values [0.5, 0.5] - prediction may have failed!\n"
                            f"   feats type: {type(feats)}\n"
                            f"   feats is None: {feats is None}"
                        )
                    
                    agent_q_values[agent_name] = q_vals
                    logger.info(f"ğŸ“Š [{agent_name}] Q-values: BUY={q_vals[0]:.4f}, SELL={q_vals[1]:.4f}")
                    
                except Exception as e:
                    prediction_failures.append((agent_name, str(e)))
                    logger.critical(
                        f"âŒ [{agent_name}] QUANTUM PREDICTION FAILED:\n"
                        f"   Error: {e}\n"
                        f"   feats type: {type(feats)}\n"
                        f"   feats is None: {feats is None}\n"
                        f"   quantum_bridge: {self.quantum_bridge is not None}\n"
                        f"   Stack trace:",
                        exc_info=True
                    )
                    # DON'T silently default - track the failure
                    agent_q_values[agent_name] = np.array([0.5, 0.5], dtype=np.float32)
            
            # V8.6.3: Log prediction summary
            if prediction_failures:
                logger.critical(
                    f"âŒ {len(prediction_failures)}/{len(self.agents)} agents FAILED prediction:\n"
                    f"   Failures: {prediction_failures}"
                )
            
            # --- Process experiences and actions ---
            # V8.6.3 DIAGNOSTIC: Log every action decision
            agent_actions, keys, agent_data = {}, {}, {}
            default_q_count = 0
            
            for agent_name, agent in self.agents.items():
                try:
                    q = agent_q_values.get(agent_name, np.array([0.5, 0.5]))
                    if not isinstance(q, np.ndarray):
                        q = np.array(q).flatten()
                    if q.size < 2:
                        logger.warning(f"âš ï¸ [{agent_name}] q.size={q.size} < 2, using default")
                        q = np.array([0.5, 0.5])
                    
                    # V8.6.3: Track default Q-values
                    if np.allclose(q, [0.5, 0.5], atol=0.001):
                        default_q_count += 1
                    
                    action = int(np.argmax(q[:2]))
                    agent_actions[agent_name] = action
                    
                    # V8.6.3: Log individual agent decisions
                    logger.info(
                        f"ğŸ¯ [{agent_name}] Q=[{q[0]:.4f}, {q[1]:.4f}] â†’ {['BUY','SELL'][action]}"
                    )

                    state = self.last_valid_features[agent_name]
                    key = f"{agent_name}_{int(time.time()*1e6)}_{random.randint(1000,9999)}"
                    states_dict = self.quantum_bridge.state_cache.copy() if hasattr(self.quantum_bridge, 'state_cache') else {agent_name: state}
                    self.store_experience_for_agent(key, agent_name, states_dict, action, q)
                    keys[agent_name] = key
                    agent_data[agent_name] = (state, action, q)
                except Exception as e:
                    logger.critical(f"âŒ [{agent_name}] Partial experience failed: {e}", exc_info=True)

            # V8.6.3: CRITICAL WARNING if too many defaults
            if default_q_count > 0:
                logger.critical(
                    f"âš ï¸âš ï¸âš ï¸ {default_q_count}/{len(self.agents)} agents using DEFAULT Q-values [0.5, 0.5]!\n"
                    f"   This causes BUY bias (argmax([0.5, 0.5]) = 0 = BUY)\n"
                    f"   Check: Is quantum_system.predict() working?\n"
                    f"   Check: Is state_cache being populated?"
                )

            # V8.6.3: Log vote distribution BEFORE voting
            from collections import Counter
            action_counts = Counter(agent_actions.values())
            total = len(agent_actions)
            buy_pct = action_counts.get(0, 0) / max(total, 1) * 100
            sell_pct = action_counts.get(1, 0) / max(total, 1) * 100
            logger.info(
                f"ğŸ—³ï¸ PRE-VOTE DISTRIBUTION: BUY={buy_pct:.1f}%, SELL={sell_pct:.1f}% "
                f"({action_counts}) from {total} agents"
            )
            
            if buy_pct >= 87.5 or sell_pct >= 87.5:
                logger.critical(
                    f"âŒ HOMOGENIZATION DETECTED: {buy_pct:.1f}% BUY, {sell_pct:.1f}% SELL\n"
                    f"   Agent votes: {dict(agent_actions)}\n"
                    f"   Default Q-value count: {default_q_count}/{len(self.agents)}\n"
                    f"   Prediction failures: {len(prediction_failures)}"
                )

            # --- Voting stage ---
            # V8.6.3 DIAGNOSTIC: Don't silently default to BUY
            try:
                voting_pred, voting_logits = self._fast_voting_predict(agent_q_values, agent_actions)
                logger.info(f"âœ… Voting model prediction: {['BUY','SELL','HOLD'][voting_pred]}")
            except Exception as e:
                logger.critical(
                    f"âŒ VOTING MODEL FAILED:\n"
                    f"   Error: {e}\n"
                    f"   agent_q_values keys: {list(agent_q_values.keys())}\n"
                    f"   agent_actions: {agent_actions}\n"
                    f"   Using majority vote from agents instead of defaulting to BUY",
                    exc_info=True
                )
                # V8.6.3: Use actual majority vote instead of defaulting to 0 (BUY)
                from collections import Counter
                action_counts = Counter(agent_actions.values())
                if action_counts:
                    voting_pred = action_counts.most_common(1)[0][0]
                    logger.warning(f"âš ï¸ Using agent majority vote fallback: {['BUY','SELL'][voting_pred]}")
                else:
                    logger.critical("âŒ No agent actions available - this should not happen!")
                    raise ValueError("Voting failed and no agent actions available")
                voting_logits = None
            initial_action_str = ACTION_MAP.get(voting_pred, "BUY")

            # --- Confidence and gating ---
            confidence_score = None
            try:
                if voting_logits is not None:
                    confidence_score, _ = self.calculate_signal_confidence(agent_q_values, agent_actions, voting_logits)
            except Exception:
                confidence_score = None

            allow_publish = True
            if getattr(self, "meta_gating_enabled", False):
                try:
                    allow_publish = await asyncio.wait_for(self._fast_meta_check(agent_data, voting_pred), timeout=1.0)
                except Exception:
                    allow_publish = True
            if not allow_publish:
                logger.critical("Meta gating blocked signal publication.")
                return

            # --- Pullback logic ---
            # Note: price already extracted earlier (before sanitization) - BUG FIX #1
            try:
                final_action_int = self.apply_pullback_logic(voting_pred, price)
            except Exception:
                final_action_int = voting_pred

            final_action_str = ACTION_MAP.get(final_action_int, "BUY")
            logger.critical(f"âœ… FINAL SIGNAL: {final_action_str} (voting: {initial_action_str})")

            # --- Compute Agent Credit Multipliers (V8.6.1 FIX) ---
            # This computes per-agent credit based on agreement and confidence
            try:
                agent_multipliers = compute_agent_credit_multipliers(
                    agent_actions=agent_actions,
                    agent_q_values=agent_q_values,
                    final_action=final_action_int,
                    method='agreement_confidence'
                )
                logger.debug(f"[CreditAssignment] Computed multipliers: {agent_multipliers}")
            except Exception as e:
                logger.warning(f"[CreditAssignment] Failed to compute multipliers: {e}")
                agent_multipliers = {}

            # --- Publish ---
            try:
                if getattr(self, "ably", None):
                    await self._publish_direct(
                        final_action_str, price, keys, agent_actions,
                        agent_q_values, voting_pred, agent_multipliers, "no_gating", voting_logits
                    )
            except Exception as e:
                logger.critical(f"Publishing failed: {e}")

        await asyncio.sleep(0.001)

    async def _publish_direct(self, final_action_str, price, keys, agent_actions,
                              q_vals, voting_pred, agent_multipliers, gating_reason,
                              voting_logits=None):
        """
        V5.0 ENHANCED: Publishing with SIGNAL DIVERSITY ENFORCEMENT.
        
        Tracks last 30 signals and flips if 20 consecutive identical signals detected.
        This is a last-resort mechanism to ensure trading diversity.
        """
        import time as time_module
        
        # Initialize signal history tracking
        if not hasattr(self, '_signal_history'):
            self._signal_history = []
        
        try:
            # V5.0: SIGNAL DIVERSITY ENFORCEMENT
            self._signal_history.append(final_action_str)
            if len(self._signal_history) > 30:
                self._signal_history.pop(0)
            
            # Check if last 20 signals are identical
            if len(self._signal_history) >= 20:
                recent_signals = self._signal_history[-20:]
                if len(set(recent_signals)) == 1:
                    stuck_action = recent_signals[-1]
                    print(f"[{time_module.strftime('%H:%M:%S')}] ğŸ”´ SIGNAL STUCK ON {stuck_action} for 20 signals!")
                    print(f"[{time_module.strftime('%H:%M:%S')}] ğŸ’‰ Injecting opposite signal for diversity...")
                    
                    # Flip the signal
                    final_action_str = 'SELL' if stuck_action == 'BUY' else 'BUY'
                    voting_pred = 1 if stuck_action == 'BUY' else 0
                    
                    # Clear history to reset
                    self._signal_history.clear()
                    self._signal_history.append(final_action_str)
            
            # Log signal publishing
            print(f"[{time_module.strftime('%H:%M:%S')}] ğŸ“¡ PUBLISHING SIGNAL {final_action_str} @ {price}")
            
            final_channel = self.ably.channels.get("final_signals")
            selected_keys = list(keys.values())

            # Calculate confidence
            confidence_score = None
            confidence_breakdown = None

            if voting_logits is not None:
                try:
                    confidence_score, confidence_breakdown = self.calculate_signal_confidence(
                        q_vals, agent_actions, voting_logits
                    )
                except Exception as e:
                    logger.warning(f"Confidence calculation failed: {e}")

            if confidence_score is None:
                confidence_score = 0.5
                confidence_breakdown = {'fallback_method': 'default'}

            # CRITICAL: Capture complete state data for quantum training
            if hasattr(self, 'exp_manager') and self.exp_manager is not None:
                for signal_key in selected_keys:
                    agent_name = signal_key.split('_')[0]  # Extract agent name from key

                    if agent_name in self.agents:
                        try:
                            # Get current states from quantum bridge cache
                            if hasattr(self, 'quantum_bridge'):
                                states_dict = self.quantum_bridge.state_cache.copy()

                                # Store complete experience
                                action = agent_actions.get(agent_name, 0)
                                q_values = q_vals.get(agent_name, np.array([0.5, 0.5]))

                                self.store_experience_for_agent(
                                    signal_key=signal_key,
                                    agent_name=agent_name,
                                    states_dict=states_dict,
                                    action=action,
                                    q_values=q_values
                                )

                                logger.debug(f"[{agent_name}] Stored complete state for {signal_key}")

                        except Exception as e:
                            logger.error(f"[{agent_name}] Failed to store experience: {e}")

            # Prepare signal data
            signal_data = {
                'timestamp': time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
                'final_action': final_action_str,
                'price': float(price) if price is not None else 0.0,
                'signal_keys': selected_keys,
                'agent_count': len(selected_keys),
                'confidence': float(confidence_score),
                'confidence_breakdown': confidence_breakdown,
                'voting_prediction': int(voting_pred),
                'agent_multipliers': agent_multipliers,
                'meta_gating_enabled': self.meta_gating_enabled,
                'confirmation_required': self.confirmation_required,
                'total_training_steps': self.total_agent_training_steps,
                'processing_mode': 'fast_learning',
                'gating_reason': gating_reason,
                'confirmed': True
            }

            # Clean and publish
            cleaned_data = self._clean_signal_data(signal_data)

            # ================================================================
            # V5.0.4: NON-BLOCKING SIGNAL PUBLISHING
            # ================================================================
            # Use AsyncSignalPublisher to prevent Ably timeouts from blocking
            # the event loop. This ensures feature messages continue flowing
            # even if signal publishing takes a long time or fails.
            # ================================================================
            signal_publisher = get_signal_publisher()
            
            if signal_publisher is not None:
                # Non-blocking publish - returns immediately
                signal_publisher.submit_signal(
                    channel=final_channel,
                    event_name="new-final-signal",
                    data=cleaned_data
                )
                logger.info(f"ğŸ“¡ Signal submitted (non-blocking): {signal_data.get('final_action')} @ {signal_data.get('price')}")
                logger.info(
                    f"SIGNAL QUEUED: {final_action_str} "
                    f"(confidence: {confidence_score:.3f}, quantum data captured)"
                )
            else:
                # Fallback to blocking publish if publisher not initialized
                # (shouldn't happen in normal operation)
                print(f"[{time_module.strftime('%H:%M:%S')}] âš ï¸ Signal publisher not initialized - using blocking publish")
                try:
                    await asyncio.wait_for(
                        final_channel.publish("new-final-signal", cleaned_data),
                        timeout=10.0
                    )
                    print(f"[{time_module.strftime('%H:%M:%S')}] âœ… âœ… Signal published (blocking fallback)")
                except asyncio.TimeoutError:
                    print(f"[{time_module.strftime('%H:%M:%S')}] âš ï¸ Signal publish timeout (blocking fallback)")
                except Exception as pub_err:
                    print(f"[{time_module.strftime('%H:%M:%S')}] âš ï¸ Signal publish failed: {pub_err}")
            # ================================================================

        except Exception as e:
            logger.error(f"Signal publish failed: {e}")
            import traceback
            traceback.print_exc()

        def calculate_signal_confidence(self, q_vals, actions, voting_logits=None):
            """
            Improved signal confidence calculation for trading bots.
            Args:
                q_vals: dict of agent_name -> [Q(BUY), Q(SELL)] (list or np.array)
                actions: dict of agent_name -> action integer (0=BUY, 1=SELL, 2=HOLD)
                voting_logits: torch.Tensor output from voting model, shape (1, 2)
            Returns:
                confidence (float in [0,1]), breakdown (dict)
            """

            # --- Agent Agreement ---
            action_counts = Counter(actions.values())
            agreement_ratio = max(action_counts.values()) / len(actions) if actions else 0.0

            # --- Q-value Spread (decisiveness between actions) ---
            q_spreads = []
            q_margins = []
            for q in q_vals.values():
                q = np.array(q, dtype=np.float32)
                if len(q) < 2:
                    continue
                # Spread = abs(Q[BUY] - Q[SELL])
                q_spreads.append(abs(q[0] - q[1]))
                # Margin = best Q - second best Q
                q_sorted = np.sort(q)
                q_margins.append(q_sorted[-1] - q_sorted[-2])
            avg_q_spread = float(np.mean(q_spreads)) if q_spreads else 0.0
            avg_q_margin = float(np.mean(q_margins)) if q_margins else 0.0

            # --- Voting Model Confidence ---
            if voting_logits is not None:
                voting_probs = F.softmax(voting_logits, dim=-1).cpu().numpy().flatten()
                voting_confidence = float(np.max(voting_probs))  # e.g., max([p_buy, p_sell])
            else:
                voting_confidence = 0.5  # fallback if voting_logits not provided

            # --- Weighted Composite Confidence ---
            # You can tune weights as needed
            weights = {
                "agreement": 0.35,   # consensus
                "spread": 0.25,      # decisiveness
                "voting": 0.25,      # model output certainty
                "margin": 0.15       # clarity of choice
            }
            confidence = (
                weights["agreement"] * agreement_ratio +
                weights["spread"] * avg_q_spread +
                weights["voting"] * voting_confidence +
                weights["margin"] * avg_q_margin
            )

            # Clamp confidence to [0, 1]
            confidence = max(0.0, min(confidence, 1.0))

            breakdown = {
                "agent_agreement": agreement_ratio,
                "q_value_spread": avg_q_spread,
                "voting_confidence": voting_confidence,
                "q_margin": avg_q_margin,
                "weights": weights
            }
            return confidence, breakdown

    async def _publish_with_confirmation(self, final_action_str, price, keys, agent_actions,
                                       q_vals, voting_pred, agent_multipliers, gating_reason,
                                       voting_logits=None):
        """Publishing with confirmation and confidence information"""
        try:
            selected_keys = list(keys.values())

            # Calculate confidence score (same as _publish_direct)
            confidence_score = None
            confidence_breakdown = None

            if voting_logits is not None:
                try:
                    confidence_score, confidence_breakdown = self.calculate_signal_confidence(
                        q_vals, agent_actions, voting_logits
                    )
                except Exception as e:
                    logger.warning(f"Confidence calculation failed during publish: {e}")

            # Fallback confidence if needed
            if confidence_score is None:
                try:
                    confidence_score = float(np.mean([np.max(q) for q in q_vals.values()]))
                    confidence_breakdown = {
                        'agent_agreement': 'N/A',
                        'q_value_spread': 'N/A',
                        'voting_confidence': 'N/A',
                        'voting_logits_available': False,
                        'fallback_method': 'q_values_mean'
                    }
                except Exception:
                    confidence_score = 0.5
                    confidence_breakdown = {
                        'agent_agreement': 'N/A',
                        'q_value_spread': 'N/A',
                        'voting_confidence': 'N/A',
                        'voting_logits_available': False,
                        'fallback_method': 'default'
                    }

            signal_data = {
                'timestamp': time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime()),
                'final_action': final_action_str,
                'price': float(price) if price is not None else 0.0,
                'signal_keys': selected_keys,
                'agent_count': len(selected_keys),

                # CONFIDENCE DATA BLOCK
                'confidence': float(confidence_score),
                'confidence_breakdown': {
                    'agent_agreement': float(confidence_breakdown.get('agent_agreement', 0)) if isinstance(confidence_breakdown.get('agent_agreement'), (int, float)) else confidence_breakdown.get('agent_agreement'),
                    'q_value_spread': float(confidence_breakdown.get('q_value_spread', 0)) if isinstance(confidence_breakdown.get('q_value_spread'), (int, float)) else confidence_breakdown.get('q_value_spread'),
                    'voting_confidence': float(confidence_breakdown.get('voting_confidence', 0)) if isinstance(confidence_breakdown.get('voting_confidence'), (int, float)) else confidence_breakdown.get('voting_confidence'),
                    'voting_logits_available': confidence_breakdown.get('voting_logits_available', False),
                    'method': confidence_breakdown.get('fallback_method', 'full_calculation')
                },

                'voting_prediction': int(voting_pred),
                'agent_multipliers': agent_multipliers,
                'meta_gating_enabled': self.meta_gating_enabled,
                'confirmation_required': self.confirmation_required,
                'total_training_steps': self.total_agent_training_steps,
                'processing_mode': 'full_protection',
                'gating_reason': gating_reason
            }

            # Clean numpy types
            cleaned_data = self._clean_signal_data(signal_data)

            # Request confirmation
            if self.confirmation_request_channel:
                logger.info(f"Requesting confirmation for {final_action_str} signal (confidence: {confidence_score:.3f})...")
                
                # V5.0.4: Non-blocking confirmation request
                signal_publisher = get_signal_publisher()
                if signal_publisher is not None:
                    signal_publisher.submit_signal(
                        channel=self.confirmation_request_channel,
                        event_name="confirm",
                        data=cleaned_data
                    )
                else:
                    try:
                        await asyncio.wait_for(
                            self.confirmation_request_channel.publish("confirm", cleaned_data),
                            timeout=5.0
                        )
                    except:
                        pass

                confirmed = await self.wait_for_confirmation(
                    signal_keys=selected_keys,
                    expected_signal=final_action_str,
                    timeout=10.0,
                    min_agree=1
                )

                if confirmed:
                    # Publish confirmed signal with confidence data
                    final_channel = self.ably.channels.get("final_signals")
                    cleaned_data['confirmed'] = True

                    # V5.0.4: Non-blocking signal publish
                    if signal_publisher is not None:
                        signal_publisher.submit_signal(
                            channel=final_channel,
                            event_name="new-final-signal",
                            data=cleaned_data
                        )
                        logger.info(f"ğŸ“¡ Signal submitted (non-blocking): {signal_data.get('signal')} @ {signal_data.get('price')}")
                    else:
                        try:
                            await asyncio.wait_for(
                                final_channel.publish("new-final-signal", cleaned_data),
                                timeout=10.0
                            )
                            logger.info(f"ğŸ“¡ Signal published: {signal_data.get('signal')} @ {signal_data.get('price')}")
                        except:
                            logger.warning("Signal publish timeout/failed")
                    
                    logger.info(f"CONFIRMED SIGNAL PUBLISHED: {final_action_str} "
                               f"(step {self.total_agent_training_steps}, confidence: {confidence_score:.3f})")
                else:
                    logger.warning(f"Signal REJECTED: No confirmation received for {final_action_str}")

        except Exception as e:
            logger.error(f"Confirmation-based publish failed: {e}")
            logger.debug("Confirmation publish error details:", exc_info=True)
    async def on_feature_message(self, message):
        """Handle incoming feature messages with validation, price fallback, and deep diagnostics."""
        import time as time_module
        
        # V5.0: Immediate confirmation that message was received
        print(f"[{time_module.strftime('%H:%M:%S')}] ğŸ“¥ FEATURE MESSAGE RECEIVED")
        
        try:
            # Validate message structure
            if not hasattr(message, 'data') or not isinstance(message.data, dict):
                logger.warning("Invalid message format: missing or invalid data")
                return

            data = message.data

            if 'agent' not in data:
                logger.warning("Feature message missing 'agent' field")
                return

            if 'features' not in data:
                logger.warning("Feature message missing 'features' field")
                return

            name = data['agent']
            features = data['features']
            
            # V5.0: Log which agent received data
            print(f"[{time_module.strftime('%H:%M:%S')}] ğŸ“¥ [{name}] Features received ({len(features)} keys)")

            # ============================================================
            # ğŸ“Š EMERGENCY DIAGNOSTIC - REMOVE AFTER DEBUGGING
            # ============================================================
            if name == 'xs':  # Limit output to one agent to prevent spam
                logger.info("=" * 80)
                logger.info(f"ğŸ“Š FEATURE MESSAGE DIAGNOSTIC - Agent: {name}")
                logger.info("=" * 80)
                logger.info(f"Feature type: {type(features)}")
                logger.info(f"Feature keys: {list(features.keys())}")
                logger.info("")
                logger.info("Checking for price fields:")
                for price_key in ['price', 'close_scaled', 'close', 'last_price', 'current_price']:
                    has_key = price_key in features
                    value = features.get(price_key, 'NOT FOUND')
                    logger.critical(f"  - {price_key}: {has_key} | Value: {value}")
                logger.info("")
                logger.info("Sample feature values:")
                for i, (k, v) in enumerate(features.items()):
                    if i >= 10:
                        break
                    logger.critical(f"  - {k}: {v}")
                logger.info("=" * 80)

            # ============================================================
            # ğŸ”§ PATCH #2: Add Price to Feature Messages (Fallback)
            # ============================================================
            if 'price' not in features and 'close_scaled' not in features:
                if 'meta' in data and isinstance(data['meta'], dict):
                    if 'close' in data['meta']:
                        features['price'] = data['meta']['close']
                    elif 'price' in data['meta']:
                        features['price'] = data['meta']['price']

                if 'price' not in features and 'close_scaled' not in features:
                    if hasattr(self, '_last_price_by_agent') and name in self._last_price_by_agent:
                        features['price'] = self._last_price_by_agent[name]
                        logger.critical(f"[{name}] ğŸ” Added fallback price: {features['price']}")

            # Store price for future use
            if not hasattr(self, '_last_price_by_agent'):
                self._last_price_by_agent = {}

            for price_key in ['price', 'close_scaled', 'close']:
                if price_key in features:
                    try:
                        self._last_price_by_agent[name] = float(features[price_key])
                        break
                    except (ValueError, TypeError):
                        pass

            # ============================================================
            # âœ… Thread-safe update of latest features
            # ============================================================
            with self.lock:
                self.latest_features[name] = features

            logger.debug(f"Features updated for {name}, total agents with features: {len(self.latest_features)}")

            # ============================================================
            # ğŸ§­ Periodic logging of sample feature keys
            # ============================================================
            if hasattr(self, '_feature_log_counter'):
                self._feature_log_counter += 1
            else:
                self._feature_log_counter = 1

            if self._feature_log_counter % 50 == 0:
                sample_keys = list(features.keys())[:5]
                logger.critical(f"[{name}] Sample features: {sample_keys}... (total: {len(features)})")

            # ============================================================
            # ğŸ” Always process latest features
            # ============================================================
            try:
                await self._process_latest_features()
            except Exception as e:
                logger.critical(f"[{name}] âŒ Error processing latest features: {e}", exc_info=True)

        except Exception as e:
            logger.critical(f"âŒ on_feature_message critical failure: {e}", exc_info=True)


    def _handle_meta_features_message(self, agent_name, message):
        """Handle meta features messages"""
        try:
            data = message.data
            if not isinstance(data, dict):
                logger.warning(f"[{agent_name}] Meta features message is not a dict.")
                return

            clean_meta = {
                k: float(v) for k, v in data.items()
                if isinstance(v, (int, float)) and "time" not in k.lower()
            }

            if clean_meta:
                with self.lock:
                    if not hasattr(self, "latest_meta_features"):
                        self.latest_meta_features = {}
                    self.latest_meta_features[agent_name] = clean_meta
            #else:
                #logger.warning(f"[{agent_name}] No usable meta features values in message: {data}")

        except Exception as e:
            logger.error(f"[{agent_name}] Meta features message handling error: {e}")

    def store_partial_experience(self, signal_key, agent_name, state, action, next_state, q_values_dict=None, voting_pred=None, extra_meta_features=None, reward=None):
        """Store partial experience for later reward assignment"""
        with self.lock:
            self.partial_experiences[signal_key] = {
                'agent_name': agent_name,
                'state': state,
                'action': action,
                'next_state': next_state,
                'timestamp': time.time()
            }

    # === PASTE THIS - REPLACE ENTIRE apply_pullback_logic METHOD ===
    def apply_pullback_logic(self, final_action, price):
        """
        Apply pullback logic to final action.
        Returns 0 (BUY) or 1 (SELL) - NO HOLD, flips signal instead.
        """
        self.recent_prices.append(price)

        if self.last_final_action is None or self.last_final_price is None:
            self.last_final_action = final_action
            self.last_final_price = price
            return final_action

        atr = self.compute_atr_proxy()
        ema20 = self.compute_ema20()
        adaptive_threshold = 0.4 * atr if atr > 0 else 0.001

        delta = price - self.last_final_price

        price_vs_ema_buy = price > ema20
        price_vs_ema_sell = price < ema20
        delta_check_buy = delta < -adaptive_threshold
        delta_check_sell = delta > adaptive_threshold

        pullback_detected = False
        if self.last_final_action == 0 and delta_check_buy and price_vs_ema_buy:  # BUY
            pullback_detected = True
        elif self.last_final_action == 1 and delta_check_sell and price_vs_ema_sell:  # SELL
            pullback_detected = True

        repeating_same_signal = self.last_final_action == final_action

        pullback_confirmed = False
        if pullback_detected and len(self.recent_prices) >= self.pullback_window:
            recent_prices = list(self.recent_prices)[-self.pullback_window:]
            if self.last_final_action == 0:  # BUY
                count_below = sum(p < self.last_final_price for p in recent_prices)
                pullback_confirmed = count_below >= int(0.7 * self.pullback_window)
            elif self.last_final_action == 1:  # SELL
                count_above = sum(p > self.last_final_price for p in recent_prices)
                pullback_confirmed = count_above >= int(0.7 * self.pullback_window)

        # CHANGED: Flip signal instead of returning HOLD
        if repeating_same_signal and pullback_detected and not pullback_confirmed:
            flipped_action = 1 - final_action
            logger.info(f"Pullback detected - flipping {ACTION_MAP[final_action]} to {ACTION_MAP[flipped_action]}")
            return flipped_action

        if pullback_confirmed and not repeating_same_signal:
            logger.info(f"Confirmed pullback - flipping from {ACTION_MAP[self.last_final_action]} to {ACTION_MAP[final_action]}")
            self.last_final_action = final_action
            self.last_final_price = price
            return final_action

        self.last_final_action = final_action
        self.last_final_price = price
        return final_action

    def compute_ema20(self):
        """Compute EMA20"""
        prices = list(self.recent_prices)
        if len(prices) < 20:
            return sum(prices) / len(prices) if prices else 0.0
        alpha = 2 / (20 + 1)
        ema = prices[0]
        for price in prices[1:]:
            ema = alpha * price + (1 - alpha) * ema
        return ema

    def compute_atr_proxy(self):
        """Compute ATR proxy"""
        prices = list(self.recent_prices)
        if len(prices) < 15:
            return 0.0
        diffs = [abs(prices[i] - prices[i - 1]) for i in range(1, len(prices))]
        return sum(diffs[-14:]) / 14

    def send_signal_change_notification(self, new_signal, price=None, confidence=None):
        try:
            self.discord_notifier.notify_signal_change(new_signal, price, confidence)
        except Exception as e:
            logger.error(f"Signal notification failed: {e}")

    def track_signal_ratio(self, voted_signals):
        if len(voted_signals) >= 300:
            buy = sum(1 for s in voted_signals if s == 'BUY')
            sell = sum(1 for s in voted_signals if s == 'SELL')
            self.discord_notifier.notify_signal_summary(buy, sell, len(voted_signals))
            voted_signals.clear()

    def _start_event_loop(self):
        """Start asyncio event loop with enhanced error handling - REPLACE EXISTING METHOD"""
        def run_safe_loop():
            try:
                logger.info("Starting safe asyncio event loop...")
                asyncio.set_event_loop(self.loop)

                # Setup safe exception handling
                setup_safe_exception_handling(self.loop)

                # Start health monitoring
                self.safe_task_manager.create_safe_task(
                    self._health_monitor(),
                    name="health_monitor"
                )

                self.loop.run_forever()

            except Exception as e:
                logger.error(f"Event loop error: {e}")
            finally:
                # Clean up tasks on shutdown
                pending = asyncio.all_tasks(self.loop)
                if pending:
                    logger.info(f"Cleaning up {len(pending)} pending tasks")
                    for task in pending:
                        task.cancel()

        threading.Thread(target=run_safe_loop, daemon=True).start()

    def _start_cleanup_thread(self):
        """Start cleanup thread for expired experiences"""
        def cleanup():
            MAX_AGE = 2700  # 45 minutes
            while True:
                now = time.time()
                with self.lock:
                    expired = [k for k, v in self.partial_experiences.items() if now - v['timestamp'] > MAX_AGE]
                    for k in expired:
                        logger.warning(f"Discarding expired partial experience: {k}")
                        del self.partial_experiences[k]
                time.sleep(60)
        threading.Thread(target=cleanup, daemon=True).start()

    def _start_autosave_thread(self):
        """Start autosave thread"""
        def autosave():
            while True:
                try:
                    logger.info("Autosaving models and data...")
                    self.save_state()
                except Exception as e:
                    logger.error(f"Autosave failed: {e}")
                time.sleep(600)
        threading.Thread(target=autosave, daemon=True).start()

    def _start_voting_model_trainer(self, interval_sec=180):
        """
        SIMPLIFIED: Only transfers samples to training queue.
        Actual training handled by batch processor.
        """
        def monitor():
            logger.info("[VOTING] Sample monitor started (checks queue, training via batch processor)")

            while True:
                time.sleep(30)

                with self.voting_collection_lock:
                    queue_size = len(self.voting_sample_queue)

                if queue_size >= 450:  # Near capacity
                    logger.warning(f"[VOTING] Queue near capacity: {queue_size}/500")
                elif queue_size % 100 == 0 and queue_size > 0:
                    logger.info(f"[VOTING] Queue status: {queue_size}/500 samples")

        threading.Thread(target=monitor, daemon=True).start()

    def diagnose_voting_model(self):
        """Fixed diagnostics with proper device handling"""
        try:
            logger.info("=" * 80)
            logger.info("VOTING MODEL DIAGNOSTICS")
            logger.info("=" * 80)

            # 1. Model parameter tracking
            if hasattr(self, '_last_voting_params'):
                current_params = {}
                params_changed = []
                total_change = 0.0

                for name, param in self.voting_model.named_parameters():
                    current_params[name] = param.data.clone()
                    if name in self._last_voting_params:
                        diff = (current_params[name] - self._last_voting_params[name]).abs().mean().item()
                        total_change += diff
                        if diff > 1e-6:
                            params_changed.append((name, diff))

                if params_changed:
                    logger.info(f"Parameters CHANGED: {len(params_changed)}/{len(current_params)}")
                    for name, diff in params_changed[:3]:
                        logger.info(f"  {name}: mean_diff={diff:.6e}")
                else:
                    logger.info("Parameters FROZEN: No changes detected!")

                logger.info(f"Total parameter change: {total_change:.6e}")
            else:
                logger.info("First diagnostic run - saving baseline parameters")

            # Save current state
            self._last_voting_params = {
                name: param.data.clone()
                for name, param in self.voting_model.named_parameters()
            }

            # 2. Check model device
            model_device = next(self.voting_model.parameters()).device
            logger.info(f"Model device: {model_device}")
            logger.info(f"System device: {self.device}")

            if str(model_device) != str(self.device):
                logger.info(f"WARNING: Device mismatch! Moving model to {self.device}")
                self.voting_model = self.voting_model.to(self.device)
                model_device = next(self.voting_model.parameters()).device
                logger.info(f"Model now on: {model_device}")

            # 3. Sample queue analysis
            with self.voting_collection_lock:
                queue_size = len(self.voting_sample_queue)
                logger.info(f"Sample queue: {queue_size}/500")

                if queue_size > 0:
                    actions = []
                    rewards = []
                    valid_masks = []

                    for sample in self.voting_sample_queue:
                        if len(sample) >= 5:
                            _, _, final_act, rew, mask = sample
                            actions.append(final_act.item() if torch.is_tensor(final_act) else final_act)
                            rewards.append(rew.item() if torch.is_tensor(rew) else rew)
                            valid_masks.append(mask.sum().item() if torch.is_tensor(mask) else 0)

                    if actions:
                        action_counts = Counter(actions)
                        logger.info(f"Action distribution: {dict(action_counts)}")
                        logger.info(f"Reward range: [{min(rewards):.2f}, {max(rewards):.2f}]")
                        logger.info(f"Avg valid agents per sample: {sum(valid_masks)/len(valid_masks):.1f}")

                        if len(action_counts) == 1:
                            logger.info("WARNING: Only ONE class in queue! Model will collapse!")
                        elif len(action_counts) == 2:
                            counts = list(action_counts.values())
                            imbalance = max(counts) / min(counts)
                            if imbalance > 5:
                                logger.info(f"WARNING: Severe class imbalance: {imbalance:.1f}x")
                else:
                    logger.info("WARNING: Sample queue is EMPTY!")

            # 4. Test model inference with PROPER DEVICE HANDLING
            if queue_size > 0:
                with self.voting_collection_lock:
                    sample = list(self.voting_sample_queue)[0]

                q_vals, acts, final_act, rew, mask = sample

                self.voting_model.eval()
                with torch.no_grad():
                    # CRITICAL FIX: Ensure all tensors on same device as model
                    model_device = next(self.voting_model.parameters()).device

                    q_batch = q_vals.unsqueeze(0).to(model_device)
                    acts_batch = acts.unsqueeze(0).to(model_device)
                    mask_batch = mask.unsqueeze(0).to(model_device)

                    logger.info(f"Inference tensors on: {q_batch.device}")

                    inputs = encode_agent_outputs(q_batch, acts_batch)
                    logger.info(f"Encoded inputs shape: {inputs.shape}, device: {inputs.device}")

                    logits = self.voting_model(inputs, mask=mask_batch)
                    probs = F.softmax(logits, dim=-1)

                    logger.info(f"Test inference:")
                    logger.info(f"  Logits: {logits[0].cpu().numpy()}")
                    logger.info(f"  Probs: {probs[0].cpu().numpy()}")
                    logger.info(f"  Prediction: {logits.argmax().item()}")
                    logger.info(f"  Ground truth: {final_act.item() if torch.is_tensor(final_act) else final_act}")

            # 5. Training system checks
            logger.info(f"Batch processor exists: {hasattr(self, 'batch_processor')}")
            if hasattr(self, 'batch_processor'):
                logger.info(f"Batch processor running: {self.batch_processor.processing}")
                batch_stats = self.batch_processor.get_stats()
                logger.info(f"Batches processed: {batch_stats.get('batches_processed', 0)}")

            # 6. Optimizer state
            if hasattr(self, 'voting_optimizer'):
                lr = self.voting_optimizer.param_groups[0]['lr']
                logger.info(f"Optimizer learning rate: {lr}")

            logger.info("=" * 80)

        except Exception as e:
            logger.info(f"DIAGNOSTIC ERROR: {e}")
            import traceback
            traceback.print_exc()

    def _load_reward_history(self):
        """Load reward history from file"""
        if os.path.exists(self.REWARD_HISTORY_PATH):
            with open(self.REWARD_HISTORY_PATH, "r") as f:
                for line in f:
                    try:
                        self.reward_history.append(float(line.strip()))
                    except Exception:
                        continue
            logger.info(f"Loaded {len(self.reward_history)} rewards from history.")
        else:
            logger.warning(f"Reward history file not found at {self.REWARD_HISTORY_PATH}")

    def _load_meta_model(self):
        """Load meta model from file or GCS"""
        if self.gcs_bucket:
            blob = self.gcs_bucket.blob(f"{self.gcs_meta_dir}/meta_model.keras")
            if blob.exists():
                local_path = self.META_MODEL_PATH
                blob.download_to_filename(local_path)
                logger.info(f"Meta-model downloaded from GCS to {local_path}")

        if os.path.exists(self.META_MODEL_PATH):
            try:
                self.meta_model = tf.keras.models.load_model(self.META_MODEL_PATH)
                self.meta_model_trained = True
                logger.info("Meta-model loaded from disk.")
            except Exception as e:
                logger.info(f"Meta-model load failed: {e}")
        else:
            logger.warning(f"Meta-model path does not exist: {self.META_MODEL_PATH}")

    def save_state(self):
        """Save system state locally and to GCS"""
        try:
            ensure_dir(self.base_path)
            self.meta_model.save(self.META_MODEL_PATH)
            logger.info(f"Meta-model saved: {self.META_MODEL_PATH}")

            torch.save(self.voting_model.state_dict(), self.VOTING_MODEL_PATH)
            logger.info(f"Voting model saved: {self.VOTING_MODEL_PATH}")

            with open(self.REWARD_PATH, "wb") as f:
                pickle.dump(self.reward_history, f)
            logger.info(f"Reward history saved: {self.REWARD_PATH}")

            with open(self.META_DATA_PATH, "wb") as f:
                pickle.dump(self.meta_data, f)
            logger.info(f"Meta-data saved: {self.META_DATA_PATH}")

            # GCS Integration
            if self.gcs_bucket:
                zip_path = unique_tmp_path("IntegratedSignalSystem_state")
                with zipfile.ZipFile(zip_path, 'w', allowZip64=True) as zipf:
                    for filename in os.listdir(self.base_path):
                        file_path = os.path.join(self.base_path, filename)
                        zipf.write(file_path, arcname=filename)
                blob = self.gcs_bucket.blob(f"{self.gcs_meta_dir}/IntegratedSignalSystem_state.zip")
                blob.upload_from_filename(zip_path)
                logger.info(f"System state uploaded to GCS")

        except Exception as e:
            logger.error(f"IntegratedSignalSystem save failed: {e}")

    def get_system_status(self):
       """Enhanced system status with supervised learning metrics"""
       status = {
           'total_training_steps': self.total_agent_training_steps,
           'training_threshold': self.training_threshold,
           'meta_gating_enabled': self.meta_gating_enabled,
           'confirmation_required': self.confirmation_required,
           'remaining_steps': max(0, self.training_threshold - self.total_agent_training_steps),
           'agent_steps': self.agent_training_steps.copy(),
           'progress_percent': min(100, (self.total_agent_training_steps / self.training_threshold) * 100),
           'mode': 'full_protection' if self.meta_gating_enabled else 'fast_learning',
           'features_enabled': {
               'parallel_processing': True,
               'voting_transformer': True,
               'meta_model_gating': self.meta_gating_enabled,
               'confirmation_system': self.confirmation_required,
               'pullback_logic': True,
               'batch_reward_processor': True,
               'exit_price_tracking': True,
               'supervised_learning': True  # NEW
           }
       }

       # Batch processor stats with supervised learning
       if hasattr(self, 'batch_processor') and self.batch_processor:
           batch_stats = self.batch_processor.get_stats()
           status['batch_processor_stats'] = batch_stats

           if batch_stats['received'] > 0:
               status['exit_price_coverage'] = (
                   batch_stats['exit_prices_received'] / batch_stats['received'] * 100
               )
               status['supervised_accuracy'] = batch_stats.get('supervised_accuracy', 0.0)

       # Agent-level supervised learning statistics
       status['agent_supervised_stats'] = {}
       for agent_name, agent in self.agents.items():
           if hasattr(agent, 'get_supervised_learning_stats'):
               agent_stats = agent.get_supervised_learning_stats()
               if agent_stats:
                   status['agent_supervised_stats'][agent_name] = agent_stats

       return status

    async def _assign_reward(self, signal_key, reward):
        """Assign reward to partial experience"""
        await asyncio.get_event_loop().run_in_executor(None, self.process_reward, signal_key, reward)

    def _process_reward_message(self, reward_msg: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Parse, validate, and handle incoming reward messages"""
        try:
            # Validate reward message
            required_fields = ['signal_key', 'reward']
            if not all(field in reward_msg for field in required_fields):
                logger.warning("[RewardProcessor] Invalid reward message structure")
                return None

            signal_key = reward_msg['signal_key']
            reward_value = reward_msg['reward']
            exit_price = reward_msg.get('exit_price')

            processed = {
                'type': 'reward_update',
                'signal_key': signal_key,
                'reward': float(reward_value),
                'exit_price': float(exit_price) if exit_price is not None else None,
                'timestamp': time.time(),
                'agent_multipliers': reward_msg.get('agent_multipliers', {})
            }

            # âœ… Complete experience and queue for training
            handle_reward(self, signal_key, processed)

            logger.debug(f"[RewardProcessor] Processed reward for {signal_key}")
            return processed

        except Exception as e:
            logger.error(f"[RewardProcessor] Processing failed: {e}")
            return None

    # === PASTE THIS - REPLACE VALIDATION SECTION IN collect_voting_training_sample_supervised ===
    def collect_voting_training_sample_supervised(self, q_values_dict, actions_dict,
                                                 final_action, reward, was_correct):
        """
        Unified collection - ONLY accepts BUY/SELL (0/1)
        """
        # VALIDATION: Only accept BUY/SELL
        if final_action not in [0, 1]:
            logger.debug(f"[VOTING] Skipping invalid final_action={final_action} (must be 0 or 1)")
            return

        # Validate agent actions
        invalid_actions = [a for a in actions_dict.values() if a not in [0, 1]]
        if invalid_actions:
            logger.debug(f"[VOTING] Skipping due to invalid agent actions (must be 0 or 1)")
            return

        with self.voting_collection_lock:
            try:
                all_agent_names = sorted(self.agents.keys())
                max_agents = len(all_agent_names)

                q_values_padded = torch.zeros(max_agents, 2, dtype=torch.float32)
                actions_padded = torch.zeros(max_agents, dtype=torch.long)
                agent_mask = torch.zeros(max_agents, dtype=torch.bool)

                valid_agents = sorted(set(q_values_dict.keys()) & set(actions_dict.keys()))

                if not valid_agents:
                    logger.debug("[VOTING] No valid agents in sample")
                    return

                for i, agent_name in enumerate(all_agent_names):
                    if agent_name in valid_agents:
                        q_val = q_values_dict[agent_name]

                        if not isinstance(q_val, (np.ndarray, list, torch.Tensor)):
                            continue

                        q_val_array = np.array(q_val, dtype=np.float32)

                        # VALIDATION: Ensure Q-values have exactly 2 elements
                        if len(q_val_array) != 2:
                            logger.warning(f"[VOTING] Agent {agent_name} has {len(q_val_array)} Q-values, expected 2")
                            continue

                        if np.any(np.isnan(q_val_array)) or np.any(np.isinf(q_val_array)):
                            continue

                        agent_action = actions_dict[agent_name]

                        # VALIDATION: Ensure action is 0 or 1
                        if agent_action not in [0, 1]:
                            logger.warning(f"[VOTING] Agent {agent_name} has invalid action {agent_action}")
                            continue

                        q_values_padded[i] = torch.tensor(q_val_array, dtype=torch.float32)
                        actions_padded[i] = torch.tensor(agent_action, dtype=torch.long)
                        agent_mask[i] = True

                if not agent_mask.any():
                    logger.debug("[VOTING] No valid agent data after processing")
                    return

                final_action_tensor = torch.tensor(final_action, dtype=torch.long)
                reward_tensor = torch.tensor(reward, dtype=torch.float32)

                self.voting_sample_queue.append((
                    q_values_padded,
                    actions_padded,
                    final_action_tensor,
                    reward_tensor,
                    agent_mask
                ))

                queue_size = len(self.voting_sample_queue)

                if queue_size % 10 == 0:
                    logger.info(f"[VOTING QUEUE] Size: {queue_size}/500")

                if queue_size >= 64:
                    logger.critical(f"[VOTING QUEUE] THRESHOLD REACHED - Triggering training!")
                    asyncio.create_task(self._trigger_voting_training())

            except Exception as e:
                logger.error(f"[VOTING] Collection error: {e}")
                traceback.print_exc()

    def _train_voting_model_batch(self, samples):
        """Training with proper device handling"""

        logger.critical(f"[VOTING] _train_voting_model_batch ENTRY with {len(samples)} samples")

        if not samples or len(samples) < 3:
            logger.critical(f"[VOTING] EARLY EXIT: Not enough samples")
            return

        # CRITICAL: Ensure model on correct device BEFORE training
        model_device = next(self.voting_model.parameters()).device
        logger.critical(f"[VOTING] Model device: {model_device}, System device: {self.device}")

        if str(model_device) != str(self.device):
            logger.critical(f"[VOTING] WARNING: Moving model to {self.device}")
            self.voting_model = self.voting_model.to(self.device)
            model_device = self.device

        with self.voting_training_lock:  # â† FIXED: Changed from self.system.voting_training_lock
            try:
                logger.critical(f"[VOTING] Acquired training lock")

                # Unpack and validate samples
                q_values_batch = []
                actions_batch = []
                final_actions_batch = []
                rewards_batch = []
                masks_batch = []

                for idx, sample_tuple in enumerate(samples):
                    if len(sample_tuple) != 5:
                        continue

                    q_vals, acts, final_act, rew, mask = sample_tuple

                    if not isinstance(final_act, torch.Tensor):
                        final_act = torch.tensor(final_act, dtype=torch.long)

                    if final_act.item() not in [0, 1]:
                        continue

                    q_values_batch.append(q_vals)
                    actions_batch.append(acts)
                    final_actions_batch.append(final_act)
                    rewards_batch.append(rew)
                    masks_batch.append(mask)

                logger.critical(f"[VOTING] Valid samples: {len(q_values_batch)}")

                if not q_values_batch:
                    logger.critical("[VOTING] No valid samples")
                    return

                # Stack tensors and MOVE TO CORRECT DEVICE
                q_values_tensor = torch.stack(q_values_batch).to(model_device)
                actions_tensor = torch.stack(actions_batch).to(model_device)
                final_actions_tensor = torch.stack(final_actions_batch).to(model_device)
                masks_tensor = torch.stack(masks_batch).to(model_device)

                logger.critical(f"[VOTING] Tensors on device: {q_values_tensor.device}")

                # Class distribution
                class_counts = torch.bincount(final_actions_tensor, minlength=2).float()
                logger.critical(f"[VOTING] Class distribution - BUY: {class_counts[0]:.0f}, SELL: {class_counts[1]:.0f}")

                if class_counts.min() == 0:
                    logger.critical("[VOTING] Only one class present!")
                    return

                # Calculate class weights on same device
                class_weights = 1.0 / (class_counts + 1e-6)
                class_weights = class_weights / class_weights.sum()
                class_weights = class_weights.to(model_device)

                # Train/val split
                n_samples = len(q_values_batch)
                n_train = max(3, int(0.8 * n_samples))

                indices = torch.randperm(n_samples)
                train_idx = indices[:n_train]

                train_q = q_values_tensor[train_idx]
                train_a = actions_tensor[train_idx]
                train_fa = final_actions_tensor[train_idx]
                train_m = masks_tensor[train_idx]

                dataset = torch.utils.data.TensorDataset(train_q, train_a, train_fa, train_m)
                dataloader = DataLoader(dataset, batch_size=min(32, n_train), shuffle=True)

                logger.critical(f"[VOTING] Starting 3 epochs of training...")

                self.voting_model.train()  # â† FIXED: Changed from self.system.voting_model

                for epoch in range(3):
                    epoch_loss = 0.0
                    epoch_correct = 0
                    epoch_samples = 0

                    for batch_q, batch_a, batch_fa, batch_m in dataloader:
                        # All tensors already on correct device from dataset
                        inputs = encode_agent_outputs(batch_q, batch_a)
                        logits = self.voting_model(inputs, mask=batch_m)  # â† FIXED
                        loss = F.cross_entropy(logits, batch_fa, weight=class_weights)

                        self.voting_optimizer.zero_grad(set_to_none=True)  # â† FIXED + memory optimization
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.voting_model.parameters(), 1.0)  # â† FIXED
                        self.voting_optimizer.step()  # â† FIXED

                        epoch_loss += loss.item()
                        predictions = logits.argmax(dim=-1)
                        epoch_correct += (predictions == batch_fa).sum().item()
                        epoch_samples += len(batch_fa)

                        # MEMORY CLEANUP
                        del inputs, logits, loss, predictions

                    epoch_acc = epoch_correct / epoch_samples if epoch_samples > 0 else 0
                    logger.critical(f"[VOTING] Epoch {epoch+1}/3: Loss={epoch_loss/len(dataloader):.4f}, Acc={epoch_acc:.1%}")

                # AGGRESSIVE CLEANUP
                del q_values_tensor, actions_tensor, final_actions_tensor, masks_tensor
                del train_q, train_a, train_fa, train_m, dataset, dataloader

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                logger.critical("[VOTING] TRAINING COMPLETE")

            except Exception as e:
                logger.critical(f"[VOTING] ERROR: {e}")
                traceback.print_exc()

                # Emergency cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    async def _health_monitor(self):
        """Health monitoring and recovery - ADD TO IntegratedSignalSystem CLASS"""
        while True:
            try:
                await asyncio.sleep(30)  # Check every 30 seconds

                # Check directory structure
                if not os.path.exists('./saves'):
                    logger.warning("Missing ./saves directory - recreating...")
                    emergency_create_directories()

                # Check Ably connection if available
                if self.ably_stabilizer and self.ably_stabilizer.should_check_connection():
                    if not self.ably_stabilizer.is_connected():
                        logger.warning("Ably connection lost")

                # Check task count
                task_count = self.safe_task_manager.get_task_count()
                if task_count > 200:  # Too many tasks
                    logger.warning(f"High task count: {task_count} - may need restart")

                # Memory cleanup
                if hasattr(self, 'partial_experiences'):
                    exp_count = len(self.partial_experiences)
                    if exp_count > 10000:
                        logger.warning(f"Large partial_experiences: {exp_count}")

            except Exception as e:
                logger.debug(f"Health monitor error: {e}")

    def training_health_check(self):
        # Get Discord stats safely
        stats = {}
        try:
            stats = logger.get_discord_stats()
        except Exception as e:
            logger.debug(f"Could not get Discord stats: {e}")
            return

        # Check Discord backlog
        unsent_logs = stats.get('unsent_logs_count', 0)
        if unsent_logs > 50:
            logger.warning("âš ï¸ Discord backlog may be blocking training")

        # Check if training threads are alive
        for agent_name, agent in self.agents.items():
            # Check if the agent has a training thread and if it's alive
            if hasattr(agent, 'training_thread') and agent.training_thread is not None:
                if not agent.training_thread.is_alive():
                    logger.error(f"âŒ Training thread dead for {agent_name}")
            else:
                # If there's no training thread, that might be normal (e.g., not using threaded training)
                pass

    def prepare_meta_model_input(
        self,
        q_values_dict, actions_dict, state, voting_pred,
        close_price=0.0,
        distance_to_nearest_support=0.0,
        distance_to_nearest_resistance=0.0,
        near_support=False,
        near_resistance=False,
        distance_to_stop_loss=0.0,
        support_strength=0.0,
        resistance_strength=0.0
    ):
        """Prepare meta-model input with strict shape validation"""
        try:
            # Process state: ensure exactly 15 dimensions
            state = np.array(state, dtype=np.float32).flatten()
            if np.any(np.isnan(state)):
                state = np.nan_to_num(state, nan=0.0)
            if state.shape[0] > 15:
                state = state[:15]
            elif state.shape[0] < 15:
                state = np.pad(state, (0, 15 - state.shape[0]), mode='constant')

            # Fixed: Explicit dict validation to avoid numpy array boolean ambiguity
            if not isinstance(q_values_dict, dict):
                logger.error(f"q_values_dict must be dict, got {type(q_values_dict)} in prepare_meta_model_input")
                return np.zeros((1, 30), dtype=np.float32)

            if len(q_values_dict) == 0:
                logger.error("q_values_dict is empty in prepare_meta_model_input")
                return np.zeros((1, 30), dtype=np.float32)

            if not isinstance(actions_dict, dict):
                logger.error(f"actions_dict must be dict, got {type(actions_dict)} in prepare_meta_model_input")
                return np.zeros((1, 30), dtype=np.float32)

            if len(actions_dict) == 0:
                logger.error("actions_dict is empty in prepare_meta_model_input")
                return np.zeros((1, 30), dtype=np.float32)

            # Get first agent's data
            first_agent = next(iter(q_values_dict))
            q = np.array(q_values_dict[first_agent], dtype=np.float32).flatten()
            if np.any(np.isnan(q)):
                q = np.nan_to_num(q, nan=0.0)
            action = actions_dict[first_agent]

            # One-hot encode action (2 dimensions)
            onehot = np.zeros(2, dtype=np.float32)
            if 0 <= action < 2:
                onehot[action] = 1.0

            # Q-value statistics (3 dimensions)
            q_stats = [float(np.max(q)), float(np.min(q)), float(np.std(q))]

            # One-hot encode voting prediction (2 dimensions)
            voting_onehot = np.zeros(2, dtype=np.float32)
            if voting_pred in [0, 1]:
                voting_onehot[voting_pred] = 1.0

            # Extra features (8 dimensions)
            meta_extra_features = [
                float(close_price),
                float(distance_to_nearest_support),
                float(distance_to_nearest_resistance),
                float(near_support),
                float(near_resistance),
                float(distance_to_stop_loss),
                float(support_strength),
                float(resistance_strength),
            ]

            # Concatenate all parts: 15 + 2 + 3 + 2 + 8 = 30
            meta_input = np.concatenate([
                state,                  # 15
                onehot,                 # 2
                q_stats,                # 3
                voting_onehot,          # 2
                meta_extra_features     # 8
            ]).astype(np.float32)

            # Final validation
            if np.any(np.isnan(meta_input)):
                meta_input = np.nan_to_num(meta_input, nan=0.0)

            if meta_input.shape[0] != 30:
                logger.error(f"Meta-model input shape error: expected 30, got {meta_input.shape[0]}")
                meta_input = np.pad(meta_input, (0, max(0, 30 - meta_input.shape[0])), mode='constant')[:30]

            # Return as (1, 30) for batch compatibility
            return meta_input.reshape(1, 30)

        except Exception as e:
            logger.error(f"âŒ Error in prepare_meta_model_input: {e}")
            import traceback
            traceback.print_exc()
            return np.zeros((1, 30), dtype=np.float32)

    def _on_new_reward(self, reward):
        self._save_reward(reward)
        if len(self.reward_history) % 30 == 0:
            recent = list(self.reward_history)[-10:]
            avg = sum(recent) / len(recent) if recent else 0
            self.discord_notifier.notify_reward_update(
                count=len(self.reward_history),
                latest=reward,
                avg=avg
            )

    def _save_reward(self, reward):
        """Save reward to history"""
        self.reward_history.append(reward)
        os.makedirs(os.path.dirname(self.REWARD_HISTORY_PATH), exist_ok=True)
        with open(self.REWARD_HISTORY_PATH, "a") as f:
            f.write(f"{reward}\n")

    def _plot_rewards(self):
        pass

    def _send_email_with_plot(self, subject, body):
        pass

    def load_state(self):
        """Load system state from files and GCS"""
        loaded_components = []

        # GCS Integration
        if self.gcs_bucket:
            tmp_zip = unique_tmp_path("system")
            try:
                blob = self.gcs_bucket.blob(f"{self.gcs_meta_dir}/IntegratedSignalSystem_state.zip")
                if blob.exists():
                    blob.download_to_filename(tmp_zip)
                    safe_unzip(tmp_zip, self.base_path)
                    logger.info(f"System state from GCS unzipped to {self.base_path}")
                else:
                    logger.info(f"GCS system state blob does not exist.")
            except Exception as e:
                logger.info(f"GCS system state load failed: {e}")

        # Load meta model
        meta_model, meta_loaded = load_keras_model(self.META_MODEL_PATH)
        if meta_loaded:
            self.meta_model = meta_model
            self.meta_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
            self.meta_model.compile(
                optimizer=self.meta_optimizer,
                loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),
                metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
            )
            self.meta_model_trained = True
            loaded_components.append("meta_model")

        # Load voting model
        if load_torch_state_dict(self.voting_model, self.VOTING_MODEL_PATH, self.device):
            loaded_components.append("voting_model")

        # Load reward history
        if os.path.exists(self.REWARD_PATH):
            try:
                with open(self.REWARD_PATH, "rb") as f:
                    self.reward_history = pickle.load(f)
                loaded_components.append("reward_history")
                logger.info(f"Reward history loaded: {self.REWARD_PATH}")
            except Exception as e:
                logger.error(f"Failed to load reward history: {e}")
        else:
            logger.warning(f"Reward history not found: {self.REWARD_PATH}")

        # Load meta data
        if os.path.exists(self.META_DATA_PATH):
            try:
                with open(self.META_DATA_PATH, "rb") as f:
                    self.meta_data = pickle.load(f)
                loaded_components.append("meta_data")
                logger.info(f"Meta-data loaded: {self.META_DATA_PATH}")
            except Exception as e:
                logger.error(f"Failed to load meta-data: {e}")
        else:
            logger.warning(f"Meta-data not found: {self.META_DATA_PATH}")

        if loaded_components:
            logger.info(f"System loaded: {', '.join(loaded_components)}")
            return True
        else:
            logger.warning(f"No system components loaded, initializing new models and saving.")
            self._init_new_models()
            self.save_state()
            return False

    def _init_new_models(self):
        """Initialize new models when none found"""
        self.meta_model_trained = False
        self.meta_data = []
        self.reward_history = []

        logger.info("New models initialized for IntegratedSignalSystem.")

    def stop(self):
        """Stop the system gracefully"""
        logger.info("System shutdown initiated...")

        # Graceful Shutdown: Save All State to GCS
        try:
            logger.info("Performing final autosave to GCS before shutdown...")
            if hasattr(self, "autosave_manager") and self.autosave_manager:
                self.autosave_manager.save_all()
                logger.info("Final autosave complete.")
                self.autosave_manager.stop()
                logger.info("AutosaveManager stopped.")
            else:
                # Fallback: Save all agents and system manually
                for agent in self.agents.values():
                    try:
                        agent.save_state(
                            bucket=self.gcs_bucket,
                            gcs_path=f"{self.gcs_meta_dir}/agents/{agent.name}_agent_state.zip",
                            local_ckpt_path=f"/tmp/{agent.name}_agent_state.zip"
                        )
                        logger.info(f"Agent '{agent.name}' state saved to GCS (manual fallback).")
                    except Exception as e:
                        logger.error(f"Error saving agent '{agent.name}' to GCS: {e}")

                try:
                    self.save_state()  # Save system state
                    logger.info("System state saved to GCS (manual fallback).")
                except Exception as e:
                    logger.error(f"Error saving system state to GCS: {e}")

        except Exception as e:
            logger.error(f"Error during final autosave: {e}")

        # Stop event loop
        try:
            if hasattr(self, "loop") and self.loop and self.loop.is_running():
                self.loop.stop()
                logger.info("Asyncio event loop stopped.")
        except Exception as e:
            logger.error(f"Error stopping event loop: {e}")

        logger.info("System shutdown complete.")
        os._exit(0)

        if hasattr(self, 'discord_sender'):
            self.discord_sender.stop()

    def train_supervised(self, state, action, reward, next_state, done=False):
        """
        FIXED: No 'or' operators with arrays
        """
        try:
            # Verify quantum bridge exists
            if not hasattr(self, 'quantum_bridge') or self.quantum_bridge is None:
                logger.error("train_supervised: Quantum bridge not initialized")
                return None

            # Store experiences for each agent
            experiences_stored = 0
            agent_names = list(self.agents.keys())

            for agent_name in agent_names:
                try:
                    # === EXTRACT STATE (FIXED: No 'or' with arrays) ===
                    agent_state = None

                    if isinstance(state, dict):
                        agent_states = state.get(agent_name, {})

                        if isinstance(agent_states, dict):
                          
                                                            
                            # âœ… FIX: Check each timeframe explicitly, no 'or'
                            agent_state = agent_states.get('10m')
                            if agent_state is None:
                                agent_state = agent_states.get('5m')
                            if agent_state is None:
                                agent_state = agent_states.get('xxl')
                            if agent_state is None:
                                agent_state = agent_states.get('xl')
                            if agent_state is None:
                                agent_state = agent_states.get('l')
                            if agent_state is None:
                                agent_state = agent_states.get('m')
                            if agent_state is None:
                                agent_state = agent_states.get('s')
                            if agent_state is None:
                                agent_state = agent_states.get('xs')

                            # If still None, try to get any value
                            if agent_state is None:
                                try:
                                    agent_state = next(iter(agent_states.values()))
                                except StopIteration:
                                    pass
                        else:
                            agent_state = agent_states
                    else:
                        agent_state = state

                    # Validate state
                    if agent_state is None:
                        continue

                    # Convert to numpy
                    if not isinstance(agent_state, np.ndarray):
                        try:
                            agent_state = np.array(agent_state, dtype=np.float32)
                        except:
                            continue

                    agent_state = agent_state.flatten()

                    # Check validity
                    if agent_state.shape[0] == 0:
                        continue

                    # === EXTRACT ACTION ===
                    agent_action = 0

                    if isinstance(action, dict):
                        act = action.get(agent_name, 0)
                        if isinstance(act, (np.ndarray, list, tuple)):
                            try:
                                agent_action = int(act[0]) if len(act) > 0 else 0
                            except:
                                agent_action = 0
                        else:
                            try:
                                agent_action = int(act)
                            except:
                                agent_action = 0

                    elif isinstance(action, (list, tuple, np.ndarray)):
                        try:
                            agent_idx = agent_names.index(agent_name)
                            if agent_idx < len(action):
                                act = action[agent_idx]
                                if isinstance(act, (np.ndarray, list, tuple)):
                                    agent_action = int(act[0]) if len(act) > 0 else 0
                                else:
                                    agent_action = int(act)
                        except:
                            agent_action = 0
                    else:
                        try:
                            if isinstance(action, (np.ndarray, list, tuple)):
                                agent_action = int(action[0]) if len(action) > 0 else 0
                            else:
                                agent_action = int(action)
                        except:
                            agent_action = 0

                    # Clamp
                    agent_action = max(0, min(1, agent_action))

                    # === EXTRACT NEXT_STATE (FIXED: No 'or' with arrays) ===
                    agent_next_state = None

                    if isinstance(next_state, dict):
                        agent_next_states = next_state.get(agent_name, {})

                        if isinstance(agent_next_states, dict):
                            # âœ… FIX: Check each timeframe explicitly
                            # âœ… FIX: Check each next timeframe explicitly, no 'or'
                            agent_next_state = agent_next_states.get('10m')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('5m')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('xxl')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('xl')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('l')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('m')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('s')
                            if agent_next_state is None:
                                agent_next_state = agent_next_states.get('xs')

                            if agent_next_state is None:
                                try:
                                    agent_next_state = next(iter(agent_next_states.values()))
                                except StopIteration:
                                    pass
                        else:
                            agent_next_state = agent_next_states
                    elif next_state is not None:
                        agent_next_state = next_state

                    # Fallback to current state
                    if agent_next_state is None:
                        agent_next_state = agent_state

                    # Convert to numpy
                    if not isinstance(agent_next_state, np.ndarray):
                        try:
                            agent_next_state = np.array(agent_next_state, dtype=np.float32)
                        except:
                            agent_next_state = agent_state

                    agent_next_state = agent_next_state.flatten()

                    # === STORE EXPERIENCE ===
                    success = self.quantum_bridge.store_experience_for_agent(
                        agent_name=agent_name,
                        state=agent_state,
                        action=agent_action,
                        reward=float(reward),
                        next_state=agent_next_state,
                        done=bool(done)
                    )

                    if success:
                        experiences_stored += 1

                except Exception as e:
                    logger.error(f"[{agent_name}] Failed to store experience: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

            # === TRIGGER TRAINING ===
            if experiences_stored > 0:
                buffer = None

                try:
                    if hasattr(self.quantum_bridge, 'hybrid_buffer'):
                        if self.quantum_bridge.hybrid_buffer is not None:
                            buffer = self.quantum_bridge.hybrid_buffer
                except:
                    pass

                if buffer is None:
                    try:
                        if hasattr(self.quantum_bridge, 'quantum_trainer'):
                            if self.quantum_bridge.quantum_trainer is not None:
                                if hasattr(self.quantum_bridge.quantum_trainer, 'buffer'):
                                    buffer = self.quantum_bridge.quantum_trainer.buffer
                    except:
                        pass

                if buffer is not None:
                    try:
                        buffer_size = len(buffer)

                        if buffer_size >= 64:
                            logger.info(f"ğŸ”¥ TRIGGERING TRAINING: Buffer={buffer_size}")

                            try:
                                if hasattr(self.quantum_bridge, 'quantum_trainer'):
                                    if self.quantum_bridge.quantum_trainer is not None:
                                        metrics = self.quantum_bridge.quantum_trainer.train_step()

                                        if metrics is not None:
                                            logger.info(
                                                f"ğŸ“Š Loss={metrics.get('total_loss', 0):.4f}, "
                                                f"Actor={metrics.get('actor_loss', 0):.4f}, "
                                                f"Critic={metrics.get('critic_loss', 0):.4f}"
                                            )
                                            return metrics
                            except Exception as train_err:
                                logger.error(f"âŒ Training error: {train_err}")
                                import traceback
                                traceback.print_exc()
                    except:
                        pass

            return None

        except Exception as e:
            logger.error(f"train_supervised failed: {e}")
            import traceback
            traceback.print_exc()
            return None

    def training_health_check(self):
        """Enhanced health check for Colab visibility"""
        try:
            # Get Discord stats safely
            stats = {}
            try:
                stats = logger.get_discord_stats()
            except Exception as e:
                print(f"ğŸ”§ [HEALTH CHECK] Could not get Discord stats: {e}")
                return

            # Check Discord backlog
            unsent_logs = stats.get('unsent_logs_count', 0)
            if unsent_logs > 50:
                warning_msg = f"âš ï¸ Discord backlog may be blocking training: {unsent_logs} unsent logs"
                print(f"ğŸ”§ [HEALTH CHECK] {warning_msg}")
                logger.warning(warning_msg)

            # Check training threads
            dead_threads = []
            for agent_name, agent in self.agents.items():
                if hasattr(agent, 'training_thread') and agent.training_thread:
                    if not agent.training_thread.is_alive():
                        dead_threads.append(agent_name)
                        error_msg = f"âŒ Training thread dead for {agent_name}"
                        print(f"ğŸ”§ [HEALTH CHECK] {error_msg}")
                        logger.error(error_msg)

            # Colab summary
            if dead_threads:
                print(f"ğŸ”§ [HEALTH CHECK] âŒ DEAD THREADS: {', '.join(dead_threads)}")
            else:
                print(f"ğŸ”§ [HEALTH CHECK] âœ… All {len(self.agents)} training threads alive")

            # Force Colab output
            sys.stdout.flush()

        except Exception as e:
            print(f"ğŸ”§ [HEALTH CHECK] Error: {e}")
            sys.stdout.flush()

TIMEFRAMES = {
    # === High-Frequency Zone (Volatility Capture) ===
    'xs': 5,     # tick
    's': 10,     # ultra
    'm': 20,     # fast

    # === Critical Trading Zones ===
    'l': 30,     # scalp
    'xl': 60,    # 1min
    'xxl': 120,  # 2min

    # === Structure & Regime Detection ===
    '5m': 300,   # 5min
    '10m': 600,  # 10min
}


# Replace with your actual Ably API key
ABLY_API_KEY = "4vT80g.E0lfvg:reqqX942--QJVafOQsgRDWsBXIDtgDxg51szTmLkIeM"

# ... any other imports ...
# --- Your imports for TimeframeAgent, IntegratedSignalSystem, AutosaveManager, etc. ---
# from timeframe_agent import TimeframeAgent
# from IntegratedSignalSystem import IntegratedSignalSystem
# from autosave_manager import AutosaveManager

def noop(*args, **kwargs): pass

# Patch all save methods globally
TimeframeAgent.save_state = noop
TimeframeAgent.save = noop
IntegratedSignalSystem.save_state = noop
IntegratedSignalSystem.save_agent = noop
AutosaveManager.save_all = noop
AutosaveManager.save_agent_to_gcs = noop
AutosaveManager.save_meta_to_gcs = noop
TD3Agent.save_state = noop
TD3Agent.save_models = noop
TD3Agent.save_models_to_gcs = noop

def noop_load_false(*args, **kwargs): return False
def noop_load_none(*args, **kwargs): return None

TimeframeAgent.load_state = noop_load_false
TimeframeAgent.load = noop_load_false
TimeframeAgent._load_models = noop_load_none
TimeframeAgent._load_replay_buffer = noop_load_none

IntegratedSignalSystem.load_state = noop_load_false
IntegratedSignalSystem._load_meta_model = noop_load_none
IntegratedSignalSystem._load_reward_history = noop_load_none

AutosaveManager.load_agent_to_gcs = noop_load_false
AutosaveManager.load_meta_to_gcs = noop_load_false

TD3Agent.load_state = noop_load_false
TD3Agent.load_models = noop_load_false
TD3Agent.load_models_from_gcs = noop_load_false

async def keep_alive():
    while True:
        logger.info("ğŸ’“ System is alive...")
        await asyncio.sleep(5)

ABLY_API_KEY =  "4vT80g.E0lfvg:reqqX942--QJVafOQsgRDWsBXIDtgDxg51szTmLkIeM"

async def keep_alive():
    while True:
        print("ğŸ’“ alive...")
        await asyncio.sleep(5)

# === Timeframes used by the system ===
TIMEFRAMES = ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']

# ============================================================================
# INFERENCE DIAGNOSTICS ATTACHER (SAFE, PERMANENT)
# ============================================================================
def attach_inference_diagnostics(system):
    """
    Attaches print-based diagnostics to live inference components.
    Safe to call once after system initialization.
    """

    from collections import Counter
    import numpy as np
    import time
    import traceback

    def print_diag(msg, level="INFO"):
        ts = time.strftime("%H:%M:%S")
        prefix = {
            "INFO": "â„¹ï¸",
            "WARNING": "âš ï¸",
            "ERROR": "âŒ",
            "CRITICAL": "ğŸ”´",
            "SUCCESS": "âœ…"
        }.get(level, "â€¢")
        print(f"[{ts}] {prefix} {msg}")

    print("\n" + "=" * 70)
    print("ATTACHING INFERENCE DIAGNOSTICS")
    print("=" * 70)

    # ----------------------------------------------------------------------
    # PATCH 1: quantum_bridge.predict_single_agent
    # ----------------------------------------------------------------------
    if hasattr(system, 'quantum_bridge'):
        qb = system.quantum_bridge

        if not hasattr(qb, '_orig_predict_single_agent'):
            qb._orig_predict_single_agent = qb.predict_single_agent

            def patched_predict_single_agent(agent_name, states_dict=None):
                try:
                    if qb.quantum_system is None:
                        print_diag(f"[{agent_name}] quantum_system is None!", "CRITICAL")
                        return np.array([0.5, 0.5], dtype=np.float32)

                    if qb.state_cache is None:
                        print_diag(f"[{agent_name}] state_cache is None!", "CRITICAL")
                        return np.array([0.5, 0.5], dtype=np.float32)

                    q_vals = qb._orig_predict_single_agent(agent_name, states_dict)

                    if np.allclose(q_vals, [0.5, 0.5], atol=1e-3):
                        print_diag(
                            f"[{agent_name}] DEFAULT Q-values [0.5, 0.5] â€“ FALLBACK ACTIVE",
                            "WARNING"
                        )
                    else:
                        action = "BUY" if q_vals[0] > q_vals[1] else "SELL"
                        print_diag(
                            f"[{agent_name}] Q=[{q_vals[0]:.4f}, {q_vals[1]:.4f}] â†’ {action}",
                            "SUCCESS"
                        )

                    return q_vals

                except Exception as e:
                    print_diag(f"[{agent_name}] PREDICTION EXCEPTION: {e}", "CRITICAL")
                    traceback.print_exc()
                    return np.array([0.5, 0.5], dtype=np.float32)

            qb.predict_single_agent = patched_predict_single_agent
            print_diag("âœ… Patched quantum_bridge.predict_single_agent", "SUCCESS")

    # ----------------------------------------------------------------------
    # PATCH 2: system._fast_voting_predict
    # ----------------------------------------------------------------------
    if hasattr(system, '_fast_voting_predict'):
        if not hasattr(system, '_orig_fast_voting_predict'):
            system._orig_fast_voting_predict = system._fast_voting_predict

            def patched_voting(q_vals, actions):
                try:
                    if actions:
                        counts = Counter(actions.values())
                        total = len(actions)

                        buy_pct = counts.get(0, 0) / max(total, 1) * 100
                        sell_pct = counts.get(1, 0) / max(total, 1) * 100

                        print_diag(
                            f"ğŸ—³ï¸ PRE-VOTE: BUY={buy_pct:.1f}%, SELL={sell_pct:.1f}% {dict(counts)}",
                            "INFO"
                        )

                        if buy_pct >= 87.5 or sell_pct >= 87.5:
                            print_diag("âš ï¸ ACTION HOMOGENIZATION DETECTED", "WARNING")

                            if q_vals:
                                default_q = sum(
                                    1 for q in q_vals.values()
                                    if isinstance(q, np.ndarray)
                                    and np.allclose(q, [0.5, 0.5], atol=1e-3)
                                )
                                print_diag(
                                    f"   Default Q-values: {default_q}/{len(q_vals)} agents",
                                    "WARNING"
                                )

                    result, logits = system._orig_fast_voting_predict(q_vals, actions)
                    action_name = ["BUY", "SELL", "HOLD"][result] if result in (0, 1, 2) else f"UNK({result})"

                    print_diag(f"âœ… FINAL VOTE: {action_name}", "SUCCESS")
                    return result, logits

                except Exception as e:
                    print_diag(f"VOTING EXCEPTION: {e}", "CRITICAL")
                    traceback.print_exc()
                    raise

            system._fast_voting_predict = patched_voting
            print_diag("âœ… Patched _fast_voting_predict", "SUCCESS")

    # ----------------------------------------------------------------------
    # PATCH 3: signal publishing
    # ----------------------------------------------------------------------
    if hasattr(system, '_publish_direct'):
        if not hasattr(system, '_orig_publish_direct'):
            system._orig_publish_direct = system._publish_direct

            async def patched_publish(*args, **kwargs):
                try:
                    final_action = args[0] if args else kwargs.get("final_action_str", "UNKNOWN")
                    price = args[1] if len(args) > 1 else kwargs.get("price", 0)

                    print_diag(f"ğŸ“¡ PUBLISHING SIGNAL: {final_action} @ {price}", "INFO")
                    result = await system._orig_publish_direct(*args, **kwargs)
                    print_diag("âœ… Signal published", "SUCCESS")
                    return result

                except Exception as e:
                    print_diag(f"âŒ PUBLISH FAILED: {e}", "CRITICAL")
                    traceback.print_exc()
                    raise

            system._publish_direct = patched_publish
            print_diag("âœ… Patched _publish_direct", "SUCCESS")

    print("=" * 70)
    print("INFERENCE DIAGNOSTICS ACTIVE")
    print("=" * 70 + "\n")


def initialize_three_tier_gpu_system(IntegratedSignalSystem):
    """
    Initialize three-tier GPU processing system for existing trading system

    USAGE: Call this function after creating your IntegratedSignalSystem
    """
    try:
        logger.info("Initializing three-tier GPU processing system...")

        # Create coordinator with agents from trading system
        coordinator = ThreeTierCoordinator(
            agents_registry=IntegratedSignalSystem.agents,
            ingestion_workers=4,
            gpu_workers=8,
            batch_size=64
        )

        # Create integration manager
        integration_manager = TradingSystemIntegrationManager(
            IntegratedSignalSystem=IntegratedSignalSystem,
            three_tier_coordinator=coordinator
        )

        # Start the system
        integration_manager.start()

        # Add integration manager to trading system for access
        IntegratedSignalSystem.three_tier_integration = integration_manager

        logger.info("Three-tier GPU processing system initialized successfully")
        return integration_manager

    except Exception as e:
        logger.error(f"Failed to initialize three-tier GPU system: {e}")
        raise

def submit_features_to_gpu_tier(integration_manager, agent_name, features, priority=3):
    """
    Submit features to three-tier GPU processing system

    USAGE: Replace direct agent.update_features() calls with this function
    """
    return integration_manager.coordinator.submit_feature_message(
        agent_name=agent_name,
        features=features,
        priority=priority
    )

def get_gpu_system_stats(integration_manager):
    """
    Get comprehensive statistics from three-tier GPU system

    USAGE: Call this to monitor system performance
    """
    return integration_manager.get_integration_stats()

# ============================================================================
# VALIDATION AND MONITORING UTILITIES
# ============================================================================

def validate_three_tier_integration(trading_system):
    """Validate that three-tier integration is working correctly"""
    checks = {
        'three_tier_integration_exists': hasattr(trading_system, 'three_tier_integration'),
        'coordinator_running': False,
        'all_tiers_running': False,
        'agents_registered': False,
        'processing_enabled': False
    }

    if checks['three_tier_integration_exists']:
        integration = trading_system.three_tier_integration

        # Check if coordinator is running
        checks['coordinator_running'] = integration.running

        # Check if all tiers are running
        tier_status = integration.coordinator.system_stats['tier_status']
        checks['all_tiers_running'] = all(
            status == 'running' for status in tier_status.values()
        )

        # Check if agents are registered
        checks['agents_registered'] = len(integration.agent_registry) > 0

        # Check if processing is enabled
        checks['processing_enabled'] = (
            integration.feature_processing_enabled and
            integration.reward_processing_enabled and
            integration.signal_generation_enabled
        )

    all_passed = all(checks.values())

    logger.info("=== THREE-TIER GPU INTEGRATION VALIDATION ===")
    for check, passed in checks.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        logger.info(f"{status} {check.replace('_', ' ').title()}")

    if all_passed:
        logger.info("ğŸ¯ ALL THREE-TIER INTEGRATION CHECKS PASSED")
    else:
        logger.info("âš ï¸ SOME THREE-TIER INTEGRATION CHECKS FAILED")

    return all_passed

def start_three_tier_monitoring(integration_manager, interval_seconds=30):
    """Start monitoring thread for three-tier system"""
    def monitor():
        while integration_manager.running:
            try:
                stats = integration_manager.get_integration_stats()

                # Log key metrics
                integration_stats = stats['integration']
                coordinator_stats = stats['coordinator']

                logger.info(f"Three-Tier Performance: "
                           f"{integration_stats['features_processed']} features, "
                           f"{integration_stats['signals_generated']} signals, "
                           f"{coordinator_stats['current_throughput']:.2f} msg/s")

                # Check for any issues
                if integration_stats['integration_errors'] > 0:
                    logger.warning(f"Integration errors detected: {integration_stats['integration_errors']}")

                time.sleep(interval_seconds)

            except Exception as e:
                logger.error(f"Three-tier monitoring error: {e}")
                time.sleep(5)

    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()
    logger.info("Three-tier system monitoring started")
    return monitor_thread

def build_quantum_agents(
    timeframes,
    timeframe_lengths,
    state_dim,
    action_dim,
    base_path_prefix,
    gcs_bucket=None,
    gcs_meta_dir=None,
    device=None,
    quantum_bridge=None
):
    """
    Build and initialize s with optional GCS integration.
    Automatically loads saved state or initializes fresh models.
    Attaches each agent to a shared QuantumSystemBridge if provided.
    """

    agents = {}

    for tf in timeframes:
        base_path = os.path.join(base_path_prefix, tf)
        seq_len = timeframe_lengths[tf]
        gcs_path = f"{gcs_meta_dir}/agents/{tf}_quantum.zip" if gcs_meta_dir else None

        try:
            # Initialize QuantumAgent
            agent = QuantumAgent(
                name=tf,
                seq_len=seq_len,
                state_dim=state_dim,
                action_dim=action_dim,
                device=device,
                base_path=base_path,
                gcs_bucket=gcs_bucket,
                gcs_path=gcs_path
            )

            # Attempt to load saved state
            try:

                loaded = agent.load_state(bucket=gcs_bucket, gcs_path=gcs_path)
            except Exception as e:
                logger.warning(f"[{tf}] Failed to load state: {e}")
                loaded = False

            if loaded:
                logger.info(f"[{tf}] Agent loaded from saved state")
            else:
                logger.info(f"[{tf}] No saved state found, initializing new models")
                agent._init_new_models()
                agent.save_state()

            # Attach quantum bridge if provided
            if quantum_bridge:
                agent.quantum_bridge = quantum_bridge

            agents[tf] = agent

        except Exception as e:
            logger.error(f"[{tf}] QuantumAgent initialization failed: {e}")

    logger.critical(f"âœ“ Created {len(agents)} QuantumAgents")

    return agents

STATE_DIM = 58
ACTION_DIM = 2
# All values are in seconds

TIMEFRAME_LENGTHS = {
    # === High-Frequency Zone (Volatility Capture) ===
    'xs': 5,     # tick
    's': 10,     # ultra
    'm': 20,     # fast

    # === Critical Trading Zones ===
    'l': 30,     # scalp
    'xl': 60,    # 1min
    'xxl': 120,  # 2min

    # === Structure & Regime Detection ===
    '5m': 300,   # 5min
    '10m': 600,  # 10min
}

# --- Your imports for TimeframeAgent, IntegratedSignalSystem, AutosaveManager, etc. ---
# from timeframe_agent import TimeframeAgent
# from IntegratedSignalSystem import IntegratedSignalSystem
# from autosave_manager import AutosaveManager

import psutil
# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import shutil
# REMOVED DUPLICATE: import json
from pathlib import Path

def check_background_threads():
    """Check for potentially orphaned threads and processes"""
    print(f"\nBACKGROUND THREAD CHECK:")
    print(f"   Active threads: {threading.active_count()}")

    for thread in threading.enumerate():
        print(f"   - {thread.name} (daemon: {thread.daemon}, alive: {thread.is_alive()})")

    current_pid = os.getpid()
    python_processes = []
    for proc in psutil.process_iter(['pid', 'name', 'create_time']):
        try:
            if 'python' in proc.info['name'].lower() and proc.info['pid'] != current_pid:
                python_processes.append(proc.info)
        except:
            pass

    if python_processes:
        print(f"\n   Found {len(python_processes)} other Python processes:")
        for p in python_processes:
            print(f"   - PID {p['pid']}: {p['name']}")
    print()

def integrate_with_system(system):
    """Integrate Discord logger with trading system"""
    original_get_status = system.get_system_status

    def enhanced_get_status():
        status = original_get_status()
        discord_stats = none
        status['discord_logging'] = {
            'reports_sent': discord_stats['total_reports_sent'],
            'unsent_logs': discord_stats['unsent_logs_count'],
            'send_threshold': 50,
            'next_send': f"When {50 - discord_stats['unsent_logs_count']} more logs received"
        }
        return status

    system.get_system_status = enhanced_get_status

    if hasattr(system, 'reward_history'):
        original_save_reward = system._save_reward

        def enhanced_save_reward(reward):
            original_save_reward(reward)
            if len(system.reward_history) % 100 == 0:
                logger.force_send_discord_report()

        system._save_reward = enhanced_save_reward

def validate_supervised_learning_integration(system):
    """Validate supervised learning integration"""
    checks = {
        'batch_processor_exists': hasattr(system, 'batch_processor') and system.batch_processor,
        'exit_price_tracking': False,
        'supervised_samples_received': False,
        'agent_supervised_methods': False,
        'supervised_accuracy_tracking': False
    }

    if checks['batch_processor_exists']:
        stats = system.batch_processor.get_stats()
        checks['exit_price_tracking'] = stats.get('exit_prices_received', 0) > 0
        checks['supervised_samples_received'] = stats.get('supervised_samples', 0) > 0
        checks['supervised_accuracy_tracking'] = 'supervised_accuracy' in stats

    if system.agents:
        sample_agent = next(iter(system.agents.values()))
        checks['agent_supervised_methods'] = hasattr(sample_agent, 'store_experience_supervised')

    all_passed = all(checks.values())

    logger.info("=== SUPERVISED LEARNING INTEGRATION VALIDATION ===")
    for check, passed in checks.items():
        status = "PASS" if passed else "FAIL"
        logger.info(f"{status} {check.replace('_', ' ').title()}")

    if all_passed:
        logger.info("SUPERVISED LEARNING FULLY INTEGRATED")
    else:
        logger.warning("SUPERVISED LEARNING INTEGRATION INCOMPLETE")

    return all_passed

def log_comprehensive_system_status(system, three_tier_integration=None):
    """Log comprehensive status with supervised learning metrics"""
    try:
        logger.info("=== SYSTEM STATUS ===")
        status = system.get_system_status()
        logger.info(f"Mode: {status['mode'].upper()}")
        logger.info(f"Training: {status['total_training_steps']}/{status['training_threshold']} steps")

        if hasattr(system, 'batch_processor') and system.batch_processor:
            batch_stats = system.batch_processor.get_stats()
            logger.info(
                f"Batch Processing: {batch_stats['processed']} rewards, "
                f"{batch_stats['batches_processed']} batches, "
                f"{batch_stats.get('rewards_per_sec', 0):.1f} rewards/sec"
            )
            logger.info(
                f"Supervised Learning: {batch_stats['supervised_samples']} samples, "
                f"Accuracy: {batch_stats.get('supervised_accuracy', 0):.1f}%, "
                f"Exit price coverage: {status.get('exit_price_coverage', 0):.1f}%"
            )

        if 'agent_supervised_stats' in status:
            for agent_name, agent_stats in status['agent_supervised_stats'].items():
                if agent_stats:
                    logger.info(
                        f"[{agent_name}] Supervised: {agent_stats['total_supervised_samples']} samples, "
                        f"Accuracy: {agent_stats['accuracy']:.1f}%, "
                        f"Win rate: {agent_stats.get('win_rate', 0)*100:.1f}%"
                    )

        if three_tier_integration:
            try:
                gpu_stats = three_tier_integration.get_integration_stats()
                coordinator_stats = gpu_stats.get('coordinator', {})
                logger.info(
                    f"GPU Processing: {gpu_stats['integration']['features_processed']} features, "
                    f"{gpu_stats['integration']['signals_generated']} signals"
                )
            except Exception as e:
                logger.warning(f"Failed to get three-tier stats: {e}")

        enabled_features = [k.replace('_', ' ').title()
                          for k, v in status['features_enabled'].items() if v]
        logger.info(f"Active Features: {', '.join(enabled_features)}")

    except Exception as e:
        logger.error(f"Status logging error: {e}")

def start_voting_diagnostics(system, interval=60):
    """Start periodic voting model diagnostics"""
    def diagnostic_loop():
        logger.critical("Voting diagnostics thread started")
        while True:
            time.sleep(interval)
            try:
                system.diagnose_voting_model()
            except Exception as e:
                logger.error(f"Diagnostic error: {e}")

    threading.Thread(target=diagnostic_loop, daemon=True).start()
    logger.critical(f"Voting diagnostics scheduled every {interval}s")

def start_batch_progressive_monitor(system):
    """Enhanced monitoring for batch-enabled progressive system"""
    def monitor():
        last_total = 0
        last_mode = None
        last_batch_stats = {}

        while True:
            time.sleep(30)
            try:
                if system.processing_lock is None:
                    logger.error("MONITOR: Processing lock is None!")
                    continue

                status = system.get_system_status()

                if status['total_training_steps'] != last_total:
                    progress = status['progress_percent']
                    if status['mode'] == 'fast_learning':
                        logger.info(f"Training Progress: {status['total_training_steps']}/{status['training_threshold']} "
                                  f"({progress:.1f}%) - {status['remaining_steps']} steps until Full Protection")
                    else:
                        logger.info(f"Full Protection Mode - Total steps: {status['total_training_steps']}")
                    last_total = status['total_training_steps']

                if status['mode'] != last_mode:
                    if status['mode'] == 'full_protection':
                        logger.critical("SYSTEM MODE CHANGE: Fast Learning -> Full Protection")
                        logger.critical("   Meta-model gating: ACTIVATED")
                        logger.critical("   Confirmation system: ACTIVATED")
                        logger.critical("   Batch processing: ACTIVE")
                    last_mode = status['mode']

                if hasattr(system, 'batch_processor') and system.batch_processor:
                    batch_stats = system.batch_processor.get_stats()
                    rewards_processed = batch_stats.get('processed', 0)
                    if rewards_processed != last_batch_stats.get('processed', 0):
                        if rewards_processed % 50 == 0 and rewards_processed > 0:
                            logger.info(f"Batch Processing: {rewards_processed} rewards processed, "
                                      f"{batch_stats.get('batches_processed', 0)} batches, "
                                      f"{batch_stats.get('rewards_per_sec', 0):.1f} rewards/sec")
                        last_batch_stats = batch_stats.copy()

            except Exception as e:
                logger.error(f"Monitor error: {e}")

    threading.Thread(target=monitor, daemon=True).start()
    logger.info("Enhanced batch progressive monitoring started")

def start_batch_performance_monitor(system):
    """Performance monitoring for batch processing"""
    def performance_monitor():
        last_stats = {}

        while True:
            time.sleep(60)
            try:
                if system.processing_lock is None:
                    logger.error("PERFORMANCE MONITOR: Processing lock is None!")
                    continue

                if hasattr(system, 'batch_processor') and system.batch_processor:
                    stats = system.batch_processor.get_stats()
                    processed_delta = stats['processed'] - last_stats.get('processed', 0)
                    batches_delta = stats['batches_processed'] - last_stats.get('batches_processed', 0)

                    if processed_delta > 0:
                        logger.info(f"Batch Performance (last minute): "
                                  f"+{processed_delta} rewards, +{batches_delta} batches, "
                                  f"Queue: {stats.get('queue_size', 0)}")
                    last_stats = stats.copy()

            except Exception as e:
                logger.error(f"Performance monitor error: {e}")

    threading.Thread(target=performance_monitor, daemon=True).start()
    logger.info("Batch performance monitoring started")

def validate_three_tier_integration(trading_system):
    """Validate three-tier integration"""
    checks = {
        'three_tier_integration_exists': hasattr(trading_system, 'three_tier_integration'),
        'coordinator_running': False,
        'all_tiers_running': False,
        'agents_registered': False,
        'processing_enabled': False
    }

    if checks['three_tier_integration_exists']:
        integration = trading_system.three_tier_integration
        checks['coordinator_running'] = integration.running

        tier_status = integration.coordinator.system_stats['tier_status']
        checks['all_tiers_running'] = all(
            status == 'running' for status in tier_status.values()
        )
        checks['agents_registered'] = len(integration.agent_registry) > 0
        checks['processing_enabled'] = (
            integration.feature_processing_enabled and
            integration.reward_processing_enabled and
            integration.signal_generation_enabled
        )

    all_passed = all(checks.values())

    logger.info("=== THREE-TIER GPU INTEGRATION VALIDATION ===")
    for check, passed in checks.items():
        status = "PASS" if passed else "FAIL"
        logger.info(f"{status} {check.replace('_', ' ').title()}")

    if all_passed:
        logger.info("ALL THREE-TIER INTEGRATION CHECKS PASSED")
    else:
        logger.info("SOME THREE-TIER INTEGRATION CHECKS FAILED")

    return all_passed

def start_three_tier_monitoring(integration_manager, interval_seconds=30):
    """Start monitoring thread for three-tier system"""
    def monitor():
        while integration_manager.running:
            try:
                stats = integration_manager.get_integration_stats()
                integration_stats = stats['integration']
                coordinator_stats = stats['coordinator']

                if integration_stats['features_processed'] > 0 or coordinator_stats['current_throughput'] > 0:
                    logger.critical(f"Three-Tier Performance: "
                               f"{integration_stats['features_processed']} features, "
                               f"{integration_stats['signals_generated']} signals, "
                               f"{coordinator_stats['current_throughput']:.2f} msg/s")

                if integration_stats['integration_errors'] > 0:
                    logger.warning(f"Integration errors detected: {integration_stats['integration_errors']}")

                time.sleep(interval_seconds)

            except Exception as e:
                logger.error(f"Three-tier monitoring error: {e}")
                time.sleep(5)

    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()
    logger.critical("Three-tier system monitoring started")
    return monitor_thread

"""
QUANTUM TRADING SYSTEM - COMPLETE INTEGRATION
==============================================
This module integrates the quantum trading system with the existing
IntegratedSignalSystem, replacing classical agents with quantum agents.

Author: Production Integration Team
Version: 1.0.0
Date: 2025-10-14
"""

# REMOVED DUPLICATE: import os
# REMOVED DUPLICATE: import sys
# REMOVED DUPLICATE: import time
# REMOVED DUPLICATE: import logging
# REMOVED DUPLICATE: import traceback
# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: from pathlib import Path

# Import existing system components
# REMOVED DUPLICATE: from google.cloud import storage
# REMOVED DUPLICATE: import torch

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# REMOVED DUPLICATE: import threading
# REMOVED DUPLICATE: import time

# =============================================================================
# QUANTUM INTEGRATED MAIN
# =============================================================================

# ============================================================================
# V4 EMERGENCY FIX: Force Agent Optimizer Creation
# ============================================================================


# ============================================================================
# V5 EMERGENCY FIX: Add Missing Critic Networks to QuantumAgent
# ============================================================================
# DEPLOYED: October 25, 2025
# FIXES: 0 critics updating bug (Root Cause: Type Mismatch)
#
# PROBLEM: QuantumAgent instances lack .critic and .critic_optimizer attributes
#          that training loop expects, causing 0 critics to update.
#
# SOLUTION: Add missing attributes directly to QuantumAgent instances after
#           creation, making them compatible with training loop expectations.
#
# EXPECTED RESULT:
#   Before: [TRAIN] Updates complete: 1 actors, 0 critics âŒ
#   After:  [TRAIN] Updates complete: 6 actors, 6 critics âœ…
# ============================================================================

# ============================================================================
# V8.5.10: WEIGHT TRANSFER FUNCTION
# ============================================================================
# This function transfers loaded checkpoint weights from quantum_bridge
# to system (IntegratedSignalSystem) to ensure both hierarchies have
# the trained weights.
# ============================================================================

def transfer_loaded_weights_to_system(quantum_bridge, system, device='cpu'):
    """
    V8.5.10: Transfer loaded checkpoint weights from quantum_bridge to system.

    This bridges the gap between:
    - quantum_bridge.quantum_system (where checkpoint loads)
    - system (IntegratedSignalSystem, which creates fresh components)

    Args:
        quantum_bridge: QuantumSystemBridge with loaded checkpoint
        system: IntegratedSignalSystem with fresh components
        device: Device to use for tensors

    Returns:
        list: Names of successfully transferred components
    """
    transferred = []
    failed = []

    print("\n   ğŸ”„ Starting weight transfer from quantum_bridge â†’ system...")

    # ========================================================================
    # 1. LATENT ENCODER: quantum_bridge.quantum_system â†’ system
    # ========================================================================
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'latent_encoder') and
            quantum_bridge.quantum_system.latent_encoder is not None and
            hasattr(system, 'latent_encoder') and
            system.latent_encoder is not None):

            src = quantum_bridge.quantum_system.latent_encoder
            dst = system.latent_encoder

            src_state = src.state_dict()
            dst_state = dst.state_dict()

            if set(src_state.keys()) == set(dst_state.keys()):
                # Check shapes match
                shapes_match = all(
                    src_state[k].shape == dst_state[k].shape
                    for k in src_state.keys()
                )
                if shapes_match:
                    dst.load_state_dict(src_state)
                    transferred.append("latent_encoder (direct)")
                else:
                    # Partial transfer
                    for key in dst_state.keys():
                        if key in src_state and src_state[key].shape == dst_state[key].shape:
                            dst_state[key] = src_state[key].clone()
                    dst.load_state_dict(dst_state)
                    transferred.append("latent_encoder (partial)")
            else:
                failed.append("latent_encoder (key mismatch)")
    except Exception as e:
        failed.append(f"latent_encoder ({e})")

    # ========================================================================
    # 2. SHARED LATENT ENCODER (if exists in both)
    # ========================================================================
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'shared_latent_encoder') and
            quantum_bridge.quantum_system.shared_latent_encoder is not None and
            hasattr(system, 'shared_latent_encoder') and
            system.shared_latent_encoder is not None):

            src = quantum_bridge.quantum_system.shared_latent_encoder
            dst = system.shared_latent_encoder

            try:
                dst.load_state_dict(src.state_dict())
                transferred.append("shared_latent_encoder")
            except RuntimeError as e:
                failed.append(f"shared_latent_encoder ({e})")
    except Exception as e:
        pass  # shared_latent_encoder may not exist

    # ========================================================================
    # 3. AGENT WEIGHTS TRANSFER
    # ========================================================================
    # The agents in quantum_bridge.quantum_system are MultiTimeframeEntangledComplexAgent
    # The agents in system.agents are QuantumAgent
    # They have DIFFERENT architectures, so we transfer what we can:
    # - actor weights (if shapes match)
    # - critic weights (if shapes match)
    # - model weights (if shapes match)
    # ========================================================================
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'agents') and
            hasattr(system, 'agents')):

            src_agents = quantum_bridge.quantum_system.agents
            dst_agents = system.agents

            for name in dst_agents.keys():
                if name in src_agents:
                    src_agent = src_agents[name]
                    dst_agent = dst_agents[name]

                    # Transfer full state_dict if possible (same architecture)
                    try:
                        src_state = src_agent.state_dict()
                        dst_state = dst_agent.state_dict()

                        # Try direct load first
                        if set(src_state.keys()) == set(dst_state.keys()):
                            dst_agent.load_state_dict(src_state)
                            transferred.append(f"agent_{name} (full)")
                            continue
                    except:
                        pass

                    # Fallback: transfer individual components
                    agent_transferred = []

                    # Actor
                    if hasattr(src_agent, 'actor') and hasattr(dst_agent, 'actor'):
                        try:
                            src_actor_state = src_agent.actor.state_dict()
                            dst_actor_state = dst_agent.actor.state_dict()
                            if set(src_actor_state.keys()) == set(dst_actor_state.keys()):
                                dst_agent.actor.load_state_dict(src_actor_state)
                                agent_transferred.append("actor")
                        except: pass

                    # Critic
                    if hasattr(src_agent, 'critic') and hasattr(dst_agent, 'critic'):
                        try:
                            src_critic_state = src_agent.critic.state_dict()
                            dst_critic_state = dst_agent.critic.state_dict()
                            if set(src_critic_state.keys()) == set(dst_critic_state.keys()):
                                dst_agent.critic.load_state_dict(src_critic_state)
                                agent_transferred.append("critic")
                        except: pass

                    # Model (Q-network)
                    if hasattr(src_agent, 'model') and hasattr(dst_agent, 'model'):
                        try:
                            src_model_state = src_agent.model.state_dict()
                            dst_model_state = dst_agent.model.state_dict()
                            if set(src_model_state.keys()) == set(dst_model_state.keys()):
                                dst_agent.model.load_state_dict(src_model_state)
                                agent_transferred.append("model")
                                # Also update target model
                                if hasattr(dst_agent, 'target_model'):
                                    dst_agent.target_model.load_state_dict(src_model_state)
                                    agent_transferred.append("target_model")
                        except: pass

                    # Training state
                    if hasattr(src_agent, 'train_step_counter'):
                        dst_agent.train_step_counter = getattr(src_agent, 'train_step_counter', 0)
                        agent_transferred.append("train_step")
                    if hasattr(src_agent, 'train_step'):
                        dst_agent.train_step = getattr(src_agent, 'train_step', 0)
                    if hasattr(src_agent, 'epsilon'):
                        dst_agent.epsilon = getattr(src_agent, 'epsilon', 0.4)
                        agent_transferred.append("epsilon")

                    if agent_transferred:
                        transferred.append(f"agent_{name} ({'+'.join(agent_transferred)})")
                    else:
                        failed.append(f"agent_{name} (architecture mismatch)")

    except Exception as e:
        failed.append(f"agents ({e})")

    # ========================================================================
    # 3b. AGENT OPTIMIZER TRANSFER - CRITICAL FOR TRAINING MOMENTUM
    # ========================================================================
    print("\n   ğŸ“Š Transferring agent optimizer states (preserves Adam momentum)...")

    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'agents') and
            hasattr(system, 'agents')):

            src_agents = quantum_bridge.quantum_system.agents
            dst_agents = system.agents

            for name in dst_agents.keys():
                if name in src_agents:
                    src_agent = src_agents[name]
                    dst_agent = dst_agents[name]

                    opt_transferred = []

                    # Actor optimizer
                    if hasattr(src_agent, 'actor_optimizer') and hasattr(dst_agent, 'actor_optimizer'):
                        try:
                            dst_agent.actor_optimizer.load_state_dict(
                                src_agent.actor_optimizer.state_dict()
                            )
                            opt_transferred.append("actor_opt")
                        except Exception as e:
                            # Save state for potential later use
                            setattr(dst_agent, '_loaded_actor_optimizer_state',
                                    src_agent.actor_optimizer.state_dict())
                            opt_transferred.append("actor_opt(saved)")

                    # Critic optimizer
                    if hasattr(src_agent, 'critic_optimizer') and hasattr(dst_agent, 'critic_optimizer'):
                        try:
                            dst_agent.critic_optimizer.load_state_dict(
                                src_agent.critic_optimizer.state_dict()
                            )
                            opt_transferred.append("critic_opt")
                        except Exception as e:
                            setattr(dst_agent, '_loaded_critic_optimizer_state',
                                    src_agent.critic_optimizer.state_dict())
                            opt_transferred.append("critic_opt(saved)")

                    # Model/Q-network optimizer
                    if hasattr(src_agent, 'optimizer') and hasattr(dst_agent, 'optimizer'):
                        try:
                            dst_agent.optimizer.load_state_dict(
                                src_agent.optimizer.state_dict()
                            )
                            opt_transferred.append("model_opt")
                        except Exception as e:
                            setattr(dst_agent, '_loaded_optimizer_state',
                                    src_agent.optimizer.state_dict())
                            opt_transferred.append("model_opt(saved)")

                    if opt_transferred:
                        transferred.append(f"agent_{name}_optimizers ({'+'.join(opt_transferred)})")
    except Exception as e:
        failed.append(f"agent_optimizers ({e})")

    # ========================================================================
    # 3c. ENCODER OPTIMIZERS - CRITICAL FOR TRAINING MOMENTUM
    # ========================================================================
    print("   ğŸ“Š Transferring encoder optimizer states...")

    # encoder_optimizer
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'encoder_optimizer') and
            quantum_bridge.quantum_system.encoder_optimizer is not None):

            src_opt = quantum_bridge.quantum_system.encoder_optimizer
            src_state = src_opt.state_dict()

            # Store for later use - optimizer state can be applied after
            # ensuring the model parameters match
            system._loaded_encoder_optimizer_state = src_state
            transferred.append("encoder_optimizer (state saved)")
    except Exception as e:
        failed.append(f"encoder_optimizer ({e})")

    # shared_encoder_optimizer
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'shared_encoder_optimizer') and
            quantum_bridge.quantum_system.shared_encoder_optimizer is not None):

            src_opt = quantum_bridge.quantum_system.shared_encoder_optimizer
            src_state = src_opt.state_dict()
            system._loaded_shared_encoder_optimizer_state = src_state
            transferred.append("shared_encoder_optimizer (state saved)")
    except Exception as e:
        pass  # May not exist

    # agent_optimizer (combined optimizer for all agents in quantum_system)
    try:
        if (hasattr(quantum_bridge, 'quantum_system') and
            quantum_bridge.quantum_system is not None and
            hasattr(quantum_bridge.quantum_system, 'agent_optimizer') and
            quantum_bridge.quantum_system.agent_optimizer is not None):

            src_opt = quantum_bridge.quantum_system.agent_optimizer
            src_state = src_opt.state_dict()
            system._loaded_agent_optimizer_state = src_state
            transferred.append("agent_optimizer (state saved)")
    except Exception as e:
        pass  # May not exist

    # ========================================================================
    # 4. QUANTUM TRAINER - Link buffer (don't copy, share reference)
    # ========================================================================
    try:
        if (hasattr(system, 'quantum_trainer') and
            hasattr(quantum_bridge, 'hybrid_buffer') and
            quantum_bridge.hybrid_buffer is not None):

            system.quantum_trainer.buffer = quantum_bridge.hybrid_buffer
            transferred.append("quantum_trainer.buffer (linked)")
    except Exception as e:
        failed.append(f"quantum_trainer.buffer ({e})")

    # ========================================================================
    # 5. QUANTUM ADVISOR - Copy weights or link reference
    # ========================================================================
    try:
        if (hasattr(quantum_bridge, 'quantum_advisor') and
            quantum_bridge.quantum_advisor is not None):

            if not hasattr(system, 'quantum_advisor') or system.quantum_advisor is None:
                # No advisor in system, link directly
                system.quantum_advisor = quantum_bridge.quantum_advisor
                transferred.append("quantum_advisor (linked)")
            elif system.quantum_advisor is quantum_bridge.quantum_advisor:
                # Already the same object
                transferred.append("quantum_advisor (already linked)")
            else:
                # Different objects, try to copy weights
                try:
                    system.quantum_advisor.load_state_dict(
                        quantum_bridge.quantum_advisor.state_dict()
                    )
                    transferred.append("quantum_advisor (weights copied)")
                except RuntimeError:
                    # Architecture mismatch, replace with loaded one
                    system.quantum_advisor = quantum_bridge.quantum_advisor
                    transferred.append("quantum_advisor (replaced)")
    except Exception as e:
        failed.append(f"quantum_advisor ({e})")

    # ========================================================================
    # 6. EXPERIENCE BUFFER - Link reference
    # ========================================================================
    try:
        if (hasattr(quantum_bridge, 'hybrid_buffer') and
            quantum_bridge.hybrid_buffer is not None and
            hasattr(system, 'experience_replay')):

            # Don't replace, but ensure they're linked for training
            # The quantum_trainer already uses hybrid_buffer
            transferred.append("experience_buffer (quantum_trainer linked)")
    except Exception as e:
        pass

    # ========================================================================
    # SUMMARY
    # ========================================================================
    print(f"\n   âœ… Successfully transferred: {len(transferred)}")
    for comp in transferred:
        print(f"      â€¢ {comp}")

    if failed:
        print(f"\n   âš ï¸  Failed to transfer: {len(failed)}")
        for comp in failed:
            print(f"      â€¢ {comp}")

    return transferred, failed


def emergency_quantumagent_critic_fix_v6(system, learning_rate=1e-5):
    """
    ğŸš¨ V6 CRITICAL FIX (V8.5.10 UPDATE): Add missing critic networks to ALL agent instances

    V8.5.10 CHANGE: Only adds critics/optimizers if they DON'T already exist,
    to prevent overwriting loaded checkpoint weights.

    This fixes both:
    1. system.agents (QuantumAgent instances)
    2. system.quantum_bridge.quantum_system.agents (MultiTimeframe agents)

    Args:
        system: IntegratedSignalSystem instance
        learning_rate: Learning rate for new optimizers (default: 1e-4)

    Returns:
        tuple: (main_fixed_count, quantum_fixed_count)
    """
    logger.critical("\n" + "="*80)
    logger.critical("ğŸš¨ V6 EMERGENCY FIX (V8.5.10): Adding critic networks ONLY IF MISSING")
    logger.critical("="*80)
    logger.critical("V8.5.10: Will NOT overwrite existing critics from checkpoint!")
    logger.critical("="*80)

    def _has_valid_critic(agent):
        """Check if agent already has a valid critic network with parameters."""
        if not hasattr(agent, 'critic'):
            return False
        if agent.critic is None:
            return False
        # Check if it has parameters (i.e., is a real network, not empty)
        try:
            params = list(agent.critic.parameters())
            if len(params) == 0:
                return False
            # A loaded checkpoint will have parameters
            return True
        except:
            return False

    def _has_valid_optimizer(agent, optimizer_name):
        """Check if agent has a valid optimizer."""
        if not hasattr(agent, optimizer_name):
            return False
        opt = getattr(agent, optimizer_name)
        if opt is None:
            return False
        # Check if optimizer has param_groups (is properly initialized)
        try:
            return len(opt.param_groups) > 0
        except:
            return False

    # ========================================================================
    # PART 1: Fix system.agents (QuantumAgent instances)
    # ========================================================================
    logger.critical("\nğŸ“‹ Part 1: Checking system.agents...")
    main_fixed = 0
    main_skipped = 0
    main_errors = []

    for agent_name, agent in system.agents.items():
        logger.info(f"ğŸ”§ Checking system.agents['{agent_name}'] (type: {type(agent).__name__})...")

        try:
            # Get state_dim from agent or use default
            state_dim = getattr(agent, 'state_dim', 58)
            device = getattr(agent, 'device', 'cpu')

            # V8.5.10: Only add critic if it doesn't exist
            if _has_valid_critic(agent):
                logger.info(f"  â­ï¸  {agent_name} already has valid critic - SKIPPING (preserving loaded weights)")
                main_skipped += 1
            else:
                # Add critic network
                agent.critic = nn.Sequential(
                    nn.Linear(state_dim, 128),
                    nn.LayerNorm(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, 128),
                    nn.LayerNorm(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, 1)
                ).to(device)
                logger.info(f"  âœ… {agent_name} critic CREATED (was missing)")
                main_fixed += 1

            # V8.5.10: Only add optimizer if it doesn't exist
            if not _has_valid_optimizer(agent, 'critic_optimizer') and hasattr(agent, 'critic'):
                agent.critic_optimizer = optim.Adam(
                    agent.critic.parameters(),
                    lr=learning_rate
                )
                logger.info(f"  âœ… {agent_name} critic_optimizer CREATED")

            if not _has_valid_optimizer(agent, 'actor_optimizer'):
                params_list = list(agent.parameters())
                if params_list:
                    agent.actor_optimizer = optim.Adam(params_list, lr=learning_rate)
                    logger.info(f"  âœ… {agent_name} actor_optimizer CREATED")

        except Exception as e:
            logger.error(f"  âŒ Failed: {e}")
            main_errors.append(f"{agent_name}: {e}")

    logger.critical(f"âœ… system.agents: {main_fixed} created, {main_skipped} preserved (loaded)")

    # ========================================================================
    # PART 2: Fix quantum_bridge.quantum_system.agents (THE CRITICAL ONES!)
    # ========================================================================
    quantum_fixed = 0
    quantum_skipped = 0
    quantum_errors = []

    if not hasattr(system, 'quantum_bridge') or not system.quantum_bridge:
        logger.warning("âš ï¸  No quantum_bridge found - skipping Part 2")
        return main_fixed, quantum_fixed

    if not hasattr(system.quantum_bridge, 'quantum_system'):
        logger.warning("âš ï¸  quantum_bridge has no quantum_system - skipping Part 2")
        return main_fixed, quantum_fixed

    if not hasattr(system.quantum_bridge.quantum_system, 'agents'):
        logger.warning("âš ï¸  quantum_system has no agents - skipping Part 2")
        return main_fixed, quantum_fixed

    logger.critical("\nğŸ“‹ Part 2: Checking quantum_bridge.quantum_system.agents (USED FOR TRAINING)...")

    quantum_agents = system.quantum_bridge.quantum_system.agents

    for agent_name, agent in quantum_agents.items():
        logger.info(f"ğŸ”§ Checking quantum_bridge agent '{agent_name}' (type: {type(agent).__name__})...")

        try:
            # Get state_dim from agent or system
            if hasattr(agent, 'state_dim'):
                state_dim = agent.state_dim
            elif hasattr(system.quantum_bridge.quantum_system, 'state_dim'):
                state_dim = system.quantum_bridge.quantum_system.state_dim
            else:
                state_dim = 58  # fallback

            # Get device
            if hasattr(agent, 'device'):
                device = agent.device
            elif hasattr(system.quantum_bridge.quantum_system, 'device'):
                device = system.quantum_bridge.quantum_system.device
            else:
                device = 'cpu'

            # V8.5.10: Only add critic if it doesn't exist
            if _has_valid_critic(agent):
                logger.info(f"  â­ï¸  {agent_name} already has valid critic - SKIPPING (preserving loaded weights)")
                quantum_skipped += 1
            else:
                # Add critic network
                agent.critic = nn.Sequential(
                    nn.Linear(state_dim, 128),
                    nn.LayerNorm(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, 128),
                    nn.LayerNorm(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, 1)
                ).to(device)
                logger.info(f"  âœ… {agent_name} critic CREATED (was missing)")
                quantum_fixed += 1

            # V8.5.10: Only add optimizer if it doesn't exist
            if not _has_valid_optimizer(agent, 'critic_optimizer') and hasattr(agent, 'critic'):
                agent.critic_optimizer = optim.Adam(
                    agent.critic.parameters(),
                    lr=learning_rate
                )
                logger.info(f"  âœ… {agent_name} critic_optimizer CREATED")

            if not _has_valid_optimizer(agent, 'actor_optimizer'):
                params_list = list(agent.parameters())
                if params_list:
                    agent.actor_optimizer = optim.Adam(params_list, lr=learning_rate)
                    logger.info(f"  âœ… {agent_name} actor_optimizer CREATED")

        except Exception as e:
            logger.error(f"  âŒ Failed: {e}")
            quantum_errors.append(f"{agent_name}: {e}")

    logger.critical(f"âœ… quantum_bridge agents: {quantum_fixed} created, {quantum_skipped} preserved (loaded)")

    # ========================================================================
    # SUMMARY
    # ========================================================================
    logger.critical("\n" + "="*80)
    logger.critical("ğŸ¯ V6 EMERGENCY FIX (V8.5.10) COMPLETE")
    logger.critical("="*80)
    logger.critical(f"   system.agents: {main_fixed} created, {main_skipped} preserved")
    logger.critical(f"   quantum_bridge agents: {quantum_fixed} created, {quantum_skipped} preserved")

    total_preserved = main_skipped + quantum_skipped
    if total_preserved > 0:
        logger.critical(f"   âœ… {total_preserved} agents preserved loaded weights from checkpoint!")

    if main_errors:
        logger.critical(f"   âš ï¸  system.agents errors: {len(main_errors)}")
        for err in main_errors:
            logger.error(f"     - {err}")

    if quantum_errors:
        logger.critical(f"   âš ï¸  quantum_bridge errors: {len(quantum_errors)}")
        for err in quantum_errors:
            logger.error(f"     - {err}")

    logger.critical("="*80 + "\n")

    return main_fixed, quantum_fixed

def validate_agent_training_readiness_v6(system):
    """
    V6: Validate that ALL agents (including quantum_bridge agents) have
    required attributes for training.

    Returns:
        dict: Validation report with details for both agent systems
    """
    logger.critical("\n" + "="*80)
    logger.critical("ğŸ” V6 VALIDATING AGENT TRAINING READINESS")
    logger.critical("="*80)

    # ========================================================================
    # Part 1: Validate system.agents
    # ========================================================================
    logger.critical("\nğŸ“‹ Part 1: Checking system.agents...")
    report_main = _validate_agent_dict(system.agents, "system.agents")

    # ========================================================================
    # Part 2: Validate quantum_bridge agents (THE ONES THAT MATTER!)
    # ========================================================================
    report_quantum = None

    if hasattr(system, 'quantum_bridge') and system.quantum_bridge:
        if hasattr(system.quantum_bridge, 'quantum_system'):
            if hasattr(system.quantum_bridge.quantum_system, 'agents'):
                logger.critical("\nğŸ“‹ Part 2: Checking quantum_bridge.quantum_system.agents (USED FOR TRAINING)...")
                report_quantum = _validate_agent_dict(
                    system.quantum_bridge.quantum_system.agents,
                    "quantum_bridge.quantum_system.agents"
                )

    # ========================================================================
    # Summary
    # ========================================================================
    logger.critical("\n" + "="*80)
    logger.critical("ğŸ“Š V6 VALIDATION SUMMARY")
    logger.critical("="*80)

    # Main system agents
    logger.critical(f"\nğŸ“¦ system.agents:")
    logger.critical(f"   Total: {report_main['total_agents']}")
    logger.critical(f"   Critics ready: {report_main['ready_for_critic_update']}/6")
    logger.critical(f"   Actors ready: {report_main['ready_for_actor_update']}/6")

    # Quantum bridge agents (THE IMPORTANT ONES)
    if report_quantum:
        logger.critical(f"\nğŸ¯ quantum_bridge.quantum_system.agents (USED FOR TRAINING!):")
        logger.critical(f"   Total: {report_quantum['total_agents']}")
        logger.critical(f"   Critics ready: {report_quantum['ready_for_critic_update']}/6")
        logger.critical(f"   Actors ready: {report_quantum['ready_for_actor_update']}/6")

        if report_quantum['ready_for_critic_update'] == 6 and report_quantum['ready_for_actor_update'] == 6:
            logger.critical("\n   ğŸ‰ ALL QUANTUM AGENTS READY FOR TRAINING!")
            logger.critical("   âœ… Expected: [TRAIN] Updates complete: 6 actors, 6 critics")
        else:
            logger.critical("\n   âŒ NOT ALL QUANTUM AGENTS ARE READY!")
            logger.critical("   âš ï¸  Training will likely show: [TRAIN] Updates complete: <6 actors, <6 critics")
    else:
        logger.critical(f"\nâš ï¸  quantum_bridge.quantum_system.agents NOT FOUND!")
        logger.critical("   âŒ This means training will definitely fail!")

    logger.critical("="*80 + "\n")

    return {
        'main': report_main,
        'quantum': report_quantum
    }
def _validate_agent_dict(agents_dict, dict_name="agents"):
    """
    Helper function to validate a dictionary of agents.

    Args:
        agents_dict: Dictionary of agent objects
        dict_name: Name for logging purposes

    Returns:
        dict: Validation report
    """
    report = {
        'total_agents': len(agents_dict),
        'ready_for_critic_update': 0,
        'ready_for_actor_update': 0,
        'agents_details': {}
    }

    for agent_name, agent in agents_dict.items():
        agent_status = {
            'type': type(agent).__name__,
            'has_critic': hasattr(agent, 'critic') and agent.critic is not None,
            'has_critic_optimizer': hasattr(agent, 'critic_optimizer') and agent.critic_optimizer is not None,
            'has_actor_optimizer': hasattr(agent, 'actor_optimizer') and agent.actor_optimizer is not None,
            'has_optimizer': hasattr(agent, 'optimizer') and agent.optimizer is not None,
            'critic_ready': False,
            'actor_ready': False
        }

        # Check if ready for critic updates (needs both critic and critic_optimizer)
        if agent_status['has_critic'] and agent_status['has_critic_optimizer']:
            agent_status['critic_ready'] = True
            report['ready_for_critic_update'] += 1

        # Check if ready for actor updates
        if agent_status['has_actor_optimizer']:
            agent_status['actor_ready'] = True
            report['ready_for_actor_update'] += 1

        report['agents_details'][agent_name] = agent_status

        # Log per-agent status
        status_icon = "âœ…" if (agent_status['critic_ready'] and agent_status['actor_ready']) else "âŒ"
        logger.critical(f"  {status_icon} {agent_name} ({agent_status['type']})")
        logger.critical(f"     Critic: {agent_status['critic_ready']} | Actor: {agent_status['actor_ready']}")

        # Detailed debug info for failed agents
        if not (agent_status['critic_ready'] and agent_status['actor_ready']):
            logger.debug(f"     Details: critic={agent_status['has_critic']}, " +
                        f"critic_opt={agent_status['has_critic_optimizer']}, " +
                        f"actor_opt={agent_status['has_actor_optimizer']}")

    return report


def force_fix_all_optimizers(system, learning_rate= 1e-6):
    """
    V4 HOTFIX: Aggressively force-create ALL optimizers for ALL agents

    This is MORE AGGRESSIVE than emergency_fix_agent_optimizers.
    It doesn't check if optimizers exist - it just creates them.

    Use this when agents were created BEFORE the V4 patch was deployed,
    so they don't have optimizers from __init__.

    Args:
        system: IntegratedSignalSystem instance
        learning_rate: Learning rate for optimizers

    Returns:
        Number of agents fixed
    """
    import torch.optim as optim
    import torch.nn as nn

    logger.critical("\n" + "="*80)
    logger.critical("ğŸš¨ V4 HOTFIX: Aggressively fixing ALL optimizers")
    logger.critical("="*80)

    fixed_count = 0
    total_optimizers_created = 0

    # ============================================================================
    # STEP 1: FIND ALL AGENT DICTIONARIES
    # ============================================================================
    agent_sources = []

    # Path 1: Direct system.agents
    if hasattr(system, 'agents'):
        agent_sources.append(('system.agents', system.agents))
        logger.info(f"âœ“ Found: system.agents ({len(system.agents)} agents)")

    # Path 2: Quantum bridge agents (MOST LIKELY LOCATION)
    if hasattr(system, 'quantum_bridge'):
        if hasattr(system.quantum_bridge, 'system'):
            if hasattr(system.quantum_bridge.system, 'agents'):
                agent_sources.append(('quantum_bridge.system.agents',
                                     system.quantum_bridge.system.agents))
                logger.info(f"âœ“ Found: quantum_bridge.system.agents ({len(system.quantum_bridge.system.agents)} agents)")

    # Path 3: Direct quantum system
    if hasattr(system, 'quantum_system'):
        if hasattr(system.quantum_system, 'agents'):
            agent_sources.append(('quantum_system.agents',
                                 system.quantum_system.agents))
            logger.info(f"âœ“ Found: quantum_system.agents ({len(system.quantum_system.agents)} agents)")

    if not agent_sources:
        logger.error("âŒ CRITICAL: No agent dictionaries found!")
        return 0

    logger.critical(f"\nğŸ“Š Found {len(agent_sources)} agent source(s)")

    # ============================================================================
    # STEP 2: FORCE-FIX EVERY AGENT (NO CHECKS, JUST CREATE)
    # ============================================================================
    for source_name, agents_dict in agent_sources:
        logger.critical(f"\n{'='*80}")
        logger.critical(f"ğŸ”§ Force-fixing: {source_name}")
        logger.critical(f"{'='*80}")
        logger.critical(f"   Agent count: {len(agents_dict)}")

        for agent_name, agent in agents_dict.items():
            logger.info(f"\n  âš¡ Agent: {agent_name}")
            agent_fixed = False

            # FORCE FIX 1: critic_optimizer (CRITICAL!)
            try:
                if hasattr(agent, 'critic') and agent.critic is not None:
                    # ALWAYS recreate - don't check if it exists
                    agent.critic_optimizer = optim.Adam(
                        agent.critic.parameters(),
                        lr=learning_rate
                    )
                    logger.info(f"     âœ… critic_optimizer force-created")
                    total_optimizers_created += 1
                    agent_fixed = True
                else:
                    logger.warning(f"     âš ï¸  No critic network found")
            except Exception as e:
                logger.error(f"     âŒ critic_optimizer creation failed: {e}")

            # FORCE FIX 2: optimizer (CRITICAL!)
            try:
                # ALWAYS recreate
                agent.optimizer = optim.Adam(
                    agent.parameters(),
                    lr=learning_rate
                )
                logger.info(f"     âœ… optimizer force-created")
                total_optimizers_created += 1
                agent_fixed = True
            except Exception as e:
                logger.error(f"     âŒ optimizer creation failed: {e}")

            # FORCE FIX 3: actor_optimizer
            try:
                # Get non-critic parameters
                actor_params = [p for n, p in agent.named_parameters()
                              if 'critic' not in n]
                if actor_params:
                    # ALWAYS recreate
                    agent.actor_optimizer = optim.Adam(
                        actor_params,
                        lr=learning_rate
                    )
                    logger.info(f"     âœ… actor_optimizer force-created")
                    total_optimizers_created += 1
                    agent_fixed = True
            except Exception as e:
                logger.error(f"     âŒ actor_optimizer creation failed: {e}")

            # FORCE FIX 4: model alias (for compatibility)
            try:
                if hasattr(agent, 'critic') and not hasattr(agent, 'model'):
                    agent.model = agent.critic
                    logger.info(f"     âœ… model alias created")
            except Exception as e:
                logger.error(f"     âŒ model alias creation failed: {e}")

            if agent_fixed:
                fixed_count += 1
                logger.info(f"     âœ… AGENT FIXED")

    # ============================================================================
    # STEP 3: VERIFICATION
    # ============================================================================
    logger.critical("\n" + "="*80)
    logger.critical(f"ğŸ‰ V4 HOTFIX COMPLETE")
    logger.critical("="*80)
    logger.critical(f"   Agents fixed: {fixed_count}")
    logger.critical(f"   Optimizers created: {total_optimizers_created}")

    if fixed_count > 0:
        logger.critical(f"   âœ… {fixed_count} agents now have working optimizers!")
        logger.critical(f"   âœ… Training should work now!")
    else:
        logger.warning(f"   âš ï¸  No agents were fixed (might already be OK)")

    logger.critical("="*80 + "\n")

    # ============================================================================
    # STEP 4: QUICK VERIFICATION TEST
    # ============================================================================
    logger.critical("ğŸ”¬ Quick verification test:")

    for source_name, agents_dict in agent_sources[:1]:  # Just check first source
        if agents_dict:
            first_agent = list(agents_dict.values())[0]
            logger.critical(f"   Testing first agent from {source_name}:")
            logger.critical(f"     Has critic_optimizer: {hasattr(first_agent, 'critic_optimizer')}")
            if hasattr(first_agent, 'critic_optimizer'):
                logger.critical(f"       Is None: {first_agent.critic_optimizer is None}")
            logger.critical(f"     Has optimizer: {hasattr(first_agent, 'optimizer')}")
            if hasattr(first_agent, 'optimizer'):
                logger.critical(f"       Is None: {first_agent.optimizer is None}")
            logger.critical(f"     Has actor_optimizer: {hasattr(first_agent, 'actor_optimizer')}")
            if hasattr(first_agent, 'actor_optimizer'):
                logger.critical(f"       Is None: {first_agent.actor_optimizer is None}")
            break

    logger.critical("")

    return fixed_count


def emergency_fix_agent_optimizers(system, learning_rate=1e-5):
    """
    V4 EMERGENCY FIX: Ensure all agents have working optimizers

    This function validates and fixes agent attributes to ensure training works.
    Handles both system.agents and system.quantum_bridge.system.agents paths.
    Automatically called after system initialization.

    CRITICAL FIXES IN V4:
    - Checks multiple agent dictionary locations (quantum_bridge path)
    - Adds critic_optimizer (enables Path 3 in _train_on_batch)
    - Adds optimizer (enables Path 4 in _train_on_batch)
    - Adds actor_optimizer (for future actor-critic training)

    Args:
        system: IntegratedSignalSystem instance
        learning_rate: Learning rate for recreated optimizers

    Returns:
        Number of agents fixed
    """
    import torch.optim as optim
    import torch.nn as nn

    logger.critical("\n" + "="*80)
    logger.critical("ğŸ”§ V4 FIX: Validating agent optimizers (ENHANCED)")
    logger.critical("="*80)

    fixed_count = 0
    already_ok_count = 0
    total_agents_found = 0

    # ============================================================================
    # STEP 1: FIND ALL AGENT DICTIONARIES (multiple possible locations)
    # ============================================================================
    agent_sources = []

    # Path 1: Direct system.agents
    if hasattr(system, 'agents'):
        agent_sources.append(('system.agents', system.agents))
        logger.info(f"âœ“ Found: system.agents ({len(system.agents)} agents)")

    # Path 2: Quantum bridge agents (THIS IS THE KEY FIX FOR YOUR SYSTEM!)
    if hasattr(system, 'quantum_bridge'):
        logger.info(f"âœ“ Found: system.quantum_bridge")
        if hasattr(system.quantum_bridge, 'system'):
            logger.info(f"âœ“ Found: system.quantum_bridge.system")
            if hasattr(system.quantum_bridge.system, 'agents'):
                agent_sources.append(('quantum_bridge.system.agents',
                                     system.quantum_bridge.system.agents))
                logger.info(f"âœ“ Found: system.quantum_bridge.system.agents ({len(system.quantum_bridge.system.agents)} agents)")

    # Path 3: Direct quantum system (alternative structure)
    if hasattr(system, 'quantum_system'):
        if hasattr(system.quantum_system, 'agents'):
            agent_sources.append(('quantum_system.agents',
                                 system.quantum_system.agents))
            logger.info(f"âœ“ Found: system.quantum_system.agents ({len(system.quantum_system.agents)} agents)")

    if not agent_sources:
        logger.error("âŒ CRITICAL: No agent dictionaries found!")
        logger.error("   System attributes: " + str([attr for attr in dir(system) if not attr.startswith('_')][:20]))
        return 0

    logger.critical(f"\nğŸ“Š Found {len(agent_sources)} agent source(s)")

    # ============================================================================
    # STEP 2: FIX EACH AGENT DICTIONARY
    # ============================================================================
    for source_name, agents_dict in agent_sources:
        logger.critical(f"\n{'='*80}")
        logger.critical(f"ğŸ” Processing: {source_name}")
        logger.critical(f"{'='*80}")
        logger.critical(f"   Agent count: {len(agents_dict)}")

        for agent_name in agents_dict:
            agent = agents_dict[agent_name]
            total_agents_found += 1

            logger.info(f"\n  ğŸ“Œ Agent: {agent_name}")
            logger.info(f"     Type: {type(agent).__name__}")

            # ======== CHECK CURRENT STATE ========
            has_critic = hasattr(agent, 'critic')
            has_critic_optimizer = hasattr(agent, 'critic_optimizer')
            has_optimizer = hasattr(agent, 'optimizer')
            has_actor_optimizer = hasattr(agent, 'actor_optimizer')
            has_model = hasattr(agent, 'model')

            logger.info(f"     Has critic: {has_critic}")
            logger.info(f"     Has critic_optimizer: {has_critic_optimizer}")
            logger.info(f"     Has optimizer: {has_optimizer}")
            logger.info(f"     Has actor_optimizer: {has_actor_optimizer}")
            logger.info(f"     Has model: {has_model}")

            # Check if optimizers are None (even if attribute exists)
            if has_critic_optimizer:
                critic_opt_ok = agent.critic_optimizer is not None
                logger.info(f"     critic_optimizer is None: {not critic_opt_ok}")
            else:
                critic_opt_ok = False

            if has_optimizer:
                opt_ok = agent.optimizer is not None
                logger.info(f"     optimizer is None: {not opt_ok}")
            else:
                opt_ok = False

            # ======== DETERMINE WHAT NEEDS FIXING ========
            needs_fix = False

            # FIX 1: Add/fix critic_optimizer (CRITICAL for Path 3 in _train_on_batch)
            if has_critic and not critic_opt_ok:
                logger.warning(f"     âš ï¸  FIXING: Creating critic_optimizer...")
                try:
                    agent.critic_optimizer = optim.Adam(
                        agent.critic.parameters(),
                        lr=learning_rate
                    )
                    logger.info(f"     âœ… critic_optimizer created")
                    needs_fix = True
                except Exception as e:
                    logger.error(f"     âŒ Failed to create critic_optimizer: {e}")

            # FIX 2: Add/fix generic optimizer (CRITICAL for Path 4 in _train_on_batch)
            if not opt_ok:
                logger.warning(f"     âš ï¸  FIXING: Creating optimizer...")
                try:
                    agent.optimizer = optim.Adam(
                        agent.parameters(),
                        lr=learning_rate
                    )
                    logger.info(f"     âœ… optimizer created")
                    needs_fix = True
                except Exception as e:
                    logger.error(f"     âŒ Failed to create optimizer: {e}")

            # FIX 3: Add actor_optimizer (OPTIONAL - for future actor updates)
            if not has_actor_optimizer and isinstance(agent, nn.Module):
                logger.info(f"     â„¹ï¸  OPTIONAL: Creating actor_optimizer...")
                try:
                    # Get all parameters except critic
                    actor_params = [p for n, p in agent.named_parameters()
                                  if 'critic' not in n]
                    if actor_params:
                        agent.actor_optimizer = optim.Adam(
                            actor_params,
                            lr=learning_rate
                        )
                        logger.info(f"     âœ… actor_optimizer created")
                        needs_fix = True
                except Exception as e:
                    logger.error(f"     âŒ Failed to create actor_optimizer: {e}")

            # FIX 4: Legacy - add model if needed (for compatibility)
            if has_critic and not has_model:
                logger.info(f"     â„¹ï¸  LEGACY: Adding model alias to critic...")
                try:
                    agent.model = agent.critic
                    logger.info(f"     âœ… model alias created")
                except Exception as e:
                    logger.error(f"     âŒ Failed to create model alias: {e}")

            # ======== SUMMARY FOR THIS AGENT ========
            if needs_fix:
                fixed_count += 1
                logger.info(f"     âœ… AGENT FIXED")
            else:
                already_ok_count += 1
                logger.info(f"     âœ… Already OK")

    # ============================================================================
    # STEP 3: FINAL SUMMARY
    # ============================================================================
    logger.critical("\n" + "="*80)
    logger.critical(f"ğŸ‰ V4 FIX COMPLETE")
    logger.critical("="*80)
    logger.critical(f"   Total agents found: {total_agents_found}")
    logger.critical(f"   Already OK: {already_ok_count}")
    logger.critical(f"   Fixed: {fixed_count}")

    if fixed_count > 0:
        logger.critical(f"   ğŸ”§ {fixed_count} agents were repaired")
        logger.critical(f"   âœ… Training should now work!")
    elif already_ok_count == total_agents_found:
        logger.critical(f"   âœ… All agents already had working optimizers")
    else:
        logger.warning(f"   âš ï¸  Some agents may still have issues")

    logger.critical("="*80 + "\n")

    return fixed_count

def ensure_quantum_advisor_accessible_v82(system, quantum_advisor):
    """
    V8.2 FIX #1: Make quantum advisor accessible from everywhere.
    This solves the "quantum advisor never called" bug.
    """
    logger.critical("="*80)
    logger.critical("ğŸ”§ V8.2 FIX 1/4: QUANTUM ADVISOR ACCESSIBILITY")
    logger.critical("="*80)

    # 1. Add to quantum_system
    if hasattr(system, 'quantum_system'):
        system.quantum_system.quantum_advisor = quantum_advisor
        logger.critical("âœ… Added to system.quantum_system")

    # 2. Add to system itself
    system.quantum_advisor = quantum_advisor
    logger.critical("âœ… Added to system")

    # 3. Add to all agents
    if hasattr(system, 'agents'):
        for name, agent in system.agents.items():
            agent.quantum_advisor = quantum_advisor
        logger.critical(f"âœ… Added to {len(system.agents)} agents")

    # 4. Add to quantum_bridge
    if hasattr(system, 'quantum_bridge'):
        system.quantum_bridge.quantum_advisor = quantum_advisor
        if hasattr(system.quantum_bridge, 'quantum_system'):
            system.quantum_bridge.quantum_system.quantum_advisor = quantum_advisor
        logger.critical("âœ… Added to quantum_bridge")

    # 5. Store in globals as fallback
    globals()['_quantum_advisor_instance_v82'] = quantum_advisor
    logger.critical("âœ… Stored in globals")
    logger.critical("="*80)


def patch_prediction_method_v82(system):
    """
    V8.2 FIX #2: Patch predict_all_agents_with_metadata to force quantum advisor.
    This ensures quantum advisor is ALWAYS applied to predictions.
    """
    logger.critical("="*80)
    logger.critical("ğŸ”§ V8.2 FIX 2/4: PREDICTION METHOD PATCH")
    logger.critical("="*80)

    if not hasattr(system, 'predict_all_agents_with_metadata'):
        logger.critical("âŒ No predict_all_agents_with_metadata method found")
        return

    # Save original
    original_predict = system.predict_all_agents_with_metadata

    def forced_quantum_predict_v82():
        """Prediction with forced quantum advisor application."""
        # Call original to get base predictions
        try:
            q_values, metadata = original_predict()
        except Exception as e:
            logger.critical(f"âŒ Original predict failed: {e}")
            return {}, {}

        # Force quantum advisor check
        quantum_advisor = (
            getattr(system, 'quantum_advisor', None) or
            (getattr(system.quantum_system, 'quantum_advisor', None) if hasattr(system, 'quantum_system') else None) or
            globals().get('_quantum_advisor_instance_v82')
        )

        if quantum_advisor is not None:
            logger.critical("ğŸ”® FORCING QUANTUM ADVISOR APPLICATION (V8.2)")

            # Get states
            states_dict = None
            if hasattr(system, '_get_state_cache_for_prediction'):
                states_dict = system._get_state_cache_for_prediction()
            elif hasattr(system, 'state_cache'):
                states_dict = system.state_cache

            if states_dict:
                # Apply quantum adjustments to each agent
                adjusted_q = {}
                for agent_name, q in q_values.items():
                    try:
                        # Get state for this agent
                        state = states_dict.get(agent_name)
                        if state is None:
                            adjusted_q[agent_name] = q
                            continue

                        # Handle dict states (multi-timeframe)
                        if isinstance(state, dict):
                            state = state.get('m', next(iter(state.values())))

                        # Convert to tensor
                        if not isinstance(state, torch.Tensor):
                            state = torch.tensor(state, dtype=torch.float32)

                        device = getattr(system, 'device', 'cpu')
                        state = state.to(device)

                        if state.dim() == 1:
                            state = state.unsqueeze(0)

                        # Convert Q-values to tensor
                        q_tensor = torch.tensor(q, dtype=torch.float32, device=device)
                        if q_tensor.dim() == 1:
                            q_tensor = q_tensor.unsqueeze(0)

                        # Apply quantum advisor
                        with torch.no_grad():
                            adj_q, adj_meta = quantum_advisor.predict_q_adjustment(state, q_tensor)

                        # Convert back to numpy
                        adjusted_q[agent_name] = adj_q.squeeze(0).detach().cpu().numpy()

                        logger.critical(
                            f"âœ… {agent_name}: BUY: {q[0]:.4f}â†’{adjusted_q[agent_name][0]:.4f} | "
                            f"SELL: {q[1]:.4f}â†’{adjusted_q[agent_name][1]:.4f}"
                        )

                    except Exception as e:
                        logger.critical(f"âŒ {agent_name}: Quantum adjustment failed: {e}")
                        adjusted_q[agent_name] = q

                q_values = adjusted_q
                metadata['quantum_forecast'] = {'applied': True, 'version': 'v8.2'}
                metadata['quantum_tempering_applied'] = True

        return q_values, metadata

    # Apply the patch
    system.predict_all_agents_with_metadata = forced_quantum_predict_v82
    system._original_predict_all_agents_v82 = original_predict
    logger.critical("âœ… Patched prediction method to force quantum advisor (V8.2)")
    logger.critical("="*80)


def enable_training_logging_v82(system):
    """
    V8.2 FIX #3: Add comprehensive logging to training.
    This solves the "agents never learn" bug by activating and monitoring training.
    """
    logger.critical("="*80)
    logger.critical("ğŸ”§ V8.2 FIX 3/4: TRAINING ACTIVATION")
    logger.critical("="*80)

    # Find trainer - V8.5.8 FIX: Look for quantum_trainer (not trainer)
    trainer = None
    if hasattr(system, 'quantum_trainer'):
        trainer = system.quantum_trainer
        logger.critical("âœ“ Found trainer at: system.quantum_trainer")
    elif hasattr(system, 'quantum_bridge'):
        if hasattr(system.quantum_bridge, 'quantum_trainer'):
            trainer = system.quantum_bridge.quantum_trainer
            logger.critical("âœ“ Found trainer at: system.quantum_bridge.quantum_trainer")
        elif hasattr(system.quantum_bridge, 'trainer'):
            trainer = system.quantum_bridge.trainer
            logger.critical("âœ“ Found trainer at: system.quantum_bridge.trainer")

    if trainer is None:
        logger.critical("âŒ No trainer found")
        logger.critical("   Checked:")
        logger.critical(f"   - system.quantum_trainer: {hasattr(system, 'quantum_trainer')}")
        if hasattr(system, 'quantum_bridge'):
            logger.critical(f"   - system.quantum_bridge.quantum_trainer: {hasattr(system.quantum_bridge, 'quantum_trainer')}")
            logger.critical(f"   - system.quantum_bridge.trainer: {hasattr(system.quantum_bridge, 'trainer')}")
        return

    # Wrap train_step with logging
    if hasattr(trainer, 'train_step'):
        original_train = trainer.train_step

        def logged_train_step_v82(*args, **kwargs):
            logger.critical("ğŸ“ TRAINING STEP EXECUTING (V8.2)...")
            try:
                result = original_train(*args, **kwargs)
                logger.critical(f"âœ… TRAINING COMPLETE (V8.2) | Result: {result}")
                return result
            except Exception as e:
                logger.critical(f"âŒ TRAINING FAILED (V8.2): {e}")
                import traceback
                traceback.print_exc()
                raise

        trainer.train_step = logged_train_step_v82
        logger.critical(f"âœ… Added logging to train_step (V8.2) | Trainer type: {type(trainer).__name__}")

    # Check RewardBatchProcessor
    if hasattr(system, 'reward_batch_processor'):
        processor = system.reward_batch_processor

        if hasattr(processor, 'running') and not processor.running:
            logger.critical("âš ï¸  Starting RewardBatchProcessor (V8.2)...")
            try:
                if hasattr(processor, 'start'):
                    processor.start()
                    logger.critical("âœ… Started RewardBatchProcessor (V8.2)")
            except Exception as e:
                logger.critical(f"âŒ Failed to start: {e}")
        else:
            logger.critical("âœ… RewardBatchProcessor already running")

        # Add batch processing logging
        if hasattr(processor, 'process_batch'):
            original_process = processor.process_batch

            def logged_process_batch_v82(batch):
                logger.critical(f"ğŸ”„ PROCESSING BATCH (V8.2) | Size: {len(batch)}")
                result = original_process(batch)
                logger.critical(f"âœ… BATCH PROCESSED (V8.2)")
                return result

            processor.process_batch = logged_process_batch_v82
            logger.critical("âœ… Added logging to batch processor (V8.2)")

    logger.critical("="*80)

def link_buffer_trainer_v82(system):
    """
    V8.2 FIX #4: Ensure trainer and buffer are linked.
    This solves the disconnect between experience storage and training.
    """
    logger.critical("="*80)
    logger.critical("ğŸ”§ V8.2 FIX 4/4: BUFFER-TRAINER LINKAGE")
    logger.critical("="*80)

    # ------------------------------------------------------------------
    # Find buffer
    # ------------------------------------------------------------------
    buffer = None
    if hasattr(system, 'hybrid_buffer'):
        buffer = system.hybrid_buffer
        logger.critical("âœ“ Found buffer at: system.hybrid_buffer")
    elif hasattr(system, 'quantum_bridge') and hasattr(system.quantum_bridge, 'hybrid_buffer'):
        buffer = system.quantum_bridge.hybrid_buffer
        logger.critical("âœ“ Found buffer at: system.quantum_bridge.hybrid_buffer")

    if buffer is None:
        logger.critical("âŒ No buffer found")
        return

    # ------------------------------------------------------------------
    # Find trainer
    # ------------------------------------------------------------------
    trainer = None
    if hasattr(system, 'quantum_trainer'):
        trainer = system.quantum_trainer
        logger.critical("âœ“ Found trainer at: system.quantum_trainer")
    elif hasattr(system, 'quantum_bridge'):
        if hasattr(system.quantum_bridge, 'quantum_trainer'):
            trainer = system.quantum_bridge.quantum_trainer
            logger.critical("âœ“ Found trainer at: system.quantum_bridge.quantum_trainer")
        elif hasattr(system.quantum_bridge, 'trainer'):
            trainer = system.quantum_bridge.trainer
            logger.critical("âœ“ Found trainer at: system.quantum_bridge.trainer")

    if trainer is None:
        logger.critical("âŒ No trainer found")
        return

    logger.critical(f"âœ… Trainer found: {type(trainer).__name__}")

    # ------------------------------------------------------------------
    # Link trainer â†” buffer
    # ------------------------------------------------------------------
    buffer_exists = False
    current_buffer = None

    try:
        current_buffer = trainer.buffer
        buffer_exists = True
        logger.critical(
            f"âœ“ Trainer has buffer: {type(current_buffer).__name__ if current_buffer else 'None'}"
        )
    except Exception:
        buffer_exists = False

    if buffer_exists:
        if current_buffer is not buffer:
            try:
                trainer.buffer = buffer
                logger.critical("âœ… Linked trainer to buffer (V8.2)")
            except Exception as e:
                logger.error(f"âŒ Failed to set buffer: {e}")
                if hasattr(trainer, 'base_trainer'):
                    trainer.base_trainer.buffer = buffer
                    logger.critical("âœ… Linked base_trainer to buffer (V8.2)")
        else:
            logger.critical("âœ… Already linked")
    else:
        try:
            trainer.buffer = buffer
            logger.critical("âœ… Created buffer link (V8.2)")
        except Exception as e:
            logger.error(f"âŒ Cannot create buffer attribute: {e}")
            if hasattr(trainer, 'base_trainer'):
                trainer.base_trainer.buffer = buffer
                logger.critical("âœ… Created buffer link on base_trainer (V8.2)")

    # ------------------------------------------------------------------
    # Add buffer logging (SAFE SIZE HANDLING)
    # ------------------------------------------------------------------
    if hasattr(buffer, 'add'):
        original_add = buffer.add
        counter = {'count': 0}

        def logged_add_v82(*args, **kwargs):
            result = original_add(*args, **kwargs)
            counter['count'] += 1
            if counter['count'] % 10 == 0:
                size = _get_buffer_size(buffer)
                logger.critical(f"ğŸ“¦ BUFFER SIZE (V8.2): {size}")
            return result

        buffer.add = logged_add_v82
        logger.critical("âœ… Added buffer logging (V8.2)")

    # ------------------------------------------------------------------
    # Final size report
    # ------------------------------------------------------------------
    size = _get_buffer_size(buffer)
    logger.critical(f"Current buffer size: {size}")
    logger.critical("="*80)



def apply_all_critical_fixes_v82(system, quantum_advisor):
    """
    V8.2 MASTER FIX FUNCTION - Apply all critical fixes to the quantum trading system.

    This is the ONE function you need to call after initialization to fix everything.

    Usage:
        # After ALL your initialization (system and quantum_advisor created):
        apply_all_critical_fixes_v82(system, quantum_advisor)

        # Then continue with your main loop

    Args:
        system: Your IntegratedSignalSystem or main system object
        quantum_advisor: Your QuantumForecastingAdvisor instance

    Returns:
        bool: True if all fixes applied successfully, False otherwise
    """
    logger.critical("")
    logger.critical("#"*80)
    logger.critical("# QUANTUM TRADING SYSTEM V8.2 - APPLYING ALL CRITICAL FIXES")
    logger.critical("#"*80)
    logger.critical("")
    logger.critical("This will fix:")
    logger.critical("  1. Quantum advisor never being called")
    logger.critical("  2. Agents never learning")
    logger.critical("  3. Silent failures and no visibility")
    logger.critical("")

    try:
        # Apply all 4 fixes
        ensure_quantum_advisor_accessible_v82(system, quantum_advisor)
        patch_prediction_method_v82(system)
        enable_training_logging_v82(system)
        link_buffer_trainer_v82(system)

        logger.critical("")
        logger.critical("#"*80)
        logger.critical("# âœ… ALL V8.2 CRITICAL FIXES APPLIED SUCCESSFULLY")
        logger.critical("#"*80)
        logger.critical("")
        logger.critical("Your system will now:")
        logger.critical("  âœ… Apply quantum advisor to every prediction")
        logger.critical("  âœ… Log all training steps")
        logger.critical("  âœ… Store experiences properly")
        logger.critical("  âœ… Show all operations in logs")
        logger.critical("")
        logger.critical("Watch for these logs:")
        logger.critical("  ğŸ”® FORCING QUANTUM ADVISOR APPLICATION (V8.2)")
        logger.critical("  âœ… Agent quantum adjustments: BUY: Xâ†’Y")
        logger.critical("  ğŸ“¦ BUFFER SIZE (V8.2): N")
        logger.critical("  ğŸ“ TRAINING STEP EXECUTING (V8.2)")
        logger.critical("  âœ… TRAINING COMPLETE (V8.2)")
        logger.critical("")
        logger.critical("If you see all of these â†’ YOUR SYSTEM IS LEARNING! ğŸ‰")
        logger.critical("")
        logger.critical("#"*80)
        logger.critical("")

        return True

    except Exception as e:
        logger.critical("")
        logger.critical("#"*80)
        logger.critical(f"# âŒ V8.2 FIX INTEGRATION FAILED: {e}")
        logger.critical("#"*80)
        logger.critical("")
        import traceback
        traceback.print_exc()
        return False


def verify_v82_fixes(system):
    """
    Verify that all V8.2 fixes have been applied correctly.

    Usage:
        verify_v82_fixes(system)

    Returns:
        bool: True if all checks pass, False otherwise
    """
    logger.critical("")
    logger.critical("="*80)
    logger.critical("ğŸ” VERIFYING V8.2 FIXES")
    logger.critical("="*80)

    checks = []
    passed = 0
    total = 0

    # Check 1: Quantum advisor accessible
    total += 1
    if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
        checks.append("âœ… Quantum advisor on system")
        passed += 1
    else:
        checks.append("âŒ No quantum advisor on system")

    # Check 2: Prediction patched
    total += 1
    if hasattr(system, '_original_predict_all_agents_v82'):
        checks.append("âœ… Prediction method patched (V8.2)")
        passed += 1
    else:
        checks.append("âŒ Prediction method not patched")

    # Check 3: Buffer exists
    total += 1
    if hasattr(system, 'hybrid_buffer') or (hasattr(system, 'quantum_bridge') and hasattr(system.quantum_bridge, 'hybrid_buffer')):
        checks.append("âœ… Buffer found")
        passed += 1
    else:
        checks.append("âŒ Buffer not found")

    # Check 4: Trainer exists - V8.5.8 FIX: Check quantum_trainer
    total += 1
    trainer_found = False
    if hasattr(system, 'quantum_trainer'):
        checks.append("âœ… Trainer found at system.quantum_trainer")
        trainer_found = True
    elif hasattr(system, 'quantum_bridge'):
        if hasattr(system.quantum_bridge, 'quantum_trainer'):
            checks.append("âœ… Trainer found at system.quantum_bridge.quantum_trainer")
            trainer_found = True
        elif hasattr(system.quantum_bridge, 'trainer'):
            checks.append("âœ… Trainer found at system.quantum_bridge.trainer")
            trainer_found = True

    if trainer_found:
        passed += 1
    else:
        checks.append("âŒ Trainer not found")

    # Print results
    for check in checks:
        logger.critical(f"   {check}")

    logger.critical("")
    logger.critical(f"V8.2 Fix Score: {passed}/{total}")
    logger.critical("")

    if passed == total:
        logger.critical("="*80)
        logger.critical("âœ… V8.2 VERIFICATION PASSED")
        logger.critical("="*80)
        logger.critical("All fixes are in place. Continue with your main loop!")
        logger.critical("="*80)
        return True
    else:
        logger.critical("="*80)
        logger.critical("âš ï¸  V8.2 VERIFICATION INCOMPLETE")
        logger.critical("="*80)
        logger.critical(f"Only {passed}/{total} checks passed.")
        logger.critical("Some components may be missing. Check initialization.")
        logger.critical("="*80)
        return False


def verify_quantum_advisor_integration(system):
    """
    Comprehensive verification of quantum advisor integration (V7).
    Run this after system initialization to verify everything works.

    Returns:
        dict: Status of all integration checks
    """
    logger.critical("="*80)
    logger.critical("ğŸ” QUANTUM ADVISOR INTEGRATION VERIFICATION (V7)")
    logger.critical("="*80)

    checks = {
        'quantum_advisor_exists': False,
        'predict_q_adjustment_exists': False,
        'quantum_system_has_advisor': False,
        'quantum_bridge_predict_works': False,
        'quantum_weight_configured': False
    }

    try:
        # Check 1: Quantum advisor exists
        if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
            checks['quantum_advisor_exists'] = True
            logger.critical("âœ… System has quantum_advisor attribute")

            # Check 2: predict_q_adjustment method exists
            if hasattr(system.quantum_advisor, 'predict_q_adjustment'):
                checks['predict_q_adjustment_exists'] = True
                logger.critical("âœ… quantum_advisor.predict_q_adjustment() method exists")

                # Test the method
                try:
                    test_state = torch.randn(1, system.state_dim, device=system.device)
                    test_q = torch.tensor([[0.6, 0.4]], device=system.device)
                    adjusted_q, metadata = system.quantum_advisor.predict_q_adjustment(test_state, test_q)
                    logger.critical(f"âœ… predict_q_adjustment() test passed | Adjustment: {(adjusted_q - test_q).abs().mean().item():.6f}")
                except Exception as e:
                    logger.critical(f"âŒ predict_q_adjustment() test failed: {e}")
            else:
                logger.critical("âŒ predict_q_adjustment() method MISSING!")
        else:
            logger.critical("âŒ quantum_advisor NOT FOUND on system")

        # Check 3: Quantum system connection
        if hasattr(system, 'quantum_system') and system.quantum_system is not None:
            if hasattr(system.quantum_system, 'quantum_advisor'):
                checks['quantum_system_has_advisor'] = True
                logger.critical("âœ… quantum_system has quantum_advisor")

        # Check 4: Quantum bridge prediction
        if hasattr(system, 'quantum_bridge'):
            try:
                # Test predict method
                test_result = system.quantum_bridge.predict_single_agent(list(system.agents.keys())[0])
                checks['quantum_bridge_predict_works'] = True
                logger.critical(f"âœ… quantum_bridge.predict_single_agent() works | Q-values: {test_result}")
            except Exception as e:
                logger.critical(f"âŒ quantum_bridge.predict_single_agent() failed: {e}")

        # Check 5: QUANTUM_WEIGHT configured
        try:
            weight = QUANTUM_WEIGHT
            checks['quantum_weight_configured'] = True
            logger.critical(f"âœ… QUANTUM_WEIGHT = {weight}")
        except:
            logger.critical("âŒ QUANTUM_WEIGHT not defined")

    except Exception as e:
        logger.critical(f"âŒ Verification error: {e}")

    # Summary
    logger.critical("="*80)
    passed = sum(checks.values())
    total = len(checks)
    logger.critical(f"ğŸ“Š VERIFICATION RESULTS: {passed}/{total} checks passed")

    if passed == total:
        logger.critical("âœ…âœ…âœ… ALL CHECKS PASSED - Quantum advisor fully integrated! âœ…âœ…âœ…")
    else:
        logger.critical("âŒ SOME CHECKS FAILED - Review issues above")
        for check, status in checks.items():
            if not status:
                logger.critical(f"   âŒ {check}")

    logger.critical("="*80)

    return checks



# ============================================================================
# V8.5.9 DIAGNOSTIC UTILITIES
# ============================================================================

def diagnose_ably_connection(system):
    """Check if Ably is connected and receiving rewards"""
    print(f"\nğŸ“¡ [ABLY CHECK]")
    print(f"="*80)

    try:
        if hasattr(system, 'ably'):
            print(f"   âœ… system.ably exists")

            if system.ably is not None:
                print(f"   âœ… Ably client initialized")

                try:
                    state = getattr(system.ably.connection, 'state', 'unknown')
                    print(f"   Connection state: {state}")

                    if state != 'connected':
                        print(f"   âŒ WARNING: Ably NOT connected!")
                        print(f"   âš ï¸  NO rewards will be received from Ably!")
                        print(f"   Action: Check API key and network connection")
                        return False
                    else:
                        print(f"   âœ… Ably is connected")
                except Exception as e:
                    print(f"   âš ï¸  Could not verify connection state: {e}")

                return True
            else:
                print(f"   âŒ Ably client is None!")
                return False
        else:
            print(f"   âŒ system.ably attribute missing!")
            return False

    except Exception as e:
        print(f"   âŒ Error: {e}")
        return False
    finally:
        print(f"="*80)


def diagnose_signal_generation(system):
    """Check if signals are being generated and tracked"""
    print(f"\nğŸ“Š [SIGNAL GENERATION CHECK]")
    print(f"="*80)

    try:
        # Check partial experiences (pending signals)
        if hasattr(system, 'partial_experiences'):
            partial_count = len(system.partial_experiences)
            print(f"   Partial experiences (pending): {partial_count}")

            if partial_count > 0:
                print(f"   âœ… Signals ARE being generated!")
                sample_keys = list(system.partial_experiences.keys())[:3]
                print(f"   Sample keys: {sample_keys}")

                # Show age of oldest pending signal
                if sample_keys:
                    import time
                    try:
                        oldest_key = min(system.partial_experiences.keys(),
                                       key=lambda k: system.partial_experiences[k].get('timestamp', time.time()))
                        oldest_exp = system.partial_experiences[oldest_key]
                        age = time.time() - oldest_exp.get('timestamp', time.time())
                        print(f"   Oldest pending signal age: {age:.1f}s")

                        if age > 300:  # 5 minutes
                            print(f"   âš ï¸  WARNING: Old signals pending!")
                            print(f"   This suggests rewards are NOT arriving from Ably")
                    except:
                        pass
            else:
                print(f"   âš ï¸  NO partial experiences waiting")
                print(f"   Possible reasons:")
                print(f"      1. No signals being generated (price data issue)")
                print(f"      2. Signals completed instantly (unlikely)")
                print(f"      3. System just started")
        else:
            print(f"   âŒ system.partial_experiences missing!")

        # Check completed experiences
        if hasattr(system, 'experience_buffer'):
            completed_count = len(system.experience_buffer)
            print(f"   Completed experiences: {completed_count}")
        else:
            print(f"   âš ï¸  experience_buffer attribute missing")

    except Exception as e:
        print(f"   âŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"="*80)


# ============================================================================
# V8.5.9 FIX: BUFFER LINKING UTILITY
# ============================================================================

def link_training_buffers(system):
    """
    Links all buffers so rewards flow to training correctly.
    This fixes the missing training logs issue.

    CRITICAL: This must be called after system initialization but before
    the training loop starts.
    """
    print("\n" + "="*80)
    print("ğŸ”§ V8.5.9 FIX: LINKING TRAINING BUFFERS")
    print("="*80)

    try:
        # Check what exists
        has_exp_buffer = hasattr(system, 'experience_buffer')
        has_quantum_bridge = hasattr(system, 'quantum_bridge')

        print(f"   experience_buffer exists: {has_exp_buffer}")
        print(f"   quantum_bridge exists: {has_quantum_bridge}")

        if not has_exp_buffer:
            print("   âŒ experience_buffer not found - creating it")
            from collections import deque
            system.experience_buffer = deque(maxlen=100000000)

        if not has_quantum_bridge:
            print("   âŒ quantum_bridge not found - cannot link buffers")
            print("="*80 + "\n")
            return False

        # Link hybrid_buffer to experience_buffer
        if not hasattr(system.quantum_bridge, 'hybrid_buffer'):
            print("   âš ï¸  hybrid_buffer not found - creating link")
            system.quantum_bridge.hybrid_buffer = system.experience_buffer
        else:
            hybrid_size = len(system.quantum_bridge.hybrid_buffer) if system.quantum_bridge.hybrid_buffer else 0
            exp_size = len(system.experience_buffer)

            print(f"   Current hybrid_buffer size: {hybrid_size}")
            print(f"   Current experience_buffer size: {exp_size}")

            if hybrid_size == 0:
                print(f"   ğŸ”— Linking hybrid_buffer â†’ experience_buffer ({exp_size} experiences)")
                system.quantum_bridge.hybrid_buffer = system.experience_buffer
            else:
                print(f"   â„¹ï¸  Buffers already populated")

        # Link trainer buffer
        if hasattr(system.quantum_bridge, 'quantum_trainer'):
            print("   quantum_trainer exists")
            if hasattr(system.quantum_bridge.quantum_trainer, 'buffer'):
                print(f"   ğŸ”— Linking trainer.buffer â†’ hybrid_buffer")
                system.quantum_bridge.quantum_trainer.buffer = system.quantum_bridge.hybrid_buffer
            else:
                print("   âš ï¸  quantum_trainer has no buffer attribute")
        else:
            print("   âš ï¸  quantum_trainer not found")

        # Verify the link
        final_size = len(system.quantum_bridge.hybrid_buffer) if system.quantum_bridge.hybrid_buffer else 0
        print(f"\n   âœ… BUFFERS LINKED SUCCESSFULLY")
        print(f"   Current buffer size: {final_size}/64 required for training")

        if final_size >= 64:
            print(f"   ğŸ“ READY TO TRAIN!")
        else:
            print(f"   â³ Need {64 - final_size} more experiences")

        print("="*80 + "\n")
        return True

    except Exception as e:
        print(f"   âŒ Buffer linking failed: {e}")
        import traceback
        traceback.print_exc()
        print("="*80 + "\n")
        return False

# ============================================================================

def quantum_integrated_main():
    """
    Full CPU/GPU-safe Quantum Trading System initializer.
    Handles:
      - Device selection (CPU/GPU)
      - Paths & Google Cloud Storage setup
      - Ably realtime async-safe initialization
      - QuantumAgents creation
      - MultiTimeframeFusion
      - QuantumSystemBridge
      - IntegratedSignalSystem
      - RewardBatchProcessor
      - AutosaveManager
      - Optional subscriptions and verification
      - V8.2 CRITICAL FIXES (Quantum Advisor + Training + Buffer)
    """
    import nest_asyncio
    nest_asyncio.apply()

    logger.critical("="*80)
    logger.critical("QUANTUM TRADING SYSTEM INITIALIZATION")
    logger.critical("="*80)

    # ------------------------
    # DEVICE SETUP
    # ------------------------
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        torch.cuda.empty_cache()
        gpu_name = torch.cuda.get_device_name(0)
        mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.critical(f"GPU active: {gpu_name} ({mem_gb:.2f} GB)")
    else:
        logger.warning("âš  CUDA not available â€” using CPU")

    # ------------------------
    # PATHS
    # ------------------------
    BASE_PATH = "/content/drive/MyDrive/RLTradingBot/Quantum"
    AGENT_PATH = os.path.join(BASE_PATH, "agents")
    os.makedirs(AGENT_PATH, exist_ok=True)

    GCS_CREDENTIALS = "/content/poised-team-467804-t9-73b24de7b4aa.json"
    GCS_BUCKET = "my-ml-buffer-2025"
    GCS_META_DIR = "quantum_meta"
    gcs_bucket = None
    gcs_available = True

    # ------------------------
    # GOOGLE CLOUD STORAGE
    # ------------------------
    try:
        from google.cloud import storage
        if os.path.exists(GCS_CREDENTIALS):
            client = storage.Client.from_service_account_json(GCS_CREDENTIALS)
            gcs_bucket = client.bucket(GCS_BUCKET)
            # Test write/read
            test_blob = gcs_bucket.blob(f"{GCS_META_DIR}/_test.txt")
            test_blob.upload_from_string("test")
            test_blob.delete()
            gcs_available = True
            logger.critical("âœ“ GCS initialized successfully")
        else:
            logger.warning("âš  GCS credentials not found")
    except Exception as e:
        logger.warning(f"âš  GCS unavailable: {e}")

    # ------------------------
    # ABLY INITIALIZATION (async-safe)
    # ------------------------
    ably_client = None
    try:
        from ably import AblyRealtime
        ABLY_KEY = "4vT80g.E0lfvg:reqqX942--QJVafOQsgRDWsBXIDtgDxg51szTmLkIeM"

        async def init_ably_async():
            client = AblyRealtime(ABLY_KEY)
            await asyncio.sleep(1.0)
            return client

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        if loop.is_running():
            ably_task = asyncio.ensure_future(init_ably_async(), loop=loop)
            start = time.time()
            while time.time() - start < 10:
                if ably_task.done():
                    ably_client = ably_task.result()
                    break
                time.sleep(0.25)
        else:
            ably_client = loop.run_until_complete(init_ably_async())

        if ably_client:
            connected = False
            start = time.time()
            while time.time() - start < 10:
                try:
                    if getattr(ably_client.connection, "state", None) == "connected":
                        connected = True
                        break
                except Exception:
                    pass
                time.sleep(0.25)
            if connected:
                logger.critical("âœ“ Ably connected successfully")
            else:
                try: ably_client.close()
                except Exception: pass
                logger.warning("âš  Ably connection failed â€” offline mode")
                ably_client = None
    except Exception as e:
        logger.warning(f"âš  Ably initialization failed ({e})")
        ably_client = None

    # ------------------------
    # QUANTUM AGENTS
    # ------------------------

    agent_names = list(TIMEFRAME_LENGTHS.keys())
    state_dim = 58
    action_dim = 2

    # ------------------------
    # QUANTUM BRIDGE (CREATE FIRST)
    # ------------------------
    # V8.5.10: Create quantum_bridge FIRST so we can use its agents
    quantum_bridge = QuantumSystemBridge(
        agent_names=agent_names,
        state_dim=state_dim,
        action_dim=action_dim,
        latent_dim=32,
        device=device
    )
    logger.critical("âœ“ Quantum Bridge initialized")

    # âœ… ADD THIS VALIDATION:
    if not hasattr(quantum_bridge, 'cache_lock'):
        logger.critical("âŒ CRITICAL: quantum_bridge missing cache_lock!")
        quantum_bridge.cache_lock = threading.Lock()
        logger.info("âœ“ Created missing cache_lock")

    if not hasattr(quantum_bridge, 'state_cache'):
        logger.critical("âŒ CRITICAL: quantum_bridge missing state_cache!")
        quantum_bridge.state_cache = {}
        logger.info("âœ“ Created missing state_cache")

    # V8.5.10: USE quantum_bridge.quantum_system.agents as the PRIMARY agents
    # This ensures checkpoint save/load and training use the SAME agents
    if hasattr(quantum_bridge, 'quantum_system') and quantum_bridge.quantum_system is not None:
        if hasattr(quantum_bridge.quantum_system, 'agents'):
            quantum_agents = dict(quantum_bridge.quantum_system.agents)
            logger.critical(f"âœ“ Using quantum_bridge.quantum_system.agents ({len(quantum_agents)} agents)")
            logger.critical(f"   Agent types: {[type(a).__name__ for a in quantum_agents.values()]}")
        else:
            logger.error("âŒ quantum_bridge.quantum_system has no agents!")
            quantum_agents = {}
    else:
        logger.error("âŒ quantum_bridge.quantum_system is None!")
        quantum_agents = {}

    # Fallback: Create QuantumAgents if quantum_bridge didn't provide them
    if not quantum_agents:
        logger.warning("âš ï¸ Falling back to creating QuantumAgent instances")
        quantum_agents = {}
        for name in agent_names:
            try:
                agent = QuantumAgent(
                    name=name,
                    seq_len=TIMEFRAME_LENGTHS[name],
                    state_dim=state_dim,
                    action_dim=action_dim,
                    device=device,
                    base_path=os.path.join(AGENT_PATH, name),
                    gcs_bucket=gcs_bucket if gcs_available else None,
                    gcs_path=f"{GCS_META_DIR}/agents/{name}_quantum.zip" if gcs_available else None
                )
                quantum_agents[name] = agent
                logger.info(f"âœ“ Fallback Agent {name} initialized")
            except Exception as e:
                logger.error(f"âš  Agent {name} failed: {e}")

    # Link quantum_bridge to all agents
    for agent in quantum_agents.values():
        agent.quantum_bridge = quantum_bridge

    # ========================================================================
    # MULTI-INSTRUMENT CHECKPOINT MANAGER (V3.0)
    # ========================================================================
    print("\n" + "="*80)
    print("âš¡ INITIALIZING MULTI-INSTRUMENT CHECKPOINT MANAGER")
    print("="*80)

    # GCS Configuration Diagnostics
    print("ğŸ” GCS Configuration Check:")
    print(f"   GCS_BUCKET: {GCS_BUCKET}")
    print(f"   GCS_CREDENTIALS: {GCS_CREDENTIALS}")
    print(f"   Credentials exist: {os.path.exists(GCS_CREDENTIALS)}")
    print(f"   gcs_available flag: {gcs_available}")

    checkpoint_mgr = None
    try:



        checkpoint_mgr = MultiInstrumentCheckpointManager(
            local_checkpoint_dir="./saves",

            # Hugging Face dataset repo (no repo_type needed)
            hf_repo_id="KarlQuant/k1rl-checkpoints",

            # HARDCODED HF TOKEN
            hf_token="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",

            current_instrument="V250",
            current_voyage=1,

            autosave_interval_minutes=10,
            max_checkpoints_per_voyage=5,
            enable_async_save=False,
            enable_compression=True
        )



        if gcs_available:
            print("âš ï¸ GCS is still enabled â€” but this manager is HF-only, skipping GCS injection")
        else:
            print("âš¡ Using Hugging Face backend only")


        # Attach to quantum_bridge
        quantum_bridge.checkpoint_manager = checkpoint_mgr
        quantum_bridge.voting_trainer = quantum_bridge.quantum_trainer if hasattr(quantum_bridge, 'quantum_trainer') else None

        print(f"âœ“ Checkpoint Manager initialized")
        print(f"   Instrument: {checkpoint_mgr.get_current_instrument_name()}")
        print(f"   Voyage: v{checkpoint_mgr._current_voyage}")
        print(f"   Local Dir: {checkpoint_mgr.local_base_dir}")
        print(f"   GCS Prefix: {checkpoint_mgr.gcs_prefix}")

        # GCS Status
        if checkpoint_mgr.gcs_bucket is not None:
            print(f"   â˜ï¸  GCS: ENABLED (gs://{checkpoint_mgr.gcs_bucket_name}/{checkpoint_mgr.gcs_prefix})")
        else:
            print(f"   âš ï¸  GCS: DISABLED (local saves only)")
            print(f"   âš ï¸  Check: Is GCS client initialized? gcs_bucket={checkpoint_mgr.gcs_bucket}")

        # V8.5.10: DEFER checkpoint loading until AFTER all components are initialized
        # The checkpoint will be loaded after IntegratedSignalSystem, quantum_advisor, etc.
        print("ğŸ“Œ Checkpoint loading DEFERRED until all components initialized")
        loaded_training_steps = 0  # Will be updated after deferred load
        checkpoint_was_loaded = False  # Will be updated after deferred load

    except Exception as e:
        print(f"âš ï¸  Checkpoint Manager failed: {e}")
        print("   System will continue without checkpointing")
        import traceback
        traceback.print_exc()
        checkpoint_mgr = None
        loaded_training_steps = 0
        checkpoint_was_loaded = False

    print("="*80 + "\n")

    # ------------------------
    # INTEGRATED SIGNAL SYSTEM
    # ------------------------
    system = IntegratedSignalSystem(
        agents=quantum_agents,
        state_dim=state_dim,
        action_dim=action_dim,
        google_drive_base_path=BASE_PATH,
        ably_realtime=ably_client,
        gcs_bucket=gcs_bucket if gcs_available else None,
        gcs_meta_dir=GCS_META_DIR,
        device=device
    )
    system.quantum_bridge = quantum_bridge
    system.quantum_voting = QuantumVotingSystem(quantum_bridge=quantum_bridge, device=device)
    logger.critical(f"âœ“ Quantum system initialized with {len(system.agents)} agents")
    setattr(system, "quantum_agents", {name: None for name in system.agents.keys()})

    # V8.5.10: Store loaded training steps on system for later use
    system._loaded_training_steps = loaded_training_steps
    system._checkpoint_was_loaded = checkpoint_was_loaded

    # ========================================================================
    # V8.5.10: EMERGENCY FIX DEFERRED
    # ========================================================================
    # The emergency critic fix is now called AFTER checkpoint loading
    # to ensure loaded weights are not overwritten.
    # See the deferred checkpoint loading section below.
    # ========================================================================
    logger.critical("ğŸ“Œ V8.5.10: Emergency critic fix DEFERRED until after checkpoint loading")


    # Now quantum training/predictions will be logged to Discord
    # And the AttributeError warnings will be gone!
    # ------------------------
    # REWARD BATCH PROCESSOR
    # ------------------------
    try:
        try: loop = asyncio.get_running_loop(); running_loop = True
        except RuntimeError: loop = asyncio.new_event_loop(); asyncio.set_event_loop(loop); running_loop=False
        batch_processor = RewardBatchProcessor(system=system, batch_size=64, flush_interval=0.2)
        system.batch_processor = batch_processor
        if running_loop: asyncio.ensure_future(batch_processor.start(), loop=loop)
        else: loop.run_until_complete(batch_processor.start())
        logger.critical("âœ“ RewardBatchProcessor started")
    except Exception as e:
        logger.error(f"Failed to start RewardBatchProcessor: {e}")
        system.batch_processor = None

    # ------------------------
    # SUBSCRIPTIONS
    # ------------------------
    try:
        for fn_name in ['_subscribe_reward_channels', '_subscribe_confirmation_channels', '_subscribe_shutdown_channel']:
            if hasattr(system, fn_name):
                if loop.is_running():
                    asyncio.ensure_future(getattr(system, fn_name)(), loop=loop)
                    logger.info(f"Scheduled {fn_name}()")
                else:
                    loop.run_until_complete(getattr(system, fn_name)())
                    logger.info(f"Executed {fn_name}()")
    except Exception as e:
        logger.warning(f"Subscription scheduling failed: {e}")

    # ------------------------
    # AUTOSAVE MANAGER
    # ------------------------
    autosave = None
    try:
        autosave = AutosaveManager(
            system.agents,
            system,
            bucket=gcs_bucket if gcs_available else None,
            agent_gcs_dir=f"{GCS_META_DIR}/agents" if gcs_available else None,
            system_gcs_dir=GCS_META_DIR if gcs_available else None,
            interval=1200
        )
        threading.Thread(target=autosave.run, daemon=True).start()
        logger.critical("âœ“ AutosaveManager running")
    except Exception as e:
        logger.warning(f"âš  AutosaveManager failed: {e}")

    # ------------------------
    # MAIN LOOP
    # ------------------------
    logger.critical("Entering main loop")
    system._running = True
    system._last_activity = time.time()
    system._last_status_check = time.time()
    # âœ… ADD THIS LINE to fix Discord warnings and add quantum logging
    attach_inference_diagnostics(system)
    quantum_logger = setup_quantum_discord_integration(system)
    system._last_diagnostic = time.time()

    # ==========================================================
    # ğŸ§  Ensure Reward Batch Processor exists before integrations
    # ==========================================================
    logger.info("Initializing RewardBatchProcessor if not present.")

    if not getattr(system, "batch_processor", None):
        # Use the proper RewardBatchProcessor class (defined at line ~6959)
        # with correct parameters: state_dim and action_dim
        system.batch_processor = RewardBatchProcessor(
            state_dim=system.state_dim,
            action_dim=2
        )
        logger.info("âœ… Batch processor initialized and attached to system")

    WEBHOOK_URL = "https://discordapp.com/api/webhooks/1422597824851345489/bmJgtiL_jyjW1XTBErBrlrtMF9atVnX7CzwIUVOhrbd2hiPtklD6sZJpk8KqLNlCyIGN"
    integrate_discord_with_quantum_system(system, WEBHOOK_URL)

    # ============================================================================
    # ğŸ”® QUANTUM ADVISOR INTEGRATION (BUG FIX #3)
    # ============================================================================
    # CRITICAL: This MUST be done BEFORE the main loop starts, otherwise the
    # advisor never gets initialized and no advisor logs will appear.
    # Previously this code was after the finally block (unreachable dead code).
    # ============================================================================
    logger.critical("="*80)
    logger.critical("ğŸ”® INITIALIZING QUANTUM ADVISOR")
    logger.critical("="*80)

    try:
        # Check if advisor exists
        has_advisor = hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None

        if not has_advisor:
            logger.critical("ğŸ“ Creating Quantum Forecasting Advisor...")

            # Get state dimension from agents
            state_dim = 58  # Default
            if system.agents:
                sample_agent = next(iter(system.agents.values()))
                if hasattr(sample_agent, 'state_dim'):
                    state_dim = sample_agent.state_dim
                    logger.critical(f"   Detected state_dim: {state_dim}")

            # Call integration function
            status = integrate_quantum_advisor(system)

            if status.get('advisor_created', False):
                logger.critical(f"âœ… Quantum advisor created successfully")
                logger.critical(f"   State dim: {state_dim}")
                logger.critical(f"   Forecast horizon: 5")
                logger.critical(f"   N qubits: 8")

                # Link advisor to all agents
                if hasattr(system, 'quantum_advisor') and system.quantum_advisor:
                    for name, agent in system.agents.items():
                        if not hasattr(agent, 'quantum_advisor'):
                            agent.quantum_advisor = system.quantum_advisor
                            logger.critical(f"  [{name}] Quantum advisor linked")
                        else:
                            agent.quantum_advisor = system.quantum_advisor
                            logger.critical(f"  [{name}] Quantum advisor updated")

                # Link quantum_advisor to bridge for checkpoint compatibility
                if hasattr(quantum_bridge, 'checkpoint_manager') and quantum_bridge.checkpoint_manager:
                    quantum_bridge.quantum_advisor = system.quantum_advisor
                    print("   âœ“ Quantum advisor linked to bridge (checkpoint ready)")
            else:
                logger.critical("âš ï¸ Quantum advisor creation failed")
                logger.critical(f"   Status: {status}")
        else:
            logger.critical("âœ… Quantum advisor already exists")

        # Verify agents have access to advisor
        logger.critical("\nğŸ“‹ Agent Advisor Status:")
        for name, agent in system.agents.items():
            if hasattr(agent, 'quantum_advisor'):
                advisor_status = "âœ… Active" if agent.quantum_advisor else "âŒ None"
            else:
                advisor_status = "âŒ No attribute"
            agent_type = type(agent).__name__
            logger.critical(f"  [{name}] Type: {agent_type} | Advisor: {advisor_status}")

    except Exception as e:
        logger.critical(f"âŒ Quantum advisor integration failed: {e}")
        import traceback
        traceback.print_exc()

    logger.critical("="*80)
    logger.critical("")


    # ============================================================================
    # ğŸ”® QUANTUM ADVISOR FIX APPLICATION
    # ============================================================================
    # Apply the complete quantum advisor diagnostic and fix
    # This ensures:
    # 1. Quantum advisor is properly initialized
    # 2. All agents are linked to the advisor
    # 3. Comprehensive logging is enabled
    # 4. Predict methods are patched to show quantum influence
    # ============================================================================
    logger.critical("="*80)
    logger.critical("ğŸ”§ APPLYING QUANTUM ADVISOR FIX")
    logger.critical("="*80)

    try:
        # Apply the complete fix
        diagnostic_results = apply_complete_quantum_advisor_fix(system)

        # Log the results
        if diagnostic_results and not diagnostic_results.get('issues'):
            logger.critical("âœ… QUANTUM ADVISOR FIX APPLIED SUCCESSFULLY")
            logger.critical("   All agents are now ready to log quantum predictions!")
        else:
            logger.critical("âš ï¸ QUANTUM ADVISOR FIX COMPLETED WITH WARNINGS")
            if diagnostic_results and diagnostic_results.get('issues'):
                for issue in diagnostic_results['issues']:
                    logger.critical(f"   - {issue}")

    except Exception as e:
        logger.critical(f"âŒ QUANTUM ADVISOR FIX ERROR: {e}")
        import traceback
        traceback.print_exc()
        logger.critical("   System will continue, but quantum advisor logs may not appear")

    logger.critical("="*80)
    logger.critical("")
    # ============================================================================
    apply_quantum_advisor_hotfixes(system)

    # ============================================================================
    # ğŸ”¥ V8.2 CRITICAL FIX APPLICATION
    # ============================================================================
    # This section applies ALL critical fixes needed to make the system learn:
    # 1. Forces quantum advisor accessibility everywhere
    # 2. Patches prediction method to use quantum advisor
    # 3. Enables training with comprehensive logging
    # 4. Links buffer to trainer properly
    #
    # CRITICAL: This must run AFTER quantum advisor initialization but BEFORE
    # the main loop starts. Without this, you'll see:
    # - "âŒ NO QUANTUM ADVISOR FOUND" in predictions
    # - No training happening
    # - Identical Q-values forever
    # ============================================================================
    logger.critical("="*80)
    logger.critical("ğŸ”¥ APPLYING V8.2 CRITICAL FIXES")
    logger.critical("="*80)
    logger.critical("")

    try:
        # Get the quantum advisor (it was created in the section above)
        quantum_advisor = None

        # Try to find quantum advisor in multiple locations
        if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
            quantum_advisor = system.quantum_advisor
            logger.critical(f"âœ“ Found quantum_advisor on system: {type(quantum_advisor).__name__}")
        elif hasattr(system, 'quantum_bridge') and hasattr(system.quantum_bridge, 'quantum_system'):
            if hasattr(system.quantum_bridge.quantum_system, 'quantum_advisor'):
                quantum_advisor = system.quantum_bridge.quantum_system.quantum_advisor
                logger.critical(f"âœ“ Found quantum_advisor on quantum_system: {type(quantum_advisor).__name__}")

        # Create quantum advisor if somehow still missing
        if quantum_advisor is None:
            logger.critical("âš ï¸  WARNING: quantum_advisor not found, creating it now...")
            quantum_advisor = QuantumForecastingAdvisor(
                state_dim=58,
                forecast_horizon=30,
                n_qubits=6,
                device=device
            )
            system.quantum_advisor = quantum_advisor
            logger.critical("âœ“ Created new quantum_advisor")

        logger.critical("")
        logger.critical("Applying 4 critical fixes...")
        logger.critical("")

        # Apply all V8.2 fixes with one function call
        success = apply_all_critical_fixes_v82(system, quantum_advisor)

        if success:
            logger.critical("")
            logger.critical("="*80)
            logger.critical("âœ… V8.2 FIXES APPLIED SUCCESSFULLY")
            logger.critical("="*80)
            logger.critical("")

            # Verify the fixes
            verify_v82_fixes(system)

            logger.critical("")
            logger.critical("ğŸ‰ SYSTEM IS NOW READY TO LEARN!")
            logger.critical("")
            logger.critical("Watch for these logs during trading:")
            logger.critical("   ğŸ”® FORCING QUANTUM ADVISOR APPLICATION (V8.2)")
            logger.critical("   âœ… xs: BUY: 0.0427â†’0.0501 | SELL: 0.0843â†’0.0921")
            logger.critical("   ğŸ“¦ BUFFER SIZE (V8.2): 70")
            logger.critical("   ğŸ“ TRAINING STEP EXECUTING (V8.2)...")
            logger.critical("   âœ… TRAINING COMPLETE (V8.2) | losses shown")
            logger.critical("")
            logger.critical("If you see ALL of these â†’ Your system is learning! ğŸ‰")

            logger.critical("="*80)
            logger.critical("ğŸ” V8.5.2 COMPONENT VERIFICATION")
            logger.critical("="*80)
            logger.critical("")

            # V8.5.2 FIX #2: Verify all system components
            try:
                checks, passed, total = system.verify_system_components()
                logger.critical(f"Component verification complete: {passed}/{total} passed")
            except Exception as e:
                logger.critical(f"âš ï¸  Component verification error: {e}")
                logger.critical("System will continue but some components may be missing")

            logger.critical("="*80)
            logger.critical("="*80)
            logger.critical("")
        else:
            logger.critical("")
            logger.critical("="*80)
            logger.critical("âŒ V8.2 FIX APPLICATION FAILED")
            logger.critical("="*80)
            logger.critical("System will continue but may not learn properly")
            logger.critical("Check logs above for errors")
            logger.critical("")

    except Exception as e:
        logger.critical("")
        logger.critical("="*80)
        logger.critical(f"âŒ V8.2 FIX ERROR: {e}")
        logger.critical("="*80)
        import traceback
        traceback.print_exc()
        logger.critical("")
        logger.critical("System will continue but V8.2 fixes may not be active")
        logger.critical("You may see 'âŒ NO QUANTUM ADVISOR FOUND' in predictions")
        logger.critical("")

    logger.critical("="*80)
    logger.critical("")
    # ============================================================================
    # END OF V8.2 CRITICAL FIX APPLICATION
    # ============================================================================

    periodic_health_check(system)

    # ============================================================================
    # V8.5.9 FIX: LINK TRAINING BUFFERS
    # ============================================================================
    link_training_buffers(system)
    # ============================================================================

    # ============================================================================
    # V8.5.10: DEFERRED CHECKPOINT LOADING
    # ============================================================================
    # CRITICAL: Load checkpoint AFTER all components are initialized to avoid
    # overwriting loaded weights with fresh initializations.
    #
    # Previously, checkpoint was loaded BEFORE IntegratedSignalSystem, quantum_advisor,
    # etc. were created, so the freshly initialized components overwrote the loaded weights.
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ“¥ V8.5.10: DEFERRED CHECKPOINT LOADING")
    print("="*80)
    print("   Loading checkpoint AFTER all components initialized")
    print("   This ensures loaded weights are NOT overwritten by fresh initializations")
    print("="*80)

    loaded_training_steps = 0
    checkpoint_was_loaded = False

    try:
        ckpt_mgr = getattr(quantum_bridge, 'checkpoint_manager', None)

        if ckpt_mgr is not None:
            print("ğŸ” Checking for existing checkpoints...")

            # Ensure quantum_advisor is linked to bridge before loading
            if hasattr(system, 'quantum_advisor') and system.quantum_advisor is not None:
                quantum_bridge.quantum_advisor = system.quantum_advisor
                print("   âœ“ quantum_advisor linked to bridge")

            # Ensure quantum_trainer is linked
            if hasattr(quantum_bridge, 'quantum_trainer') and quantum_bridge.quantum_trainer is not None:
                quantum_bridge.voting_trainer = quantum_bridge.quantum_trainer
                print("   âœ“ quantum_trainer linked to bridge")

            try:
                load_result = ckpt_mgr.load_to_bridge(
                    bridge=quantum_bridge,
                    device=str(device)
                )

                if load_result.success:
                    ckpt_id = f"{load_result.instrument}_v{load_result.voyage_number}_step_{load_result.training_steps}"
                    print(f"\nâœ… CHECKPOINT LOADED!")
                    print(f"   ID: {ckpt_id}")
                    print(f"   Training Steps: {load_result.training_steps}")
                    print(f"   Timestamp: {load_result.timestamp}")
                    print(f"   File Size: {load_result.file_size_mb:.2f} MB")
                    print(f"   Components loaded: {load_result.get_summary()}")

                    # V8.5.10: Store loaded training steps for resume
                    loaded_training_steps = load_result.training_steps
                    checkpoint_was_loaded = True
                    print(f"\n   âœ… RESUME MODE: Will continue from step {loaded_training_steps}")
                    print(f"   âœ… Loaded weights preserved (post-initialization load)")
                else:
                    print("\nâ„¹ï¸  No existing checkpoint - starting fresh")
                    loaded_training_steps = 0
                    checkpoint_was_loaded = False
                    if load_result.errors:
                        for err in load_result.errors:
                            print(f"      Error: {err}")

            except Exception as load_err:
                print(f"\nâ„¹ï¸  Checkpoint load skipped: {load_err}")
                print("   Starting with fresh weights")
                loaded_training_steps = 0
                checkpoint_was_loaded = False
        else:
            print("âš ï¸  No checkpoint manager available")

    except Exception as e:
        print(f"âš ï¸  Deferred checkpoint load failed: {e}")
        import traceback
        traceback.print_exc()

    # Update system with loaded values
    system._loaded_training_steps = loaded_training_steps
    system._checkpoint_was_loaded = checkpoint_was_loaded

    # ========================================================================
    # V8.5.10: VERIFY AGENTS ARE UNIFIED
    # ========================================================================
    # Since we now use quantum_bridge.quantum_system.agents as system.agents,
    # the checkpoint loads DIRECTLY into the agents used by the system.
    # No weight transfer needed for agents - they're the same objects!
    # ========================================================================
    if checkpoint_was_loaded and loaded_training_steps > 0:
        print("\n" + "="*80)
        print("ğŸ”„ V8.5.10: VERIFYING UNIFIED AGENT ARCHITECTURE")
        print("="*80)

        # Verify agents are the same objects
        agents_unified = True
        if hasattr(quantum_bridge, 'quantum_system') and quantum_bridge.quantum_system is not None:
            if hasattr(quantum_bridge.quantum_system, 'agents'):
                bridge_agents = quantum_bridge.quantum_system.agents
                system_agents = system.agents

                for name in system_agents.keys():
                    if name in bridge_agents:
                        if system_agents[name] is bridge_agents[name]:
                            print(f"   âœ… agent_{name}: UNIFIED (same object)")
                        else:
                            print(f"   âš ï¸  agent_{name}: SEPARATE (different objects!)")
                            agents_unified = False
                    else:
                        print(f"   âŒ agent_{name}: NOT IN BRIDGE")
                        agents_unified = False

        if agents_unified:
            print(f"\n   âœ… All agents are unified - checkpoint loaded directly!")
            print(f"   âœ… No weight transfer needed - agents ARE the checkpoint targets")
        else:
            print(f"\n   âš ï¸  Agents not unified - attempting weight transfer...")
            try:
                transferred, failed = transfer_loaded_weights_to_system(
                    quantum_bridge=quantum_bridge,
                    system=system,
                    device=str(device)
                )

                if len(transferred) > 0:
                    print(f"\n   âœ… WEIGHT TRANSFER COMPLETE: {len(transferred)} components")
                else:
                    print(f"\n   âš ï¸  No weights transferred - architectures may differ")

            except Exception as transfer_err:
                print(f"\n   âŒ Weight transfer error: {transfer_err}")
                import traceback
                traceback.print_exc()

        # Still transfer non-agent components (latent_encoder, optimizers, etc.)
        print("\n   ğŸ“Š Transferring non-agent components...")
        try:
            # Transfer latent encoder if system has its own
            if hasattr(system, 'latent_encoder') and system.latent_encoder is not None:
                if hasattr(quantum_bridge.quantum_system, 'latent_encoder'):
                    src = quantum_bridge.quantum_system.latent_encoder
                    dst = system.latent_encoder
                    if src is not dst:  # Different objects
                        try:
                            dst.load_state_dict(src.state_dict())
                            print("      âœ… latent_encoder transferred")
                        except Exception as e:
                            print(f"      âš ï¸  latent_encoder transfer failed: {e}")
                    else:
                        print("      âœ… latent_encoder: already unified")

            # Transfer quantum_advisor if needed
            if hasattr(quantum_bridge, 'quantum_advisor') and quantum_bridge.quantum_advisor is not None:
                if hasattr(system, 'quantum_advisor'):
                    if system.quantum_advisor is not quantum_bridge.quantum_advisor:
                        try:
                            system.quantum_advisor.load_state_dict(
                                quantum_bridge.quantum_advisor.state_dict()
                            )
                            print("      âœ… quantum_advisor weights transferred")
                        except:
                            system.quantum_advisor = quantum_bridge.quantum_advisor
                            print("      âœ… quantum_advisor replaced with bridge's")
                    else:
                        print("      âœ… quantum_advisor: already unified")

        except Exception as e:
            print(f"      âš ï¸  Non-agent transfer error: {e}")

        print("="*80 + "\n")
    else:
        print("\nğŸ“Œ Fresh start - no weight transfer needed\n")
    # ========================================================================

    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V8.5.10: EMERGENCY CRITIC FIX (POST-CHECKPOINT LOADING)
    # ============================================================================
    # This fix now runs AFTER checkpoint loading to ensure:
    # 1. If checkpoint had critics â†’ they are preserved (not overwritten)
    # 2. If critics are still missing â†’ fresh ones are created
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ”§ V8.5.10: EMERGENCY CRITIC FIX (POST-CHECKPOINT)")
    print("="*80)
    print("   Running AFTER checkpoint load to preserve loaded weights")
    print("   Only missing components will be created")
    print("="*80 + "\n")

    logger.critical("ğŸš¨ Deploying V6 emergency fix (V8.5.10) - POST-CHECKPOINT")
    logger.critical("   Will only create critics if MISSING after checkpoint load")

    # Call the V8.5.10 version which checks before creating
    main_fixed, quantum_fixed = emergency_quantumagent_critic_fix_v6(system, learning_rate=1e-5)

    # Validate
    total_quantum = 0
    if hasattr(system, 'quantum_bridge') and system.quantum_bridge:
        if hasattr(system.quantum_bridge, 'quantum_system'):
            if hasattr(system.quantum_bridge.quantum_system, 'agents'):
                total_quantum = len(system.quantum_bridge.quantum_system.agents)

    validation_report = validate_agent_training_readiness_v6(system)

    logger.critical(f"   Result: {main_fixed} system.agents, {quantum_fixed} quantum_bridge agents")
    if checkpoint_was_loaded:
        logger.critical(f"   âœ… Checkpoint weights should be preserved!")

    # Fallback to old fixes if needed
    if main_fixed == 0 and quantum_fixed == 0 and total_quantum > 0:
        logger.critical("âš ï¸  No agents fixed - trying fallback fixes...")
        force_fix_all_optimizers(system)
        emergency_fix_agent_optimizers(system)

    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V8.5.8 FIX: ADD TRAINING LOOP (LOGGING SILENCED - PRINTS ONLY)
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ“ TRAINING LOOP ACTIVE")
    print("="*80)
    print("   â€¢ Training every 10 seconds when buffer >= 64")
    print("   â€¢ Batch size: 32")
    print("   â€¢ Only training output will be shown below")
    print("="*80 + "\n")

    # V8.5.10: Use loaded training steps as base, not 0
    loaded_steps = getattr(system, '_loaded_training_steps', 0)
    training_counter = loaded_steps // 32  # Convert steps to training iterations
    print(f"   ğŸ“Š Resuming from training_counter = {training_counter} (loaded {loaded_steps} steps)")
    last_training_time = time.time()

    # ============================================================================
    # V5.0.3: INITIALIZE NON-BLOCKING TRAINING EXECUTOR
    # ============================================================================
    # This prevents the "stale data" issue by running training in background
    print("\n" + "="*80)
    print("âš¡ V5.0.3: INITIALIZING NON-BLOCKING TRAINING")
    print("="*80)
    async_trainer = init_async_trainer(max_workers=1)
    system._async_trainer = async_trainer  # Store reference for cleanup
    print("   âœ… Training will run in background thread")
    print("   âœ… Data ingestion will NOT be blocked during training")
    print("   âœ… State cache will remain fresh")
    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V5.0.4: INITIALIZE NON-BLOCKING SIGNAL PUBLISHER
    # ============================================================================
    # This prevents Ably publish timeouts from blocking the event loop
    print("\n" + "="*80)
    print("âš¡ V5.0.4: INITIALIZING NON-BLOCKING SIGNAL PUBLISHER")
    print("="*80)
    signal_publisher = init_signal_publisher(timeout=10.0, max_retries=2)
    system._signal_publisher = signal_publisher  # Store reference for cleanup
    print("   âœ… Signal publishing will run in background thread")
    print("   âœ… Ably timeouts will NOT block the event loop")
    print("   âœ… Feature messages will continue flowing during publish")
    print("   âœ… Timeout: 10s, Retries: 2")
    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V5.0.5: INITIALIZE AGENT-LEVEL LEARNING DIAGNOSTICS
    # ============================================================================
    # Enables detection of policy collapse, over-coordination, and gradient
    # homogenisation in CTDE multi-agent RL systems (per MARL literature).
    # 
    # References:
    #   - Lowe et al. 2017 (MADDPG)
    #   - Rashid et al. 2018 (QMIX)
    #   - Foerster et al. 2018 (COMA)
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ”¬ V5.0.5: INITIALIZING AGENT-LEVEL LEARNING DIAGNOSTICS")
    print("="*80)
    
    # Get agent names from system
    agent_names = list(getattr(system, 'agent_names', ['xs', 's', 'm', 'l', 'xl', 'xxl', '5m', '10m']))
    
    try:
        diagnostics = init_agent_diagnostics(
            agent_names=agent_names,
            action_dim=2,  # BUY/SELL
            enabled=True
        )
        system._agent_diagnostics = diagnostics  # Store reference
        print(f"   âœ… Monitoring {len(agent_names)} agents for learning health")
        print("   âœ… Policy collapse detection (entropy threshold: 0.15 bits)")
        print("   âœ… Over-coordination detection (agreement threshold: 90%)")
        print("   âœ… Q-value saturation detection (|Q| threshold: 5.0)")
        print("   âœ… Gradient suppression detection (norm threshold: 1e-6)")
    except Exception as e:
        print(f"   âš ï¸  Diagnostics initialization failed: {e}")
        print("   âš ï¸  Training will continue without advanced diagnostics")
        system._agent_diagnostics = None
    
    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V5.0.6: INITIALIZE EXPLORATION MANAGER (Anti-Collapse)
    # ============================================================================
    # Provides per-agent exploration with entropy-aware adaptation.
    # Collapsed agents automatically receive boosted exploration.
    #
    # References:
    #   - Haarnoja et al. 2018 (SAC entropy regularization)
    #   - Lowe et al. 2017 (MADDPG exploration)
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ² V5.0.6: INITIALIZING EXPLORATION MANAGER (Anti-Collapse)")
    print("="*80)
    
    try:
        exploration_mgr = init_exploration_manager(
            agent_names=agent_names,
            initial_temperature=1.0,     # Softmax temperature (1.0 = neutral)
            min_temperature=0.1,         # Don't go fully greedy
            temperature_decay=0.9995,    # Slow decay
            base_epsilon=0.1,            # 10% random for healthy agents
            collapse_epsilon=0.3,        # 30% random for collapsed agents
            entropy_threshold=0.15,      # Below this = collapsed
            severe_collapse_threshold=0.05  # Below this = severe
        )
        system._exploration_manager = exploration_mgr  # Store reference
        print(f"   âœ… Exploration manager for {len(agent_names)} agents")
        print("   âœ… Temperature scaling: 1.0 â†’ 0.1 (softmax)")
        print("   âœ… Base epsilon: 10% (healthy agents)")
        print("   âœ… Collapse epsilon: 30% (collapsed agents)")
        print("   âœ… Entropy-aware adaptation enabled")
    except Exception as e:
        print(f"   âš ï¸  ExplorationManager initialization failed: {e}")
        system._exploration_manager = None
    
    print("="*80 + "\n")
    # ============================================================================

    # ============================================================================
    # V5.0.6: INITIALIZE GATED ENTANGLEMENT MODULE
    # ============================================================================
    # Caps z_shared influence to ensure local signals dominate.
    # Prevents over-coordination by limiting entanglement to max 30%.
    #
    # References:
    #   - Rashid et al. 2018 (QMIX - value decomposition)
    #   - Sunehag et al. 2018 (VDN - mixing networks)
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ”— V5.0.6: INITIALIZING GATED ENTANGLEMENT MODULE")
    print("="*80)
    
    try:
        state_dim = getattr(system, 'state_dim', 62)
        gated_entanglement = GatedEntanglement(
            state_dim=state_dim,
            hidden_dim=64,
            max_gate=0.3  # Cap z_shared influence at 30%
        ).to(system.device if hasattr(system, 'device') else 'cpu')
        
        system._gated_entanglement = gated_entanglement
        print(f"   âœ… GatedEntanglement: state_dim={state_dim}")
        print("   âœ… Max gate: 30% (local signals dominate)")
        print("   âœ… Learnable gate initialized with low values")
        print("   âœ… Prevents z_shared from overwhelming local state")
    except Exception as e:
        print(f"   âš ï¸  GatedEntanglement initialization failed: {e}")
        system._gated_entanglement = None
    
    print("="*80 + "\n")
    # ============================================================================
    # Skip if resuming from existing checkpoint - don't overwrite progress!
    # ============================================================================
    try:
        ckpt_mgr = getattr(system.quantum_bridge, 'checkpoint_manager', None) if hasattr(system, 'quantum_bridge') else None
        checkpoint_was_loaded = getattr(system, '_checkpoint_was_loaded', False)
        loaded_steps = getattr(system, '_loaded_training_steps', 0)

        if ckpt_mgr is not None:
            if checkpoint_was_loaded and loaded_steps > 0:
                # V8.5.10: Don't save initial checkpoint when resuming
                print("\n" + "="*80)
                print("ğŸ“‚ CHECKPOINT RESUME MODE")
                print("="*80)
                print(f"   âœ… Resumed from step {loaded_steps}")
                print(f"   â­ï¸  Skipping initial checkpoint save (would overwrite progress)")
                print(f"   ğŸ“Š Next save will be at step {loaded_steps} + new training")
                print("="*80 + "\n")
            else:
                # Fresh start - save initial checkpoint at step 0
                print("\n" + "="*80)
                print("ğŸ’¾ SAVING INITIAL CHECKPOINT (Fresh Start)...")
                print("="*80)

                # Ensure bridge has components
                if hasattr(system, 'quantum_advisor'):
                    system.quantum_bridge.quantum_advisor = system.quantum_advisor
                if hasattr(system.quantum_bridge, 'quantum_trainer'):
                    system.quantum_bridge.voting_trainer = system.quantum_bridge.quantum_trainer

                save_result = ckpt_mgr.save_from_bridge(
                    bridge=system.quantum_bridge,
                    training_steps=0,
                    upload_to_gcs=True,
                    notes="Initial checkpoint at system start (fresh)"
                )

                if save_result.success:
                    ckpt_id = f"{save_result.instrument}_v{save_result.voyage_number}_step_{save_result.training_steps}"
                    print(f"âœ… INITIAL CHECKPOINT SAVED: {ckpt_id}")
                    print(f"   Local: {save_result.local_path}")
                    if save_result.gcs_path:
                        print(f"   GCS: {save_result.gcs_path}")
                else:
                    print(f"âš ï¸  Initial checkpoint issues: {save_result.errors}")

                print("="*80 + "\n")

            system._last_checkpoint_time = time.time()
    except Exception as init_ckpt_err:
        print(f"âš ï¸  Initial checkpoint error: {init_ckpt_err}")
    # ============================================================================

    def _get_buffer_size(buf):
        """Return a safe size for buffers that may expose .size (method or int) or be sized via len()."""
        if buf is None:
            return 0
        if hasattr(buf, "size"):
            try:
                return buf.size() if callable(buf.size) else buf.size
            except Exception:
                # Fall back to len() if the attribute exists but calling/reading failed
                try:
                    return len(buf)
                except Exception:
                    return 0
        try:
            return len(buf)
        except Exception:
            return 0

    # ============================================================================
    # V5.0: HEALTH MONITORING SYSTEM
    # ============================================================================
    # These background threads monitor system health and alert on issues:
    # 1. State cache freshness (alerts if data stale > 30s)
    # 2. Ably connection watchdog (alerts on disconnect)
    # ============================================================================
    print("\n" + "="*80)
    print("ğŸ¥ V5.0: STARTING HEALTH MONITORING SYSTEM")
    print("="*80)

    # State freshness monitoring
    _v5_state_hash = [None]
    _v5_stale_count = [0]

    def _v5_monitor_state_freshness():
        """V5.0: Monitor if state_cache is being updated with fresh data."""
        while getattr(system, '_running', True):
            try:
                if hasattr(system, 'quantum_bridge') and system.quantum_bridge.state_cache is not None:
                    current_hash = hash(system.quantum_bridge.state_cache.tobytes())
                    
                    if current_hash == _v5_state_hash[0]:
                        _v5_stale_count[0] += 1
                        if _v5_stale_count[0] >= 3:  # 30 seconds stale
                            print(f"[{time.strftime('%H:%M:%S')}] ğŸ”´ STATE CACHE STALE FOR {_v5_stale_count[0]*10}s! No new data flowing.")
                    else:
                        if _v5_stale_count[0] >= 3:
                            print(f"[{time.strftime('%H:%M:%S')}] âœ… State cache updated after {_v5_stale_count[0]*10}s stale period")
                        _v5_stale_count[0] = 0
                    
                    _v5_state_hash[0] = current_hash
                
                time.sleep(10)
            except Exception as e:
                time.sleep(10)

    # Ably connection watchdog
    _v5_disconnect_count = [0]

    def _v5_ably_watchdog():
        """V5.0: Monitor Ably connection status."""
        while getattr(system, '_running', True):
            try:
                if hasattr(system, 'ably') and system.ably is not None:
                    state = str(getattr(system.ably.connection, 'state', 'unknown'))
                    
                    if 'DISCONNECTED' in state.upper() or 'FAILED' in state.upper():
                        _v5_disconnect_count[0] += 1
                        if _v5_disconnect_count[0] >= 3:
                            print(f"[{time.strftime('%H:%M:%S')}] ğŸ”´ ABLY DISCONNECTED FOR {_v5_disconnect_count[0]*10}s! Rewards not flowing.")
                    else:
                        if _v5_disconnect_count[0] >= 3:
                            print(f"[{time.strftime('%H:%M:%S')}] âœ… Ably reconnected after {_v5_disconnect_count[0]*10}s")
                        _v5_disconnect_count[0] = 0
                
                time.sleep(10)
            except Exception:
                time.sleep(10)

    # Start monitoring threads
    import threading
    threading.Thread(target=_v5_monitor_state_freshness, daemon=True, name="V5_StateMonitor").start()
    threading.Thread(target=_v5_ably_watchdog, daemon=True, name="V5_AblyWatchdog").start()

    print("âœ… State freshness monitor started (checks every 10s)")
    print("âœ… Ably connection watchdog started (checks every 10s)")
    print("="*80 + "\n")
    # ============================================================================
    
    
    try:
        while system._running:  # âœ… use _running
            now = time.time()
            time.sleep(1)
    
            # ====================================================================
            # V8.5.8: TRAINING EXECUTION (EVERY 10 SECONDS)
            # ====================================================================
            if now - last_training_time >= 10:  # Train every 10 seconds
                # =================================================================
                # V8.5.9 FIX: DIAGNOSTIC LOGGING
                # =================================================================
                print(f"\n{'='*80}")
                print(f"ğŸ” TRAINING CHECK (every 10s)")
                print(f"{'='*80}")
    
                # Check experience_buffer
                exp_buf_size = 0
                if hasattr(system, 'experience_buffer') and system.experience_buffer is not None:
                    exp_buf_size = _get_buffer_size(system.experience_buffer)
                    print(f"   experience_buffer size: {exp_buf_size}")
                else:
                    print(f"   experience_buffer: NOT FOUND")
    
                # Check hybrid_buffer
                hybrid_buf_size = 0
                if hasattr(system, 'quantum_bridge'):
                    print(f"   quantum_bridge: EXISTS")
                    if hasattr(system.quantum_bridge, 'hybrid_buffer'):
                        if system.quantum_bridge.hybrid_buffer is not None:
                            hybrid_buf_size = _get_buffer_size(system.quantum_bridge.hybrid_buffer)
                            print(f"   hybrid_buffer size: {hybrid_buf_size}")
                        else:
                            print(f"   hybrid_buffer: IS NONE")
                    else:
                        print(f"   hybrid_buffer: NOT FOUND")
                else:
                    print(f"   quantum_bridge: NOT FOUND")
    
                # Check trainer
                has_trainer = False
                if hasattr(system, 'quantum_bridge'):
                    if hasattr(system.quantum_bridge, 'quantum_trainer'):
                        print(f"   quantum_trainer: EXISTS")
                        has_trainer = True
                    elif hasattr(system.quantum_bridge, 'trainer'):
                        print(f"   trainer (alt location): EXISTS")
                        has_trainer = True
    
                if not has_trainer:
                    print(f"   trainer: NOT FOUND")
    
                print(f"   Training conditions: trainer={has_trainer}, buffer_size={hybrid_buf_size}, ready={has_trainer and hybrid_buf_size >= 64}")
                print(f"{'='*80}\n")
                # =================================================================
    
                # =================================================================
                # V8.5.9 FIX: EXTENDED DIAGNOSTICS (EVERY 30 SECONDS)
                # =================================================================
                # Run deeper diagnostics every 30 seconds to identify reward flow issues
                if int(now) % 30 < 10:  # Every 30 seconds
                    diagnose_ably_connection(system)
                    diagnose_signal_generation(system)
                # =================================================================
    
                try:
                    # Check if we have trainer and buffer
                    trainer = None
                    buffer = None
                    buffer_size = 0
    
                    # V8.5.8 FIX: Correct trainer location is quantum_trainer (not trainer)
                    if hasattr(system, 'quantum_bridge'):
                        # Try quantum_trainer first (correct location)
                        if hasattr(system.quantum_bridge, 'quantum_trainer'):
                            trainer = system.quantum_bridge.quantum_trainer
                        # Fallback to trainer attribute
                        elif hasattr(system.quantum_bridge, 'trainer'):
                            trainer = system.quantum_bridge.trainer
    
                        # Get buffer
                        if hasattr(system.quantum_bridge, 'hybrid_buffer'):
                            buffer = system.quantum_bridge.hybrid_buffer
                            buffer_size = _get_buffer_size(buffer)
    
                    # Fallback: check system directly
                    if not trainer and hasattr(system, 'quantum_trainer'):
                        trainer = system.quantum_trainer
                    if not trainer and hasattr(system, 'trainer'):
                        trainer = system.trainer
                    if not buffer and hasattr(system, 'hybrid_buffer'):
                        buffer = system.hybrid_buffer
                        buffer_size = _get_buffer_size(buffer)
    
                    # Execute training if conditions met
                    if trainer and buffer and buffer_size >= 64:
                        # ============================================================
                        # V5.0.3: NON-BLOCKING TRAINING EXECUTION
                        # ============================================================
                        # Check if async trainer is available and not already training
                        if async_trainer.is_training():
                            # Training already in progress - don't block, just skip
                            print(f"   â³ Training #{training_counter + 1} skipped - previous training still running")
                            print(f"   âœ… Data ingestion continues uninterrupted")
                        else:
                            training_counter += 1

                            # ============================================================
                            # PRINT TO CONSOLE - TRAINING SUBMITTED (NON-BLOCKING)
                            # ============================================================
                            print("\n" + "ğŸ“"*40)
                            print(f"TRAINING #{training_counter} SUBMITTED (non-blocking)")
                            print(f"Buffer: {buffer_size} experiences | Batch: 32")
                            print(f"   âš¡ Training will execute in background thread")
                            print(f"   âœ… Main loop continues immediately")
                            print("ğŸ“"*40)
                            # ============================================================

                            # Submit training to background thread (returns immediately)
                            submitted = async_trainer.submit_training(
                                trainer=trainer,
                                buffer=buffer,
                                batch_size=32,
                                training_number=training_counter
                            )

                            if submitted:
                                print(f"   âœ… Training #{training_counter} running in background")
                            else:
                                print(f"   âš ï¸ Could not submit training #{training_counter}")
                                training_counter -= 1  # Revert counter

                        # ============================================================
                        # CHECK LAST TRAINING RESULT (if any completed)
                        # ============================================================
                        last_result = async_trainer.get_last_result()
                        if last_result and 'training_number' in last_result:
                            last_num = last_result.get('training_number', 0)
                            # Only show if we haven't reported this result yet
                            if not hasattr(system, '_last_reported_training') or system._last_reported_training < last_num:
                                system._last_reported_training = last_num
                                if 'error' not in last_result:
                                    print(f"\n   ğŸ“Š Last completed: Training #{last_num}")
                                    bg_time = last_result.get('training_time_background', 0)
                                    print(f"      Background time: {bg_time:.1f}s (main loop was NOT blocked)")
                        # ============================================================
    
                    elif not trainer:
                        if training_counter == 0:  # Only log once at startup
                            logger.warning("âš ï¸  [TRAINING] Trainer not found - checking locations...")
                            # Diagnostic logging
                            logger.warning(f"   - system.quantum_bridge exists: {hasattr(system, 'quantum_bridge')}")
                            if hasattr(system, 'quantum_bridge'):
                                logger.warning(f"   - quantum_bridge.quantum_trainer exists: {hasattr(system.quantum_bridge, 'quantum_trainer')}")
                                logger.warning(f"   - quantum_bridge.trainer exists: {hasattr(system.quantum_bridge, 'trainer')}")
                            logger.warning(f"   - system.quantum_trainer exists: {hasattr(system, 'quantum_trainer')}")
                            logger.warning(f"   - system.trainer exists: {hasattr(system, 'trainer')}")
                            training_counter = -1  # Set to -1 so we don't log again
    
                    elif buffer_size < 64:
                        if training_counter % 6 == 0:  # Log every minute (6 * 10s)
                            logger.info(
                                f"ğŸ“Š [TRAINING] Waiting for buffer | "
                                f"Current: {buffer_size}/64 required"
                            )
    
                except Exception as e:
                    logger.error(f"âŒ [TRAINING LOOP] Unexpected error: {e}")
                    import traceback
                    traceback.print_exc()
    
                last_training_time = now

            # ============================================================================

            # ============================================================================
            # CHECKPOINT SAVE (EVERY 10 MINUTES BASED ON EXECUTION TIME)
            # ============================================================================
            try:
                ckpt_mgr = getattr(system.quantum_bridge, 'checkpoint_manager', None) if hasattr(system, 'quantum_bridge') else None

                if ckpt_mgr is not None:
                    # Initialize last checkpoint time if not set
                    if not hasattr(system, '_last_checkpoint_time'):
                        system._last_checkpoint_time = time.time()

                    # Check if 10 minutes (600 seconds) have passed based on EXECUTION TIME
                    elapsed_since_checkpoint = now - system._last_checkpoint_time

                    if elapsed_since_checkpoint >= 600:
                        print(f"\n{'='*80}")
                        print(f"ğŸ’¾ CHECKPOINT SAVE (10 min interval)")
                        print(f"   Execution time since last save: {elapsed_since_checkpoint/60:.1f} minutes")
                        print(f"{'='*80}")

                        # Ensure bridge has required components
                        if hasattr(system, 'quantum_advisor'):
                            system.quantum_bridge.quantum_advisor = system.quantum_advisor
                        if hasattr(system.quantum_bridge, 'quantum_trainer'):
                            system.quantum_bridge.voting_trainer = system.quantum_bridge.quantum_trainer

                        # V8.5.10: Calculate training steps INCLUDING loaded base
                        loaded_base = getattr(system, '_loaded_training_steps', 0)
                        new_steps = training_counter * 32
                        total_steps = loaded_base + new_steps
                        print(f"   ğŸ“Š Total steps: {loaded_base} (loaded) + {new_steps} (new) = {total_steps}")

                        save_result = ckpt_mgr.save_from_bridge(
                            bridge=system.quantum_bridge,
                            training_steps=total_steps,
                            upload_to_gcs=True,
                            notes=f"Auto-save after {elapsed_since_checkpoint/60:.1f} min execution"
                        )

                        if save_result.success:
                            ckpt_id = f"{save_result.instrument}_v{save_result.voyage_number}_step_{save_result.training_steps}"
                            print(f"âœ… CHECKPOINT SAVED: {ckpt_id}")
                            print(f"   Local: {save_result.local_path}")
                            if save_result.gcs_path:
                                print(f"   GCS: {save_result.gcs_path}")
                        else:
                            print(f"âš ï¸  Checkpoint save issues: {save_result.errors}")

                        system._last_checkpoint_time = time.time()
                        print(f"{'='*80}\n")

            except Exception as ckpt_err:
                print(f"âš ï¸  Checkpoint error: {ckpt_err}")
            # ============================================================================

            # ğŸ” Periodic diagnostics every 2 minutes
            if now - system._last_diagnostic > 120:
                try:
                    system.diagnose_training_pipeline()
                except Exception as e:
                    logger.warning(f"Diagnostic check failed: {e}")
                system._last_diagnostic = now
    except KeyboardInterrupt:
        print("ğŸ”§ [SYSTEM] Shutting down...")
    except Exception as e:
        print(f"ğŸ”§ [SYSTEM] Unexpected error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ================================================================
        # FINAL CHECKPOINT SAVE BEFORE SHUTDOWN
        # ================================================================
        try:
            ckpt_mgr = getattr(system.quantum_bridge, 'checkpoint_manager', None) if hasattr(system, 'quantum_bridge') else None

            if ckpt_mgr is not None:
                print("\n" + "="*80)
                print("ğŸ’¾ SAVING FINAL CHECKPOINT BEFORE SHUTDOWN...")
                print("="*80)

                # Ensure bridge has components
                if hasattr(system, 'quantum_advisor'):
                    system.quantum_bridge.quantum_advisor = system.quantum_advisor
                if hasattr(system.quantum_bridge, 'quantum_trainer'):
                    system.quantum_bridge.voting_trainer = system.quantum_bridge.quantum_trainer

                # V8.5.10: Get training steps INCLUDING loaded base
                loaded_base = getattr(system, '_loaded_training_steps', 0)
                new_steps = training_counter * 32 if 'training_counter' in dir() else 0
                total_steps = loaded_base + new_steps
                print(f"   ğŸ“Š Final steps: {loaded_base} (loaded) + {new_steps} (new) = {total_steps}")

                save_result = ckpt_mgr.save_from_bridge(
                    bridge=system.quantum_bridge,
                    training_steps=total_steps,
                    upload_to_gcs=True,
                    notes="Final shutdown save"
                )

                if save_result.success:
                    ckpt_id = f"{save_result.instrument}_v{save_result.voyage_number}_step_{save_result.training_steps}"
                    print(f"âœ… FINAL CHECKPOINT SAVED: {ckpt_id}")
                    print(f"   Local: {save_result.local_path}")
                    if save_result.gcs_path:
                        print(f"   GCS: {save_result.gcs_path}")
                else:
                    print(f"âš ï¸  Final checkpoint may have issues: {save_result.errors}")

                print("="*80 + "\n")

        except Exception as final_ckpt_err:
            print(f"âš ï¸  Final checkpoint error: {final_ckpt_err}")
        # ================================================================

        system.stop()

class QuantumMetaController:
    """
    Meta controller for quantum trading system with periodic resets
    """

    def __init__(self, reset_interval_minutes=3000):
        self.reset_interval = reset_interval_minutes * 60
        self.voyage_number = 0
        self.meta_log_path = Path("./meta_logs")
        self.meta_log_path.mkdir(exist_ok=True)
        self.voyage_history = []

    def run_voyage(self):
        """Execute one voyage with quantum system"""
        self.voyage_number += 1
        start_time = time.time()

        print("\n" + "="*80)
        print(f"QUANTUM VOYAGE {self.voyage_number} INITIATED")
        print(f"   Start Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   Duration: {self.reset_interval/60:.0f} minutes")
        print("="*80 + "\n")

        logger.critical(f"QUANTUM VOYAGE {self.voyage_number} STARTING")

        self.prepare_fresh_environment()

        voyage_deadline = start_time + self.reset_interval
        system_container = {
            'system': None,
            'three_tier': None,
            'autosave': None,
            'error': None
        }

        def run_with_cleanup():
            try:
                # Run quantum integrated main
                system, three_tier, autosave = quantum_integrated_main()
                system_container['system'] = system
                system_container['three_tier'] = three_tier
                system_container['autosave'] = autosave
            except Exception as e:
                logger.error(f"quantum_integrated_main error: {e}")
                traceback.print_exc()
                system_container['error'] = e

        system_thread = threading.Thread(target=run_with_cleanup, daemon=False)
        system_thread.start()

        last_status = start_time
        while time.time() < voyage_deadline and system_thread.is_alive():
            time.sleep(3)

            if time.time() - last_status > 300:
                elapsed = (time.time() - start_time) / 60
                remaining = (voyage_deadline - time.time()) / 60
                progress = (elapsed / (self.reset_interval/60)) * 100

                print(f"\nQUANTUM VOYAGE {self.voyage_number} STATUS:")
                print(f"   Elapsed: {elapsed:.1f}min | Remaining: {remaining:.1f}min | Progress: {progress:.0f}%")

                # Print quantum metrics if available
                if system_container['system'] and hasattr(system_container['system'], 'quantum_bridge'):
                    try:
                        metrics = system_container['system'].quantum_bridge.get_system_metrics()
                        print(f"   Quantum Buffer: {metrics['buffer_size']}")
                        print(f"   Entanglement: {metrics['entanglement']['mean']:.4f}")
                    except:
                        pass

                last_status = time.time()

        print(f"\nCleaning up Quantum Voyage {self.voyage_number}...")
        self.cleanup_voyage(system_container)

        if system_thread.is_alive():
            print("   Waiting for main thread to terminate...")
            system_thread.join(timeout=5)
            if system_thread.is_alive():
                print("   WARNING: Main thread did not terminate cleanly")

        self.log_voyage_stats(start_time)
        print(f"\nQUANTUM VOYAGE {self.voyage_number} COMPLETE")
        print("="*80 + "\n")

    def cleanup_voyage(self, system_container):
        """Clean up quantum system resources"""
        try:
            if system_container['autosave']:
                print("   Stopping autosave manager...")
                try:
                    system_container['autosave'].stop()
                    print("   - Autosave stopped")
                except Exception as e:
                    print(f"   - Autosave stop error: {e}")

            if system_container['system']:
                print("   Stopping quantum system...")
                system = system_container['system']

                # Stop running flag
                system._running = False

                # Stop quantum-specific components
                if hasattr(system, 'quantum_bridge'):
                    try:
                        # Quantum bridge doesn't need explicit stop
                        print("   - Quantum bridge cleanup")
                    except Exception as e:
                        print(f"   - Quantum bridge error: {e}")

                # Stop batch processor (FIXED - now has stop() method)
                if hasattr(system, 'batch_processor') and system.batch_processor:
                    try:
                        if hasattr(system.batch_processor, 'stop'):
                            system.batch_processor.stop()
                            print("   - Batch processor stopped")
                        else:
                            print("   - Batch processor has no stop method")
                    except Exception as e:
                        print(f"   - Batch processor stop error: {e}")

                # Stop Discord
                if hasattr(system, 'discord_sender'):
                    try:
                        system.discord_sender.stop()
                        print("   - Discord sender stopped")
                    except Exception as e:
                        print(f"   - Discord sender stop error: {e}")

                # Cancel async tasks
                if hasattr(system, 'safe_task_manager'):
                    try:
                        system.safe_task_manager.cancel_all_tasks()
                        print("   - Async tasks cancelled")
                    except Exception as e:
                        print(f"   - Task cancellation error: {e}")

                # Close Ably
                if hasattr(system, 'ably') and system.ably:
                    try:
                        system.ably.close()
                        print("   - Ably connection closed")
                    except Exception as e:
                        print(f"   - Ably close error: {e}")

                # Stop event loop
                if hasattr(system, 'loop') and system.loop:
                    try:
                        if system.loop.is_running():
                            system.loop.stop()
                        print("   - Event loop stopped")
                    except Exception as e:
                        print(f"   - Event loop stop error: {e}")

            # Garbage collection
            import gc
            gc.collect()
            print("   - Garbage collection completed")

            # CUDA cleanup
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("   - CUDA cache cleared")

            time.sleep(2)

            remaining_threads = threading.active_count()
            print(f"   Cleanup complete ({remaining_threads} threads remaining)")

        except Exception as e:
            logger.error(f"Cleanup error: {e}")
            print(f"   Cleanup encountered errors: {e}")

    def prepare_fresh_environment(self):
        """Prepare fresh environment for quantum system"""
        print("Preparing fresh quantum environment...")

        import shutil
        import random
        import numpy as np
        import tensorflow as tf

        # Clear save directories
        save_dirs = ["./saves", "/tmp/agents", "/tmp/rl_data"]
        for save_dir in save_dirs:
            if os.path.exists(save_dir):
                try:
                    shutil.rmtree(save_dir)
                    print(f"   - Cleared {save_dir}")
                except Exception as e:
                    logger.warning(f"Could not clear {save_dir}: {e}")

        # Set random seeds
        seed = int(time.time()) % 100000
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        tf.random.set_seed(seed)
        print(f"   - Random seed: {seed}")
        print()

    def log_voyage_stats(self, start_time):
        """Log quantum voyage statistics"""
        import json
        import pickle

        duration = time.time() - start_time

        stats = {
            'voyage': self.voyage_number,
            'start_time': time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)),
            'duration_minutes': duration / 60,
            'end_time': time.strftime("%Y-%m-%d %H:%M:%S"),
            'system_type': 'quantum'
        }

        # Try to get quantum-specific stats
        try:
            if os.path.exists("./saves/reward_history.pkl"):
                with open("./saves/reward_history.pkl", "rb") as f:
                    rewards = pickle.load(f)
                    stats['rewards_processed'] = len(rewards)
        except:
            pass

        log_file = self.meta_log_path / f"quantum_voyage_{self.voyage_number}.json"
        with open(log_file, 'w') as f:
            json.dump(stats, f, indent=2)

        self.voyage_history.append(stats)

        print(f"\nQUANTUM VOYAGE {self.voyage_number} SUMMARY:")
        print(f"   Duration: {stats['duration_minutes']:.1f} minutes")
        if 'rewards_processed' in stats:
            print(f"   Rewards Processed: {stats['rewards_processed']}")

        logger.critical(f"Quantum Voyage {self.voyage_number} stats logged")

    def run_forever(self):
        """Run continuous quantum voyage cycles"""
        print("\n" + "="*80)
        print("QUANTUM META CONTROLLER ACTIVATED")
        print(f"   Reset Interval: {self.reset_interval/60:.0f} minutes")
        print(f"   System Type: Pure Quantum")
        print("="*80 + "\n")

        logger.critical("QUANTUM META CONTROLLER ACTIVE")

        while True:
            try:
                self.run_voyage()

                pause_seconds = 10
                print(f"Pausing {pause_seconds} seconds before next quantum voyage...\n")
                time.sleep(pause_seconds)

            except KeyboardInterrupt:
                raise
            except Exception as e:
                logger.error(f"Quantum voyage error: {e}")
                traceback.print_exc()
                time.sleep(30)

    def get_voyage_history(self):
        """Return quantum voyage history"""
        return self.voyage_history

# ============================================================================
# VERIFICATION UTILITY - V7 QUANTUM ADVISOR INTEGRATION
# ============================================================================


# ============================================================================
# ENTRY POINT
# ============================================================================

def main():
    """
    Main entry point for quantum trading system (V7)
    """
    # ============================================================================
    # AGGRESSIVE LOGGING SILENCER - ONLY TRAINING PRINTS VISIBLE
    # ============================================================================
    import logging
    import sys

    # 1. Disable ALL logging completely
    logging.disable(logging.CRITICAL)  # This disables ALL logging calls

    # 2. Set root logger to highest level as backup
    logging.getLogger().setLevel(logging.CRITICAL + 100)

    # 3. Remove all handlers from root logger
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # 4. Silence specific noisy modules
    for logger_name in [
        '__main__',
        'asyncio',
        'matplotlib',
        'PIL',
        'urllib3',
        'qiskit',
        'qiskit.transpiler',
        'qiskit.compiler',
        'qiskit.passmanager',
        'qiskit.passmanager.base_tasks',
        'websocket',
        'ably',
        'discord',
        'httpx',
        'httpcore',
    ]:
        try:
            logger = logging.getLogger(logger_name)
            logger.disabled = True
            logger.propagate = False
            logger.setLevel(logging.CRITICAL + 100)
            for handler in logger.handlers[:]:
                logger.removeHandler(handler)
        except:
            pass

    # 5. Suppress warnings module
    import warnings
    warnings.filterwarnings('ignore')

    print("\n" + "="*80)
    print("ğŸ”‡ ALL LOGGING DISABLED - ONLY TRAINING PRINTS WILL SHOW")
    print("="*80)
    print()
    # ============================================================================

    print("\n" + "="*80)
    print("QUANTUM TRADING SYSTEM V8.6.0 - DIVERSITY PRESERVATION ACTIVE")
    print("="*80)
    print("   - Pure quantum predictions (no classical fallback)")
    print("   - Multi-timeframe entanglement")
    print("   - Quantum voting system")
    print("   - âœ¨ QUANTUM ADVISOR Q-VALUE TEMPERING (ACTIVE)")
    print("   - ğŸ”® Complete CRITICAL-level logging")
    print("   - âš¡ V8.5.5: Each agent uses its OWN timeframe state")
    print("   - ğŸ¯ V8.5.6: Forecast bias detection and diagnostics")
    print("   - ğŸ“Š V8.5.6: State variance and signal monitoring")
    print("   - ğŸ”‡ V8.5.7: Qiskit verbose logging silenced")
    print("   - ğŸ“ V8.5.9: BUFFER LINKED + TRAINING ACTIVE (every 10s)")
    print("   - ğŸ”„ V8.5.10: PROPER CHECKPOINT RESUME (preserves step count + weights)")
    print("   - ğŸ­ V8.6.0: DIVERSITY PRESERVATION MODULE INTEGRATED")
    print("     â””â”€ AgentSpecificLatentProjector: Prevents z_shared dominance")
    print("     â””â”€ DiversityLoss: Penalizes action homogenization")
    print("     â””â”€ AdaptiveCoordinationModule: Confidence-weighted blending")
    print("     â””â”€ TimeframeStateAugmenter: Prevents state staleness")
    print("   - âš¡ V5.0.3: NON-BLOCKING TRAINING EXECUTOR")
    print("     â””â”€ Training runs in background thread")
    print("     â””â”€ Data ingestion NEVER blocked during training")
    print("     â””â”€ State cache remains fresh (fixes stale data issue)")
    print("   - ğŸ“¡ V5.0.4: NON-BLOCKING SIGNAL PUBLISHER")
    print("     â””â”€ Signal publishing runs in background thread")
    print("     â””â”€ Ably timeouts do NOT block event loop")
    print("     â””â”€ Features continue flowing during publish")
    print("   - ğŸ”¬ V5.0.5: AGENT-LEVEL LEARNING DIAGNOSTICS")
    print("     â””â”€ Policy collapse detection (entropy < 0.15 bits)")
    print("     â””â”€ Over-coordination detection (>90% agreement)")
    print("     â””â”€ Q-value saturation detection (|Q| > 5.0)")
    print("     â””â”€ MARL health summary per training step")
    print("   - ğŸ›¡ï¸ V5.0.6: MARL ANTI-COLLAPSE MECHANISMS")
    print("     â””â”€ Entropy bonus in actor loss (-Î²Â·H(Ï€))")
    print("     â””â”€ Fixed Q-spread loss: -log(var) not 1/var")
    print("     â””â”€ Temperature-scaled action selection")
    print("     â””â”€ Per-agent epsilon (boosted for collapsed)")
    print("     â””â”€ Automatic collapse recovery (soft reset)")
    print("   - Automatic periodic resets (3000 minutes)")
    print("   - All helper functions maintained")
    print("="*80 + "\n")

    print("â³ Initializing system (this may take 30-60 seconds)...\n")

    # Capture stdout during initialization to suppress any stray prints
    import io
    original_stdout = sys.stdout

    # Redirect stdout to null during initialization
    sys.stdout = io.StringIO()

    try:
        controller = QuantumMetaController(reset_interval_minutes=3000)
    finally:
        # Restore stdout after initialization
        sys.stdout = original_stdout

    print("âœ… System initialized! Training will begin when buffer >= 64 experiences.\n")
    print("="*80)
    print("WAITING FOR TRAINING TO START...")
    print("="*80 + "\n")

    try:
        controller.run_forever()
    except KeyboardInterrupt:
        print("\n\nQuantum system stopped by user")

        history = controller.get_voyage_history()
        if history:
            print(f"\nQUANTUM SESSION SUMMARY:")
            print(f"   Total Voyages: {len(history)}")
            print(f"   Total Duration: {sum(v['duration_minutes'] for v in history):.1f} minutes")

        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
# ============================================================================

#+263780563561  ENG Karl Muzunze Masvingo Zimbabwe

"""

A chorus of identical voices produces monotony; a cacophony of random sounds produces chaos. But an ensemble of diverse instruments, each playing its part while attending to the whole, produces the symphony of profitable trading.


"""